{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "import numpy as np \n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "RAW_DIR = \"../raw/\"\n",
    "LIST_RAW_CSV = \"../list_raw/all.csv\"\n",
    "LIST_RAW_JSON = \"../transformed/all_json.json\"\n",
    "TRANSFORMED_DIR = \"../transformed/\"\n",
    "OUTPUT = \"output.txt\"\n",
    "GBTOUTPUT = \"gbt.txt\"\n",
    "TITLEOUTPUT = \"title.txt\"\n",
    "full_keys = [\"author\",\"title\",\"journal\",\"year\",\"DOI\",\"month\",\"citations(google scholar)\",\"abstract\",\"keywords\",\"reference_count\",\"ccfClass\",\"important\",\"references\"]\n",
    "full_keys_default = {\"author\":[],\"title\":\"\",\"journal\":\"\",\"year\":0,\"DOI\":\"\",\"month\":0,\"citations(google scholar)\":-1,\"abstract\":\"\",\"keywords\":[],\"reference_count\":0,\"ccfClass\":\"\",\"important\":None,\"references\":[]}\n",
    "# 常用函数定义\n",
    "# 保存至transform\n",
    "def save_tran_json(addr,jsonfile):\n",
    "    with open(TRANSFORMED_DIR + addr, 'w') as o:\n",
    "        o.write(json.dumps(jsonfile,ensure_ascii=False))\n",
    "        \n",
    "def save_tran_plain(addr,txtfile):\n",
    "    with open(TRANSFORMED_DIR + addr, 'w') as o:\n",
    "        o.write(txtfile)\n",
    "        \n",
    "def save_tran_lines(addr,txtfile):\n",
    "    with open(TRANSFORMED_DIR + addr, 'a+') as o:\n",
    "        for line in txtfile:\n",
    "            o.write(line+'\\n')    \n",
    "# 从transformd中读取\n",
    "def read_tran_json(addr):\n",
    "    with open(TRANSFORMED_DIR + addr,encoding = 'utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "    \n",
    "#  查询ccf等级\n",
    "def search_ccf(s):\n",
    "    ccf_all_addr = \"/Users/Halloween/Desktop/Study/复杂网络/lrcns/raw-data/ccf/ccf_all.csv\"\n",
    "\n",
    "    ccf_all = []\n",
    "    ccf_search_res = {}\n",
    "    with open(ccf_all_addr,encoding = 'utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            ccf_all.append(line)\n",
    "    level = \"\"\n",
    "    field = \"\"\n",
    "    cORj = \"\"\n",
    "    col = -1\n",
    "#     print(ccf_all)\n",
    "    for line in reversed(ccf_all):\n",
    "        if col != 1 and col != 2 and col != -1:\n",
    "            col = -1\n",
    "            continue\n",
    "        if col == -1:\n",
    "            for word in line[1:3]:\n",
    "                if word == \"\":\n",
    "                    continue\n",
    "                if(string_similar(word,s)>0.9):\n",
    "                    col = line.index(word)\n",
    "                    ccf_search_res['ccf_search_res'] = word\n",
    "#                     print('\"'+word.strip('\"')+'\"'+'\"'+s.strip('\"')+'\"')\n",
    "#                     print(col)\n",
    "#                     print(line)\n",
    "                    break\n",
    "        else:\n",
    "            if len(line[0])>1 and line[0][1]=='、' and level == \"\":\n",
    "                level = line[0][2]\n",
    "#                 print(level)\n",
    "            if len(line[0])>10 and line[0][:2]==\"中国\" and field == \"\":\n",
    "                field = line[0][line[0].find(\"（\")+1:line[0].find(\"）\")]\n",
    "                cORj = line[0][line[0].find(\"术\")+1:line[0].find(\"（\")]\n",
    "#                 print(field)\n",
    "    ccf_search_res.update({\"level\":level,\"field\":field,\"type\":cORj})\n",
    "    return {\"level\":level,\"field\":field,\"type\":cORj}\n",
    "# 数据审查函数\n",
    "def inspect(to_inspect):\n",
    "    papers_arr = to_inspect\n",
    "    all_error = 0\n",
    "    for paper in papers_arr:\n",
    "        inspect = []\n",
    "        for key in full_keys:\n",
    "            if(key not in paper.keys() or type(paper[key]) != type(full_keys_default[key]) or paper[key] == full_keys_default[key]):\n",
    "                inspect.append(key + \" lost\")\n",
    "                continue\n",
    "            if(key == \"author\" or key == \"keywords\" or key == \"references\"):\n",
    "                if(type(paper[key]) != list or len(paper[key])==0):\n",
    "                    inspect.append(key + \" lost\")\n",
    "                    continue\n",
    "            if(key == \"year\"):\n",
    "                if(paper[key]<1900):\n",
    "                    inspect.append(key + \" error\")\n",
    "            if(key == \"month\"):\n",
    "                if(paper[key]<0 or paper[key] >12):\n",
    "                    inspect.append(key + \" error\")\n",
    "            if(key == \"citations(google scholar)\" or key == \"reference_count\"):\n",
    "                if(paper[key]<0):\n",
    "                    inspect.append(key + \" error\")\n",
    "            if(key == 'ccfClass'):\n",
    "                if(paper[key] not in ['A','B','C']):\n",
    "                    inspect.append(key + \" error\")\n",
    "            if(type(paper[key])==int):\n",
    "                if(paper[key]<0):\n",
    "                    inspect.append(key + \" less than 0\")\n",
    "                continue\n",
    "            if(type(paper[key])==float):\n",
    "                inspect.append(key + \" not int\")\n",
    "                continue\n",
    "        paper['inspect'] = list(inspect)\n",
    "        print(paper['title']+str(paper['inspect']))\n",
    "        all_error += len(paper['inspect'])\n",
    "    print(\">>>>SUMMARY ERRORs:\"+str(all_error)+\"<<<<<\")\n",
    "    return papers_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse sth like DOI [DOI 10.****] or DOI DOI 10.***\n",
    "def parseDOI(doi):\n",
    "    return [d.strip(' ').strip('[').strip(']').strip('DOI ').lower() for d in doi.strip(' ').strip('[').strip(']').strip('DOI ').split(',')]\n",
    "\n",
    "def parseWoS(PATH,DEST):\n",
    "    # \"key\":[type,de]\n",
    "    FIELD_TYPE = {\"PT\":[str],\"AU\":[list,'\\n'],\"AF\":[list,'\\n'],\"BA\":[list,'\\n'],\"BF\":[list,'\\n'],\"CA\":[list,'\\n'],\"GP\":[list,'\\n'],\"BE\":[list,'\\n'],\"TI\":[str],\"SO\":[str],\"SE\":[str],\"BS\":[str],\"LA\":[str],\"DT\":[str],\"CT\":[str],\"CY\":[str],\"CL\":[str],\"SP\":[list,'\\n'],\"HO\":[str],\"DE\":[list,';'],\"ID\":[list,';'],\"AB\":[str],\"C1\":[str],\"RP\":[str],\"EM\":[list,';'],\"RI\":[list,';'],\"OI\":[list,';'],\"FU\":[list,';'],\"FX\":[str],\"CR\":[list,'\\n'],\"NR\":[int],\"TC\":[int],\"Z9\":[int],\"U1\":[int],\"U2\":[int],\"PU\":[list,','],\"PI\":[str],\"PA\":[str],\"SN\":[str],\"EI\":[str],\"BN\":[str],\"J9\":[str],\"JI\":[str],\"PD\":[str],\"PY\":[int],\"VL\":[str],\"IS\":[str],\"SI\":[str],\"PN\":[str],\"SU\":[str],\"MA\":[str],\"BP\":[str],\"EP\":[str],\"AR\":[str],\"DI\":[str],\"D2\":[str],\"EA\":[str],\"EY\":[str],\"PG\":[str],\"P2\":[str],\"WC\":[list,';'],\"SC\":[list,';'],\"GA\":[str],\"PM\":[str],\"UT\":[str],\"OA\":[str],\"HP\":[str],\"HC\":[str],\"DA\":[str]}\n",
    "    # 清空文件\n",
    "    open(DEST,'w').close()\n",
    "    wos_arr = []\n",
    "    addr_list = []\n",
    "    for addr in os.walk(PATH):\n",
    "        for fname in addr[2]:\n",
    "            if(fname[0]!='.' and fname.split('.')[-1]==\"txt\"):\n",
    "                addr_list.append(str(addr[0]+\"/\"+fname))\n",
    "\n",
    "                \n",
    "    for addr in addr_list:\n",
    "        print(addr)\n",
    "#         单文件最多500条，可以读入内存\n",
    "        with open(addr,encoding = 'utf-8-sig', errors = 'ignore') as f:\n",
    "            fstrs = f.readlines()  \n",
    "        tg_key = {}        \n",
    "        tag = \"\"\n",
    "        tag_type = [\"\"]\n",
    "        for line in fstrs:\n",
    "            if(line[:2] == \"EF\"):\n",
    "#                 结束本文件\n",
    "                break\n",
    "            elif(line[:2] == \"ER\"):\n",
    "#                 if('WC' in tg_key.keys()):\n",
    "#                     print(tg_key['WC'])\n",
    "#             存放记录\n",
    "                wos_arr.append(dict(tg_key))\n",
    "                tg_key = {}        \n",
    "                tag = \"\"\n",
    "                tag_type = [\"\"]\n",
    "            elif(line[:2] != \"  \" and line[:2] in FIELD_TYPE.keys()):\n",
    "#                 找到一个新filed\n",
    "                if(tag_type[0]==list):\n",
    "#                 上一个\n",
    "                    tg_key[tag] = [x.replace('\\n','').strip(' ') for x in list(tg_key[tag].strip('\\n').split(tag_type[1]))]\n",
    "#                     if(tag == 'WC'):\n",
    "#                         print(tg_key[tag])\n",
    "# #                         print(tg_key['WC'])\n",
    "                if 'CR' == tag:\n",
    "                    tg_key_CR = []\n",
    "                    for cr in tg_key['CR']:\n",
    "                        DOI_loc = cr.find(\"DOI\")\n",
    "                        if (DOI_loc != -1):\n",
    "                            ref = cr[:DOI_loc].strip(', ')\n",
    "                            DOI = parseDOI(cr[DOI_loc:])\n",
    "                            tg_key_CR.append({\"ref\":ref,\"DOI\":DOI})\n",
    "                        else:\n",
    "                            tg_key_CR.append({\"ref\": cr,\"DOI\":[\"\"]})\n",
    "                    tg_key['CR'] = tg_key_CR\n",
    "    \n",
    "                tag = line[:2]\n",
    "                tag_type = FIELD_TYPE[tag]\n",
    "                if(tag_type[0]==str):\n",
    "                    tg_key[tag] = \"\"\n",
    "                    if(tag==\"DI\"):\n",
    "                        tg_key[tag] += line[3:].lower().strip('\\n')\n",
    "                    else:\n",
    "                        tg_key[tag] += line[3:].strip('\\n')\n",
    "                elif(tag_type[0]==int):\n",
    "                    tg_key[tag] = int(line[3:].strip('\\n'))\n",
    "                elif(tag_type[0]==list):\n",
    "                    tg_key[tag] = \"\"\n",
    "                    tg_key[tag] += line[3:]\n",
    "                    if (tag_type[1] != '\\n'):\n",
    "                        tg_key[tag] = tg_key[tag].strip('\\n')\n",
    "            elif(line[:2] == \"  \"):\n",
    "#                 追加\n",
    "\n",
    "                if(tag_type[0]==str):\n",
    "                    tg_key[tag] += ' '\n",
    "                    tg_key[tag] += line[3:].strip('\\n')\n",
    "                elif(tag_type[0]==list):\n",
    "                    if (tag_type[1] != '\\n'):\n",
    "                        tg_key[tag] += ' '\n",
    "                        tg_key[tag] = tg_key[tag].strip('\\n')\n",
    "                    tg_key[tag] += line[3:]\n",
    "\n",
    "    print(len(wos_arr))\n",
    "    with open(DEST,'a+') as o:\n",
    "        o.write(json.dumps(wos_arr, ensure_ascii=False))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "/Users/Halloween/Downloads/saved500/savedrecs (1).txt\n/Users/Halloween/Downloads/saved500/savedrecs (11).txt\n/Users/Halloween/Downloads/saved500/savedrecs (10).txt\n/Users/Halloween/Downloads/saved500/savedrecs (7).txt\n/Users/Halloween/Downloads/saved500/savedrecs (6).txt\n/Users/Halloween/Downloads/saved500/savedrecs .txt\n/Users/Halloween/Downloads/saved500/savedrecs (9).txt\n/Users/Halloween/Downloads/saved500/savedrecs (5).txt\n/Users/Halloween/Downloads/saved500/savedrecs (4).txt\n/Users/Halloween/Downloads/saved500/savedrecs (8).txt\n/Users/Halloween/Downloads/saved500/savedrecs (3).txt\n/Users/Halloween/Downloads/saved500/savedrecs (2).txt\n5600\n"
    }
   ],
   "source": [
    "parseWoS(\"/Users/Halloween/Downloads/saved500\",\"/Users/Halloween/Desktop/Study/复杂网络/lrcns/raw-data/transformed/wos2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "5600\n"
    }
   ],
   "source": [
    "wos_arr = read_tran_json('wos2.txt')\n",
    "print(len(wos_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[{'ref': 'Adamic LA, 2003, SOC NETWORKS, V25, P211', 'DOI': ['10.1016/s0378-8733(03)00009-1']}, {'ref': 'Akoglu L, 2015, DATA MIN KNOWL DISC, V29, P626', 'DOI': ['10.1007/s10618-014-0365-y']}, {'ref': 'Bayati M, 2009, IEEE DATA MINING, P705', 'DOI': ['10.1109/icdm.2009.135']}, {'ref': 'Belkin M, 2002, ADV NEUR IN, V14, P585', 'DOI': ['']}, {'ref': 'Belkin M, 2003, NEURAL COMPUT, V15, P1373', 'DOI': ['10.1162/089976603321780317']}, {'ref': 'Benson AR, 2016, SCIENCE, V353, P163', 'DOI': ['10.1126/science.aad9029']}, {'ref': 'Berline N., 2003, HEAT KERNELS DIRAC O', 'DOI': ['']}, {'ref': 'Bollacker K, 2008, P 2008 ACM SIGMOD IN, P1247', 'DOI': ['10.1145/1376616.1376746', '10.1145/1376616.1376746']}, {'ref': \"Bourigault S, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P393\", 'DOI': ['10.1145/2556195.2556216']}, {'ref': 'Breitkreutz BJ, 2008, NUCLEIC ACIDS RES, V36, pD637', 'DOI': ['10.1093/nar/gkm1001']}, {'ref': 'Burt RS, 2004, AM J SOCIOL, V110, P349', 'DOI': ['10.1086/421787']}, {'ref': 'Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548', 'DOI': ['10.1109/tpami.2010.231']}, {'ref': 'Cao S., 2015, P 24 ACM INT C INF K, P891', 'DOI': ['10.1145/2806416.2806512']}, {'ref': 'Cao SS, 2016, THIRTIETH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1145', 'DOI': ['']}, {'ref': 'CARTWRIGHT D, 1956, PSYCHOL REV, V63, P277', 'DOI': ['10.1037/h0046049']}, {'ref': 'Chang J., 2009, INT C ART INT STAT, P81', 'DOI': ['']}, {'ref': \"Chang SY, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P119\", 'DOI': ['10.1145/2783258.2783296']}, {'ref': 'Chen Siheng, 2017, ARXIV170205764', 'DOI': ['']}, {'ref': \"Chen T, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P295\", 'DOI': ['10.1145/3018661.3018735']}, {'ref': 'Chua T.-S., 2009, P ACM INT C IM VID R, P48', 'DOI': ['10.1145/1646396.1646452']}, {'ref': 'Chung F. R., 1997, SPECTRAL GRAPH THEOR', 'DOI': ['']}, {'ref': 'Cygan M, 2015, THEOR COMPUT SYST, V56, P394', 'DOI': ['10.1007/s00224-014-9558-4']}, {'ref': 'De Choudhury M., 2010, ICWSM, P34', 'DOI': ['']}, {'ref': 'Fan RE, 2008, J MACH LEARN RES, V9, P1871', 'DOI': ['']}, {'ref': 'Fu Y, 2012, GRAPH EMBEDDING PATT', 'DOI': ['']}, {'ref': 'Getoor L., 2005, ACM SIGKDD EXPLORATI, V7, P3', 'DOI': ['10.1145/1117454.1117456']}, {'ref': 'Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821', 'DOI': ['10.1073/pnas.122653799']}, {'ref': 'Grover A, 2016, P 22 ACM SIGKDD INT, P1225', 'DOI': ['']}, {'ref': 'Guille A, 2013, SIGMOD REC, V42, P17', 'DOI': ['']}, {'ref': 'He XF, 2004, ADV NEUR IN, V16, P153', 'DOI': ['']}, {'ref': 'Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18', 'DOI': ['10.1109/5254.708428']}, {'ref': 'Herman I, 2000, IEEE T VIS COMPUT GR, V6, P24', 'DOI': ['10.1109/2945.841119']}, {'ref': 'Hochreiter S, 1997, NEURAL COMPUT, V9, P1735', 'DOI': ['10.1162/neco.1997.9.8.1735']}, {'ref': 'Hu RJ, 2016, PROC INT CONF DATA, P385', 'DOI': ['10.1109/icde.2016.7498256']}, {'ref': \"Huang H, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P499\", 'DOI': ['10.1145/2567948.2576940']}, {'ref': 'Huang X., 2017, P 2017 SIAM INT C DA, P633', 'DOI': ['10.1137/1.9781611974973.71']}, {'ref': \"Huang X, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P731\", 'DOI': ['10.1145/3018661.3018667']}, {'ref': 'Huang Z., 2017, ARXIV170105291', 'DOI': ['']}, {'ref': \"Huang ZP, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1595\", 'DOI': ['10.1145/2939672.2939815']}, {'ref': \"Jacob Y, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P373\", 'DOI': ['10.1145/2556195.2556225']}, {'ref': 'KATZ L, 1953, PSYCHOMETRIKA, V18, P39', 'DOI': ['']}, {'ref': 'Krioukov D, 2010, PHYS REV E, V82', 'DOI': ['10.1103/physreve.82.036106']}, {'ref': 'LeCun Y, 2015, NATURE, V521, P436', 'DOI': ['10.1038/nature14539']}, {'ref': 'Lee DD, 2001, ADV NEUR IN, V13, P556', 'DOI': ['']}, {'ref': 'Leskovec J., 2016, SNAP DATASETS STANFO', 'DOI': ['']}, {'ref': 'Leskovec J, 2007, ACM T WEB, V1', 'DOI': ['10.1145/1232722.1232727']}, {'ref': 'Levy O., 2014, ADV NEURAL INFORM PR, P2177', 'DOI': ['10.1162/153244303322533223']}, {'ref': \"Li C, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P577\", 'DOI': ['10.1145/3038912.3052643']}, {'ref': 'Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019', 'DOI': ['10.1002/asi.20591']}, {'ref': 'Lim K. W., 2016, ARXIV160906826', 'DOI': ['']}, {'ref': 'Lu LY, 2011, PHYSICA A, V390, P1150', 'DOI': ['10.1016/j.physa.2010.11.027']}, {'ref': 'MacQueen J., 1967, P 5 BERK S MATH STAT, V1, P281', 'DOI': ['10.1234/12345678']}, {'ref': 'Mahoney Matt, 2011, LARGE TEXT COMPRESSI', 'DOI': ['']}, {'ref': 'Man T., 2016, P 25 INT JOINT C ART, P1823', 'DOI': ['']}, {'ref': 'McCallum AK, 2000, INFORM RETRIEVAL, V3, P127', 'DOI': ['10.1023/a:1009953814988']}, {'ref': 'Mikolov T., 2013, P ICLR WORKSH TRACK', 'DOI': ['']}, {'ref': 'Mikolov T, 2013, ADV NEURAL INFORM PR, P3111', 'DOI': ['10.1162/jmlr.2003.3.4-5.951']}, {'ref': 'Mikolov T, 2010, NTFRSPEECH, V2, P3', 'DOI': ['']}, {'ref': 'Natarajan N, 2014, BIOINFORMATICS, V30, P60', 'DOI': ['10.1093/bioinformatics/btu269']}, {'ref': 'Newman MEJ, 2006, PHYS REV E, V74', 'DOI': ['10.1103/physreve.74.036104']}, {'ref': 'Niepert Mathias, 2016, INT C MACH LEARN, P2014', 'DOI': ['']}, {'ref': 'Ou M, 2016, P 22 ACM SIGKDD INT, P672', 'DOI': ['']}, {'ref': \"Ou MD, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P895\", 'DOI': ['10.1145/2783258.2783283']}, {'ref': 'PAIGE CC, 1981, SIAM J NUMER ANAL, V18, P398', 'DOI': ['10.1137/0718026']}, {'ref': 'Pan S., 2016, NETWORK, V11, P12', 'DOI': ['']}, {'ref': 'Perozzi B., 2014, KDD, P701', 'DOI': ['10.1145/2623330.2623732']}, {'ref': 'Roweis ST, 2000, SCIENCE, V290, P2323', 'DOI': ['10.1126/science.290.5500.2323']}, {'ref': 'Ruck D W, 1990, IEEE Trans Neural Netw, V1, P296', 'DOI': ['10.1109/72.80266']}, {'ref': 'Sen P, 2008, AI MAG, V29, P93', 'DOI': ['10.1609/aimag.v29i3.2157']}, {'ref': 'Seo E., 2012, SPIE DEFENSE SECURIT', 'DOI': ['']}, {'ref': 'Staudt CL, 2016, NETW SCI, V4, P508', 'DOI': ['10.1017/nws.2016.20']}, {'ref': 'Sun Xiaofei, 2016, ARXIV161002906', 'DOI': ['']}, {'ref': 'Sunt YZ, 2011, PROC VLDB ENDOW, V4, P992', 'DOI': ['']}, {'ref': 'Tang J, 2012, P 5 ACM INT C WEB SE, P743', 'DOI': ['http://doi.acm.org/10.1145/2124295.2124382', '10.1145/2124295.2124382']}, {'ref': 'Tang J., 2008, P 14 ACM SIGKDD INT, P990', 'DOI': ['10.1145/1401890.1402008']}, {'ref': 'Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067', 'DOI': ['10.1145/2736277.2741093']}, {'ref': 'Tang L, 2009, P 18 ACM C INF KNOWL, P1107', 'DOI': ['10.1145/1645953.1646094']}, {'ref': 'Tang L, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P817', 'DOI': ['']}, {'ref': 'Tenenbaum JB, 2000, SCIENCE, V290, P2319', 'DOI': ['10.1126/science.290.5500.2319']}, {'ref': 'Tian F, 2014, PROCEEDINGS OF THE TWENTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1293', 'DOI': ['']}, {'ref': 'Traud AL, 2012, PHYSICA A, V391, P4165', 'DOI': ['10.1016/j.physa.2011.12.021']}, {'ref': 'Tu C-c, 2016, IJCAI, P3889', 'DOI': ['']}, {'ref': 'van der Maaten L, 2008, J MACH LEARN RES, V9, P2579', 'DOI': ['']}, {'ref': 'Vincent P, 2010, J MACH LEARN RES, V11, P3371', 'DOI': ['']}, {'ref': \"Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225\", 'DOI': ['10.1145/2939672.2939753']}, {'ref': 'Wang S., 2017, P 2017 SIAM INT C DA, P327', 'DOI': ['']}, {'ref': 'Wang X, 2017, THIRTY-FIRST AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P203', 'DOI': ['']}, {'ref': \"Wei XK, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P1611\", 'DOI': ['10.1145/3038912.3052575']}, {'ref': \"Xu LC, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P741\", 'DOI': ['10.1145/3018661.3018723']}, {'ref': 'Yan SC, 2005, PROC CVPR IEEE, P830', 'DOI': ['']}, {'ref': \"Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365\", 'DOI': ['10.1145/2783258.2783417']}, {'ref': 'Yang C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2111', 'DOI': ['']}, {'ref': 'Yang XS, 2017, INT CONF ACOUST SPEE, P5690', 'DOI': ['10.1109/icassp.2017.7953246']}, {'ref': 'Yeung S, 2016, PROC CVPR IEEE, P2678', 'DOI': ['10.1109/cvpr.2016.293']}, {'ref': 'Zhang Q, 2015, LECT NOTES ARTIF INT, V9362, P113', 'DOI': ['10.1007/978-3-319-25207-0_10']}, {'ref': '2014, IEEE DATA MINING, P270', 'DOI': ['10.1109/icdm.2014.119']}]\n"
    }
   ],
   "source": [
    "print(wos_arr[0]['CR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "3735\n"
    }
   ],
   "source": [
    "# 编号\n",
    "# 所有paper 含有 title 以此编号，再将编号对应doi\n",
    "# 识别时，通过TI 识别paper\n",
    "ti_set = set([x['TI'] for x in wos_arr if 'TI' in x.keys()])\n",
    "ti_to_paper = {x['TI']:x for x in wos_arr if 'TI' in x.keys()}\n",
    "idx_to_ti = {i:t for i,t in enumerate(title_set)}\n",
    "ti_to_idx = {t:i for i,t in enumerate(title_set)}\n",
    "# DOI 根据title编号\n",
    "doi_set = set([x['DI'] for x in wos_arr if 'DI' in x.keys()])\n",
    "\n",
    "doi_to_paper = {x['DI']:x for x in wos_arr if 'DI' in x.keys()}\n",
    "idx_to_doi = {ti_to_idx[paper['TI']]:paper['DI'] for paper in wos_arr if 'DI' in paper.keys() }\n",
    "doi_to_idx = {paper['DI']:ti_to_idx[paper['TI']] for paper in wos_arr if 'DI' in paper.keys() }\n",
    "\n",
    "print(len(idx_to_doi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "15309\n"
    }
   ],
   "source": [
    "# author 编号\n",
    "author_set = set([author for paper in wos_arr if 'AF' in paper.keys() for author in paper['AF'] ])\n",
    "idx_to_af = {i:a for i,a in enumerate(author_set)}\n",
    "af_to_idx = {a:i for i,a in enumerate(author_set)}\n",
    "print(len(idx_to_af))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "3743\n65000\n61257\n62237\n"
    }
   ],
   "source": [
    "# 只在引文ref rdoi (all doi) \n",
    "rdoi_set = {ref['DOI'][0] for paper in wos_arr if 'CR' in paper.keys() for ref in paper['CR'] if ref['DOI'][0] != \"\" and ref['DOI'][0] not in doi_set}\n",
    "idx_to_rdoi = {i+len(ti_set):t for i,t in enumerate(rdoi_set)}\n",
    "rdoi_to_idx = {t:i+len(ti_set) for i,t in enumerate(rdoi_set)}\n",
    "# 所有doi\n",
    "adoi_set = rdoi_set.union(doi_set)\n",
    "idx_to_adoi = dict()\n",
    "idx_to_adoi.update(idx_to_rdoi)\n",
    "idx_to_adoi.update(idx_to_doi)\n",
    "adoi_to_idx = dict()\n",
    "adoi_to_idx.update(doi_to_idx)\n",
    "adoi_to_idx.update(rdoi_to_idx)\n",
    "print(len(doi_set))\n",
    "print(len(adoi_set))\n",
    "print(len(rdoi_set))\n",
    "print(len({ref['DOI'][0] for paper in wos_arr if 'CR' in paper.keys() for ref in paper['CR'] if ref['DOI'][0] != \"\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(5541, 15309)\n"
    }
   ],
   "source": [
    "# 构造矩阵\n",
    "WA = np.zeros((len(idx_to_ti),len(idx_to_af)))\n",
    "for paper in wos_arr:\n",
    "    if 'AF' in paper.keys():\n",
    "        for author in paper['AF']:\n",
    "            WA[ti_to_idx[paper['TI']],af_to_idx[author]] = 1\n",
    "print(WA.shape)\n",
    "AW = WA.T            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(5541, 66798)\n"
    }
   ],
   "source": [
    "# (文章数，文章数+非文章DOI的引用DOI)\n",
    "WR = np.zeros((len(idx_to_ti),len(idx_to_ti) + len(idx_to_rdoi)))\n",
    "for paper in wos_arr:\n",
    "    if 'CR' in paper.keys():\n",
    "        for ref in paper['CR']:\n",
    "            if ref['DOI'][0] != \"\" :\n",
    "                if ref['DOI'][0] in rdoi_to_idx.keys():\n",
    "                    WR[ti_to_idx[paper['TI']],rdoi_to_idx[ref['DOI'][0]]] = 1\n",
    "                elif ref['DOI'][0] in doi_to_idx.keys():\n",
    "                    WR[ti_to_idx[paper['TI']],doi_to_idx[ref['DOI'][0]]] = 1\n",
    "print(WR.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(5541, 5541)\n"
    }
   ],
   "source": [
    "WRW = np.dot(WR,WR.T)\n",
    "print(WRW.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_numpy_matrix(WRW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(TRANSFORMED_DIR+'matrix.csv',WRW,fmt=\"%d\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0, 1, 1], [1, 0, 0]]\n"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(5541, 5541)\n"
    }
   ],
   "source": [
    "WAW = np.dot(WA,AW)\n",
    "print(WAW.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[1. 0. 0. ... 0. 0. 0.]\n [0. 2. 0. ... 0. 0. 0.]\n [0. 0. 3. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 5. 0. 0.]\n [0. 0. 0. ... 0. 6. 0.]\n [0. 0. 0. ... 0. 0. 4.]]\n"
    }
   ],
   "source": [
    "print(WW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_set = set([x['DI'] for x in wos_arr if 'DI' in x.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx_to_doi)\n",
    "print(doi_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doi_to_paper[idx_to_doi[1]])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_set = set()\n",
    "wc_dict = {}\n",
    "for paper in wos_arr:\n",
    "    if('WC' in paper.keys()):\n",
    "#         wc_set.update(paper['WC'])\n",
    "        for wc in paper['WC']:\n",
    "            if(wc not in wc_dict):\n",
    "                wc_dict[wc] = 1\n",
    "            else:\n",
    "                wc_dict[wc] += 1\n",
    "print(wc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in wos_arr:\n",
    "    if(\"DI\" in paper.keys()):\n",
    "        print(paper[\"WC\"])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_cnt = 0\n",
    "for paper in wos_arr:\n",
    "    if(\"DI\" in paper.keys()):\n",
    "        print(paper['DI'],end=\"\")\n",
    "        if(\"CR\" in paper.keys()):\n",
    "            for ref in paper['CR']:\n",
    "                if(ref[\"DOI\"][0]!=\"\"):\n",
    "                    print(\";\"+ref[\"DOI\"][0],end = \"\")\n",
    "        print(\"\")\n",
    "        \n",
    "print(doi_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for paper in wos_arr:\n",
    "    try:\n",
    "        for sentence in (paper['AB'].split('.')):\n",
    "            if \"propose\" in sentence:\n",
    "                print(sentence)\n",
    "    except:\n",
    "        cnt += 1\n",
    "        \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}