{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import requests\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "RAW_DIR = \"../raw/\"\n",
    "LIST_RAW_CSV = \"../list_raw/all.csv\"\n",
    "LIST_RAW_JSON = \"../transformed/all_json.json\"\n",
    "TRANSFORMED_DIR = \"../transformed/\"\n",
    "OUTPUT = \"output.txt\"\n",
    "GBTOUTPUT = \"gbt.txt\"\n",
    "TITLEOUTPUT = \"title.txt\"\n",
    "full_keys = [\"author\",\"title\",\"journal\",\"year\",\"DOI\",\"month\",\"citations(google scholar)\",\"abstract\",\"keywords\",\"reference_count\",\"ccfClass\",\"important\",\"references\"]\n",
    "full_keys_default = {\"author\":[],\"title\":\"\",\"journal\":\"\",\"year\":0,\"DOI\":\"\",\"month\":0,\"citations(google scholar)\":-1,\"abstract\":\"\",\"keywords\":[],\"reference_count\":0,\"ccfClass\":\"\",\"important\":None,\"references\":[]}\n",
    "# 常用函数定义\n",
    "# 调用crossref接口\n",
    "def string_similar(s1, s2):\n",
    "    return difflib.SequenceMatcher(None, s1, s2).ratio()\n",
    "def get_crossref_info(ref):\n",
    "    url = \"https://doi.crossref.org/servlet/query\"\n",
    "\n",
    "    querystring = {\"usr\":\"halloweenwx@163.com\",\"pwd\":\"halloween\",\"format\":\"json\",\"qdata\":\"\"\"<?xml version = \"1.0\" encoding=\"UTF-8\"?>\n",
    "    <query_batch version=\"2.0\" xmlns = \"http://www.crossref.org/qschema/2.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n",
    "      <head>\n",
    "          <email_address>hisham@atypon.com</email_address>\n",
    "          <doi_batch_id>Sample multi resolve</doi_batch_id>\n",
    "      </head>\n",
    "      <body>\n",
    "           <query key=\"mykey\" enable-multiple-hits=\"true\">\n",
    "      <unstructured_citation>\"\"\"+ref+\"\"\"</unstructured_citation>\n",
    "    </query>\n",
    "         </body>\n",
    "    </query_batch>\"\"\"}\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': \"PostmanRuntime/7.20.1\",\n",
    "        'Accept': \"*/*\",\n",
    "        'Cache-Control': \"no-cache\",\n",
    "        'Postman-Token': \"123bedd6-8fb9-43e5-980a-54336e7aa684,6312f073-8e24-4222-abb2-789899bbc01f\",\n",
    "        'Host': \"doi.crossref.org\",\n",
    "        'Accept-Encoding': \"gzip, deflate\",\n",
    "        'Connection': \"keep-alive\",\n",
    "        'cache-control': \"no-cache\"\n",
    "        }\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "#     print(response.text)\n",
    "    return response.text\n",
    "\n",
    "# 保存至transform\n",
    "def save_tran_json(addr,jsonfile):\n",
    "    with open(TRANSFORMED_DIR + addr, 'w') as o:\n",
    "        o.write(json.dumps(jsonfile,ensure_ascii=False))\n",
    "        \n",
    "def save_tran_plain(addr,txtfile):\n",
    "    with open(TRANSFORMED_DIR + addr, 'w') as o:\n",
    "        o.write(txtfile)\n",
    "        \n",
    "def save_tran_lines(addr,txtfile):\n",
    "    with open(TRANSFORMED_DIR + addr, 'a+') as o:\n",
    "        for line in txtfile:\n",
    "            o.write(line+'\\n')    \n",
    "# 从transformd中读取\n",
    "def read_tran_json(addr):\n",
    "    with open(TRANSFORMED_DIR + addr,encoding = 'utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "    \n",
    "#  查询ccf等级\n",
    "def search_ccf(s):\n",
    "    ccf_all_addr = \"/Users/Halloween/Desktop/Study/复杂网络/lrcns/raw-data/ccf/ccf_all.csv\"\n",
    "\n",
    "    ccf_all = []\n",
    "    ccf_search_res = {}\n",
    "    with open(ccf_all_addr,encoding = 'utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            ccf_all.append(line)\n",
    "    level = \"\"\n",
    "    field = \"\"\n",
    "    cORj = \"\"\n",
    "    col = -1\n",
    "#     print(ccf_all)\n",
    "    for line in reversed(ccf_all):\n",
    "        if col != 1 and col != 2 and col != -1:\n",
    "            col = -1\n",
    "            continue\n",
    "        if col == -1:\n",
    "            for word in line[1:3]:\n",
    "                if word == \"\":\n",
    "                    continue\n",
    "                if(string_similar(word,s)>0.9):\n",
    "                    col = line.index(word)\n",
    "                    ccf_search_res['ccf_search_res'] = word\n",
    "#                     print('\"'+word.strip('\"')+'\"'+'\"'+s.strip('\"')+'\"')\n",
    "#                     print(col)\n",
    "#                     print(line)\n",
    "                    break\n",
    "        else:\n",
    "            if len(line[0])>1 and line[0][1]=='、' and level == \"\":\n",
    "                level = line[0][2]\n",
    "#                 print(level)\n",
    "            if len(line[0])>10 and line[0][:2]==\"中国\" and field == \"\":\n",
    "                field = line[0][line[0].find(\"（\")+1:line[0].find(\"）\")]\n",
    "                cORj = line[0][line[0].find(\"术\")+1:line[0].find(\"（\")]\n",
    "#                 print(field)\n",
    "    ccf_search_res.update({\"level\":level,\"field\":field,\"type\":cORj})\n",
    "    return {\"level\":level,\"field\":field,\"type\":cORj}\n",
    "# 数据审查函数\n",
    "def inspect(to_inspect):\n",
    "    papers_arr = to_inspect\n",
    "    all_error = 0\n",
    "    for paper in papers_arr:\n",
    "        inspect = []\n",
    "        for key in full_keys:\n",
    "            if(key not in paper.keys() or type(paper[key]) != type(full_keys_default[key]) or paper[key] == full_keys_default[key]):\n",
    "                inspect.append(key + \" lost\")\n",
    "                continue\n",
    "            if(key == \"author\" or key == \"keywords\" or key == \"references\"):\n",
    "                if(type(paper[key]) != list or len(paper[key])==0):\n",
    "                    inspect.append(key + \" lost\")\n",
    "                    continue\n",
    "            if(key == \"year\"):\n",
    "                if(paper[key]<1900):\n",
    "                    inspect.append(key + \" error\")\n",
    "            if(key == \"month\"):\n",
    "                if(paper[key]<0 or paper[key] >12):\n",
    "                    inspect.append(key + \" error\")\n",
    "            if(key == \"citations(google scholar)\" or key == \"reference_count\"):\n",
    "                if(paper[key]<0):\n",
    "                    inspect.append(key + \" error\")\n",
    "            if(key == 'ccfClass'):\n",
    "                if(paper[key] not in ['A','B','C']):\n",
    "                    inspect.append(key + \" error\")\n",
    "            if(type(paper[key])==int):\n",
    "                if(paper[key]<0):\n",
    "                    inspect.append(key + \" less than 0\")\n",
    "                continue\n",
    "            if(type(paper[key])==float):\n",
    "                inspect.append(key + \" not int\")\n",
    "                continue\n",
    "        paper['inspect'] = list(inspect)\n",
    "        print(paper['title']+str(paper['inspect']))\n",
    "        all_error += len(paper['inspect'])\n",
    "    print(\">>>>SUMMARY ERRORs:\"+str(all_error)+\"<<<<<\")\n",
    "    return papers_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '100001334', 'title': 'Ontologies in HYDRA - Middleware for Ambient Intelligent Devices.', 'authors': [{'name': 'Peter Kostelnik', 'id': '2702511795'}, {'name': 'Martin Sarnovsky', 'id': '2041014688'}, {'name': 'Jan Hreno', 'id': '2398560122'}], 'venue': {'raw': 'AMIF'}, 'year': 2009, 'n_citation': 2, 'page_start': '43', 'page_end': '46', 'doc_type': '', 'publisher': '', 'volume': '', 'issue': '', 'fos': [{'name': 'Lernaean Hydra', 'w': 0.4178039}, {'name': 'Database', 'w': 0.4269269}, {'name': 'World Wide Web', 'w': 0.415332377}, {'name': 'Ontology (information science)', 'w': 0.459045082}, {'name': 'Computer science', 'w': 0.399807781}, {'name': 'Middleware', 'w': 0.5905041}, {'name': 'Ambient intelligence', 'w': 0.5440575}]}\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Volumes/temp/dblp_papers_v11.txt\",encoding = 'utf-8') as f:\n",
    "    paper = json.loads(f.readline())\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
