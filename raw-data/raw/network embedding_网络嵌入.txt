[{"author": ["Man T", "Shen H", "Liu S"], "title": "Understanding Attention and Generalization in Graph Neural Networks", "journal": "IJCAI. 2016", "year": 2016, "DOI": "0", "month": 0, "citations(google scholar)": 54, "abstract": "We aim to better understand attention over nodes in graph neural networks (GNNs) and identify factors influencing its effectiveness. We particularly focus on the ability of attention GNNs to generalize to larger, more complex or noisy graphs. Motivated by insights from the work on Graph Isomorphism Networks, we design simple graph reasoning tasks that allow us to study attention in a controlled environment. We find that under typical conditions the effect of attention is negligible or even harmful, but under certain conditions it provides an exceptional gain in performance of more than 60% in some of our classification tasks. Satisfying these conditions in practice is challenging and often requires supervised training of attention. We propose an alternative recipe and train attention in a weaklysupervised fashion that approaches the performance of supervised models, and, compared to unsupervised models, improves results on several synthetic as well as real datasets. Source code and datasets are available at https://github.com/bknyaz/graph_attention_pool.", "keywords": ["0"], "reference_count": 28, "ccfClass": "A", "important": true, "references": [{"ref": "[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998\u20136008, 2017."}, {"ref": "[2] Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach. Attentive explanations: Justifying decisions and pointing to the evidence. arXiv preprint arXiv:1612.04757, 2016."}, {"ref": "[3] Andreea Deac, Petar Veli\u010ckovi\u0107, and Pietro Sormanni. Attentive cross-modal paratope prediction. Journal of Computational Biology, 2018."}, {"ref": "[4] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations (ICLR), 2018."}, {"ref": "[5] Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated attention networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018."}, {"ref": "[6] John Boaz Lee, Ryan Rossi, and Xiangnan Kong. Graph classification using structural attention. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1666\u20131674. ACM, 2018."}, {"ref": "[7] John Boaz Lee, Ryan A Rossi, Sungchul Kim, Nesreen K Ahmed, and Eunyee Koh. Attention models in graphs: A survey. arXiv preprint arXiv:1807.07984, 2018."}, {"ref": "[8] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, pages 3844\u20133852, 2016."}, {"ref": "[9] Uri Shaham, Kelly Stanton, Henry Li, Boaz Nadler, Ronen Basri, and Yuval Kluger. Spectralnet: Spectral clustering using deep neural networks. arXiv preprint arXiv:1801.01587, 2018."}, {"ref": "[10] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. In Advances in Neural Information Processing Systems, pages 4805\u20134815, 2018."}, {"ref": "[11] Hongyang Gao and Shuiwang Ji. Graph U-Net. In Submitted to the Seventh International Conference on Learning Representations (ICLR), 2018."}, {"ref": "[12] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations (ICLR), 2019."}, {"ref": "[13] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998."}, {"ref": "[14] Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graph evolution: Densification and shrinking diameters. ACM Transactions on Knowledge Discovery from Data (TKDD), 1(1):2, 2007."}, {"ref": "[15] Anshumali Shrivastava and Ping Li. A new space for comparing graphs. In Proceedings of the 2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, pages 62\u201371. IEEE Press, 2014."}, {"ref": "[16] Karsten M Borgwardt, Cheng Soon Ong, Stefan Sch\u00f6nauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl_1):i47\u2013i56, 2005."}, {"ref": "[17] Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. Journal of molecular biology, 330(4):771\u2013783, 2003."}, {"ref": "[18] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016."}, {"ref": "[19] Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In Proc. CVPR, volume 1, page 3, 2017."}, {"ref": "[20] Matthias Fey, Jan Eric Lenssen, Frank Weichert, and Heinrich M\u00fcller. Splinecnn: Fast geometric deep learning with continuous b-spline kernels. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 869\u2013877, 2018."}, {"ref": "[21] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, Sabine S\u00fcsstrunk, et al. Slic superpixels compared to state-of-the-art superpixel methods. 2012."}, {"ref": "[22] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1365\u20131374. ACM, 2015."}]}, {"author": ["Hu R", "Aggarwal C C", "Ma S"], "title": "An embedding approach to anomaly detection", "journal": "2016 IEEE", "year": 2016, "DOI": "0", "month": 0, "citations(google scholar)": 28, "abstract": "0", "keywords": ["0"], "reference_count": 2, "ccfClass": "0", "important": true, "references": [{"ref": "[1] R. S. Burt. Structural holes and good ideas. American Journal of Sociology, 110(2): 349-399, 2004."}, {"ref": "[2] M. McPherson, L. Simth-lovin, and J. Cook. Birds of a feather: Homophily in social networks. Annual review of sociology, Vol. 27: 415-444, 2001."}]}, {"author": ["Cao S", "Lu W", "Xu Q"], "title": "Deep neural networks for learning graph representations", "journal": "Thirtieth AAAI Conference on Artificial Intelligence. 2016.", "year": 2016, "DOI": "0", "month": 0, "citations(google scholar)": 265, "abstract": "In this paper, we propose a novel model for learning graph representations, which generates a low-dimensional vector representation for each vertex by capturing the graph structural information. Different from other previous research efforts, we adopt a random surfing model to capture graph structural information directly, instead of using the samplingbased method for generating linear sequences proposed by Perozzi et al. (2014). The advantages of our approach will be illustrated from both theorical and empirical perspectives. We also give a new perspective for the matrix factorization method proposed by Levy and Goldberg (2014), in which the pointwise mutual information (PMI) matrix is considered as an analytical solution to the objective function of the skipgram model with negative sampling proposed by Mikolov et al. (2013). Unlike their approach which involves the use of the SVD for finding the low-dimensitonal projections from the PMI matrix, however, the stacked denoising autoencoder is introduced in our model to extract complex features and model non-linearities. To demonstrate the effectiveness of our model, we conduct experiments on clustering and visualization tasks, employing the learned vertex representations as features. Empirical results on datasets of varying sizes show that our model outperforms other stat-of-the-art models in such tasks.", "keywords": ["0"], "reference_count": 29, "ccfClass": "A", "important": true, "references": [{"ref": "Agirre, E.; Alfonseca, E.; Hall, K.; Kravalova, J.; Pas\u00b8ca, M.; and Soroa, A. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In NAACL, 19\u201327."}, {"ref": "Arthur, D., and Vassilvitskii, S. 2007. k-means++: The advantages of careful seeding. In SODA, 1027\u20131035."}, {"ref": "Bengio, Y.; Lamblin, P.; Popovici, D.; Larochelle, H.; et al. 2007. Greedy layer-wise training of deep networks. In NIPS, 153\u2013160."}, {"ref": "Bourlard, H., and Kamp, Y. 1988. Auto-association by multilayer perceptrons and singular value decomposition. Biological cybernetics 59(4-5):291\u2013294."}, {"ref": "Bullinaria, J. A., and Levy, J. P. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior research methods 39(3):510\u2013526."}, {"ref": "Bullinaria, J. A., and Levy, J. P. 2012. Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd. Behavior research methods 44(3):890\u2013907."}, {"ref": "Cao, S.; Lu, W.; and Xu, Q. 2015. Grarep: Learning graph representations with global structural information. In CIKM, 891\u2013900."}, {"ref": "Church, K. W., and Hanks, P. 1990. Word association norms, mutual information, and lexicography. Computational linguistics 16(1):22\u201329."}, {"ref": "Dahl, G. E.; Yu, D.; Deng, L.; and Acero, A. 2012. Contextdependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on 20(1):30\u201342."}, {"ref": "Eckart, C., and Young, G. 1936. The approximation of one matrix by another of lower rank. Psychometrika 1(3):211\u2013218."}, {"ref": "Finkelstein, L.; Gabrilovich, E.; Matias, Y.; Rivlin, E.; Solan, Z.; Wolfman, G.; and Ruppin, E. 2001. Placing search in context: The concept revisited. In WWW, 406\u2013414."}, {"ref": "Gutmann, M. U., and Hyvarinen, A. 2012. Noise-contrastive es- \u00a8 timation of unnormalized statistical models, with applications to natural image statistics. JMLR 13(1):307\u2013361."}, {"ref": "Hinton, G. E., and Salakhutdinov, R. R. 2006. Reducing the dimensionality of data with neural networks. Science 313(5786):504\u2013 507."}, {"ref": "Hinton, G. E., and Zemel, R. S. 1994. Autoencoders, minimum description length, and helmholtz free energy. In NIPS, 3\u201310."}, {"ref": "Huang, E. H.; Socher, R.; Manning, C. D.; and Ng, A. Y. 2012. Improving word representations via global context and multiple word prototypes. In ACL, 873\u2013882."}, {"ref": "Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet classification with deep convolutional neural networks. In NIPS, 1097\u20131105."}, {"ref": "Levy, O., and Goldberg, Y. 2014. Neural word embedding as implicit matrix factorization. In NIPS, 2177\u20132185."}, {"ref": "Levy, O.; Goldberg, Y.; and Dagan, I. 2015. Improving distributional similarity with lessons learned from word embeddings. TACL 3:211\u2013225."}, {"ref": "Lichman, M. 2013. UCI machine learning repository."}, {"ref": "Lund, K., and Burgess, C. 1996. Producing high-dimensional"}, {"ref": "semantic spaces from lexical co-occurrence. Behavior Research Methods, Instruments, and Computers 28(2):203\u2013208."}, {"ref": "Macskassy, S. A., and Provost, F. 2003. A simple relational classifier. Technical report, DTIC Document."}, {"ref": "Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, 3111\u20133119."}, {"ref": "Miller, G. A., and Charles, W. G. 1991. Contextual correlates of semantic similarity. Language and cognitive processes 6(1):1\u201328."}, {"ref": "Mnih, A., and Teh, Y. W. 2012. A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426."}, {"ref": "Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In EMNLP, 1532\u20131543."}, {"ref": "Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk: Online learning of social representations. In SIGKDD, 701\u2013710."}, {"ref": "Shaoul, C. 2010. The westbury lab wikipedia corpus. Edmonton, AB: University of Alberta."}, {"ref": "Strehl, A.; Ghosh, J.; and Mooney, R. 2000. Impact of similarity measures on web-page clustering. In AAAI, 58\u201364."}, {"ref": "Tang, L., and Liu, H. 2009a. Relational learning via latent social dimensions. In SIGKDD, 817\u2013826."}, {"ref": "Tang, L., and Liu, H. 2009b. Scalable learning of collective behavior based on sparse social dimensions. In CIKM, 1107\u20131116."}, {"ref": "Tang, L., and Liu, H. 2011. Leveraging social media networks for classification. Data Mining and Knowledge Discovery 23(3):447\u2013 478."}, {"ref": "Tang, J.; Qu, M.; Wang, M.; Zhang, M.; Yan, J.; and Mei, Q. 2015."}, {"ref": "Line: Large-scale information network embedding. In WWW, 1067\u20131077."}, {"ref": "Tian, F.; Gao, B.; Cui, Q.; Chen, E.; and Liu, T.-Y. 2014. Learning deep representations for graph clustering. In AAAI."}, {"ref": "Van der Maaten, L., and Hinton, G. 2008. Visualizing data using t-sne. JMLR 9(2579-2605):85."}, {"ref": "Vincent, P.; Larochelle, H.; Bengio, Y.; and Manzagol, P.-A. 2008."}, {"ref": "Extracting and composing robust features with denoising autoencoders. In ICML, 1096\u20131103."}, {"ref": "Zar, J. H. 1972. Significance testing of the spearman rank correlation coefficient. Journal of the American Statistical Association 67(339):578\u2013580."}, {"ref": "Zesch, T.; Muller, C.; and Gurevych, I. 2008. Using wiktionary for \u00a8 computing semantic relatedness. In AAAI, volume 8, 861\u2013866."}]}, {"author": ["Veli\u010dkovi\u0107 P", "Cucurull G", "Casanova A"], "title": "Graph attention networks", "journal": "arXiv preprint arXiv:1710.10903", "year": 2017, "DOI": "0", "month": 0, "citations(google scholar)": 695, "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods\u2019 features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).", "keywords": ["0"], "reference_count": 45, "ccfClass": "0", "important": true, "references": [{"ref": "Mart\u00b4\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, \u00b4 Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Watten- \u00b4 berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org. James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1993\u20132001, 2016."}, {"ref": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. International Conference on Learning Representations (ICLR), 2015."}, {"ref": "Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of machine learning research, 7 (Nov):2399\u20132434, 2006."}, {"ref": "Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. International Conference on Learning Representations (ICLR), 2014."}, {"ref": "Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016."}, {"ref": "Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol- \u00a8 ger Schwenk, and Yoshua Beio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014."}, {"ref": "Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network \u00b4 learning by exponential linear units (elus). International Conference on Learning Representations (ICLR), 2016."}, {"ref": "Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks \u00a8 on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, pp. 3844\u20133852, 2016. Misha Denil, Sergio Gomez Colmenarejo, Serkan Cabi, David Saxton, and Nando de Freitas. Pro- \u00b4 grammable agents. arXiv preprint arXiv:1706.06383, 2017."}, {"ref": "Yan Duan, Marcin Andrychowicz, Bradly Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever,"}, {"ref": "Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. arXiv preprint arXiv:1703.07326, 2017."}, {"ref": "David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan \u00b4 Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. In Advances in neural information processing systems, pp. 2224\u20132232, 2015."}, {"ref": "Paolo Frasconi, Marco Gori, and Alessandro Sperduti. A general framework for adaptive processing"}, {"ref": "of data structures. IEEE transactions on Neural Networks, 9(5):768\u2013786, 1998."}, {"ref": "Jonas Gehring, Michael Auli, David Grangier, and Yann N. Dauphin. A convolutional encoder model for neural machine translation. CoRR, abs/1611.02344, 2016. URL http://arxiv. org/abs/1611.02344."}, {"ref": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 249\u2013256, 2010."}, {"ref": "Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In IEEE International Joint Conference on Neural Networks, pp. 729734, 2005."}, {"ref": "William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. Neural Information Processing Systems (NIPS), 2017."}, {"ref": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016."}, {"ref": "Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163, 2015."}, {"ref": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. \u00a8 Neural computation, 9(8): 1735\u20131780, 1997."}, {"ref": "Yedid Hoshen. Vain: Attentional multi-agent predictive modeling. In I. Guyon,"}, {"ref": "U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 2698\u2013 2708. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 6863-vain-attentional-multi-agent-predictive-modeling.pdf."}, {"ref": "Simon Jegou, Michal Drozdzal, David V \u00b4 azquez, Adriana Romero, and Yoshua Bengio. The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation. In Workshop on Computer Vision in Vehicle Technology CVPRW, 2017."}, {"ref": "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014."}, {"ref": "Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. International Conference on Learning Representations (ICLR), 2017."}, {"ref": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. International Conference on Learning Representations (ICLR), 2016."}, {"ref": "Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017."}, {"ref": "Qing Lu and Lise Getoor. Link-based classification. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 496\u2013503, 2003."}, {"ref": "Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(Nov):2579\u20132605, 2008."}, {"ref": "Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M ` Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. arXiv preprint arXiv:1611.08402, 2016."}, {"ref": "Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In Proceedings of The 33rd International Conference on Machine Learning, volume 48, pp. 2014\u20132023, 2016."}, {"ref": "Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge"}, {"ref": "discovery and data mining, pp. 701\u2013710. ACM, 2014."}, {"ref": "Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323\u20132326, 2000."}, {"ref": "Adam Santoro, David Raposo, David GT Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. arXiv preprint arXiv:1706.01427, 2017."}, {"ref": "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61\u201380, 2009."}, {"ref": "Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008."}, {"ref": "A. Sperduti and A. Starita. Supervised neural networks for the classification of structures. Trans. Neur. Netw., 8(3):714\u2013735, May 1997. ISSN 1045-9227. doi: 10.1109/72.572108."}, {"ref": "Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1):1929\u20131958, 2014."}, {"ref": "Aravind Subramanian, Pablo Tamayo, Vamsi K Mootha, Sayan Mukherjee, Benjamin L Ebert, Michael A Gillette, Amanda Paulovich, Scott L Pomeroy, Todd R Golub, Eric S Lander, et al. Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. Proceedings of the National Academy of Sciences, 102(43):15545\u201315550, 2005."}, {"ref": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017."}, {"ref": "Jason Weston, Fred \u00b4 eric Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi- supervised embedding. In Neural Networks: Tricks of the Trade, pp. 639\u2013655. Springer, 2012. Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. CoRR, abs/1410.3916, 2014. URL http://arxiv.org/abs/1410.3916."}, {"ref": "Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with"}, {"ref": "graph embeddings. In International Conference on Machine Learning, pp. 40\u201348, 2016."}, {"ref": "Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the 20th International conference on Machine learning (ICML-03), pp. 912\u2013919, 2003."}, {"ref": "Marinka Zitnik and Jure Leskovec. Predicting multicellular function through multi-layer tissue networks. Bioinformatics, 33(14):i190\u2013i198, 2017."}]}, {"author": ["Kipf T N", "Welling M"], "title": "arXiv preprint arXiv:1609.02907", "journal": "Semi-supervised classification with graph convolutional networks", "year": 2016, "DOI": "0", "month": 0, "citations(google scholar)": 2136, "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.", "keywords": ["0"], "reference_count": 32, "ccfClass": "0", "important": true, "references": [{"ref": "Mart\u00b4\u0131n Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015."}, {"ref": "James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in neural information processing systems (NIPS), 2016."}, {"ref": "Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of machine learning research (JMLR), 7(Nov):2399\u20132434, 2006."}, {"ref": "Ulrik Brandes, Daniel Delling, Marco Gaertler, Robert Gorke, Martin Hoefer, Zoran Nikoloski, and Dorothea Wagner. On modularity clustering. IEEE Transactions on Knowledge and Data Engineering, 20(2):172\u2013188, 2008."}, {"ref": "Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In International Conference on Learning Representations (ICLR), 2014."}, {"ref": "Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr, and Tom M. Mitchell. Toward an architecture for never-ending language learning. In AAAI, volume 5, pp. 3, 2010."}, {"ref": "Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on \u00a8 graphs with fast localized spectral filtering. In Advances in neural information processing systems (NIPS), 2016."}, {"ref": "Brendan L. Douglas. The Weisfeiler-Lehman method and graph isomorphism testing. arXiv preprint arXiv:1101.5211, 2011."}, {"ref": "David K. Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan \u00b4 Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In Advances in neural information processing systems (NIPS), pp. 2224\u20132232, 2015."}, {"ref": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, volume 9, pp. 249\u2013256, 2010."}, {"ref": "Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks., volume 2, pp. 729\u2013734. IEEE, 2005."}, {"ref": "Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016."}, {"ref": "David K. Hammond, Pierre Vandergheynst, and Remi Gribonval. Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis, 30(2):129\u2013150, 2011."}, {"ref": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016."}, {"ref": "Thorsten Joachims. Transductive inference for text classification using support vector machines. In International Conference on Machine Learning (ICML), volume 99, pp. 200\u2013209, 1999."}, {"ref": "Diederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015."}, {"ref": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. In International Conference on Learning Representations (ICLR), 2016."}, {"ref": "Qing Lu and Lise Getoor. Link-based classification. In International Conference on Machine Learning (ICML), volume 3, pp. 496\u2013503, 2003."}, {"ref": "Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research (JMLR), 9(Nov):2579\u20132605, 2008."}, {"ref": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (NIPS), pp. 3111\u20133119, 2013."}, {"ref": "Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In International Conference on Machine Learning (ICML), 2016."}, {"ref": "Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 701\u2013710. ACM, 2014."}, {"ref": "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61\u201380, 2009."}, {"ref": "Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008."}, {"ref": "Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research (JMLR), 15(1):1929\u20131958, 2014."}, {"ref": "Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pp. 1067\u20131077. ACM, 2015."}, {"ref": "Boris Weisfeiler and A. A. Lehmann. A reduction of a graph to a canonical form and an algebra arising during this reduction. Nauchno-Technicheskaya Informatsia, 2(9):12\u201316, 1968."}, {"ref": "Jason Weston, Fred \u00b4 eric Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi- \u00b4 supervised embedding. In Neural Networks: Tricks of the Trade, pp. 639\u2013655. Springer, 2012."}, {"ref": "Zhilin Yang, William Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning (ICML), 2016."}, {"ref": "Wayne W. Zachary. An information flow model for conflict and fission in small groups. Journal of anthropological research, pp. 452\u2013473, 1977."}, {"ref": "Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Scholkopf. \u00a8"}, {"ref": "Learning with local and global consistency. In Advances in neural information processing systems (NIPS), volume 16, pp. 321\u2013328, 2004."}, {"ref": "Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In International Conference on Machine Learning (ICML), volume 3, pp. 912\u2013919, 2003."}]}, {"author": ["Wu Z", "Pan S", "Chen F"], "title": "A comprehensive survey on graph neural networks", "journal": "arXiv preprint arXiv:1901.00596", "year": 2019, "DOI": "0", "month": 0, "citations(google scholar)": 167, "abstract": "Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders and spatial-temporal graph n ural net orks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes and benchmarks of the existing algorithms on different learning tasks. Finally, we propose potential research directions in this rapidly growing field.", "keywords": ["Deep Learning", "graph neural networks", "graph convolutional networks", "graph representation learning", "graph autoencoder", "network embedding"], "reference_count": 169, "ccfClass": "0", "important": true, "references": [{"ref": "[1] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \"You only look once: Unified, real-time object detection,\" in Proc. of CVPR, 2016, pp. 779\u2013788."}, {"ref": "[2] S. Ren, K. He, R. Girshick, and J. Sun, \"Faster r-cnn: Towards realtime object detection with region proposal networks,\" in NIPS, 2015, pp. 91\u201399."}, {"ref": "[3] M.-T. Luong, H. Pham, and C. D. Manning, \"Effective approaches to attention-based neural machine translation,\" in Proc. of EMNLP, 2015, pp. 1412\u20131421."}, {"ref": "[4] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey et al., \"Google\u2019s neural machine translation system: Bridging the gap between human and machine translation,\" arXiv preprint arXiv:1609.08144, 2016."}, {"ref": "[5] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., \"Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\" IEEE Signal processing magazine, vol. 29, no. 6, pp. 82\u201397, 2012."}, {"ref": "[6] Y. LeCun, Y. Bengio et al., \"Convolutional networks for images, speech, and time series,\" The handbook of brain theory and neural networks, vol. 3361, no. 10, p. 1995, 1995."}, {"ref": "[7] S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\" Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997."}, {"ref": "[8] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,\" Journal of machine learning research, vol. 11, no. Dec, pp. 3371\u20133408, 2010."}, {"ref": "[9] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, \"Geometric deep learning: going beyond euclidean data,\" IEEE Signal Processing Magazine, vol. 34, no. 4, pp. 18\u201342, 2017."}, {"ref": "[10] W. L. Hamilton, R. Ying, and J. Leskovec, \"Representation learning on graphs: Methods and applications,\" in NIPS, 2017, pp. 1024\u20131034."}, {"ref": "[11] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner et al., \"Relational inductive biases, deep learning, and graph networks,\" arXiv preprint arXiv:1806.01261, 2018."}, {"ref": "[12] J. B. Lee, R. A. Rossi, S. Kim, N. K. Ahmed, and E. Koh, \"Attention models in graphs: A survey,\" arXiv preprint arXiv:1807.07984, 2018."}, {"ref": "[13] A. Sperduti and A. Starita, \"Supervised neural networks for the classification of structures,\" IEEE Transactions on Neural Networks, vol. 8, no. 3, pp. 714\u2013735, 1997."}, {"ref": "[14] M. Gori, G. Monfardini, and F. Scarselli, \"A new model for learning in graph domains,\" in Proc. of IJCNN, vol. 2. IEEE, 2005, pp. 729\u2013734."}, {"ref": "[15] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, \"The graph neural network model,\" IEEE Transactions on Neural Networks, vol. 20, no. 1, pp. 61\u201380, 2009."}, {"ref": "[16] C. Gallicchio and A. Micheli, \"Graph echo state networks,\" in IJCNN. IEEE, 2010, pp. 1\u20138."}, {"ref": "[17] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel, \"Gated graph sequence neural networks,\" in Proc. of ICLR, 2015."}, {"ref": "[18] H. Dai, Z. Kozareva, B. Dai, A. Smola, and L. Song, \"Learning steadystates of iterative algorithms over graphs,\" in Proc. of ICML, 2018, pp. 1114\u20131122."}, {"ref": "[19] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, \"Spectral networks and locally connected networks on graphs,\" in Proc. of ICLR, 2014."}, {"ref": "[20] M. Henaff, J. Bruna, and Y. LeCun, \"Deep convolutional networks on graph-structured data,\" arXiv preprint arXiv:1506.05163, 2015."}, {"ref": "[21] M. Defferrard, X. Bresson, and P. Vandergheynst, \"Convolutional neural networks on graphs with fast localized spectral filtering,\" in NIPS, 2016, pp. 3844\u20133852."}, {"ref": "[22] T. N. Kipf and M. Welling, \"Semi-supervised classification with graph convolutional networks,\" in Proc. of ICLR, 2017."}, {"ref": "[23] R. Levie, F. Monti, X. Bresson, and M. M. Bronstein, \"Cayleynets: Graph convolutional neural networks with complex rational spectral filters,\" IEEE Transactions on Signal Processing, vol. 67, no. 1, pp. 97\u2013109, 2017."}, {"ref": "[24] A. Micheli, \"Neural network for graphs: A contextual constructive approach,\" IEEE Transactions on Neural Networks, vol. 20, no. 3, pp. 498\u2013511, 2009."}, {"ref": "[25] J. Atwood and D. Towsley, \"Diffusion-convolutional neural networks,\" in NIPS, 2016, pp. 1993\u20132001."}, {"ref": "[26] M. Niepert, M. Ahmed, and K. Kutzkov, \"Learning convolutional neural networks for graphs,\" in Proc. of ICML, 2016, pp. 2014\u20132023."}, {"ref": "[27] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, \"Neural message passing for quantum chemistry,\" in Proc. of ICML, 2017, pp. 1263\u20131272."}, {"ref": "[28] P. Cui, X. Wang, J. Pei, and W. Zhu, \"A survey on network embedding,\" IEEE Transactions on Knowledge and Data Engineering, 2017."}, {"ref": "[29] D. Zhang, J. Yin, X. Zhu, and C. Zhang, \"Network representation learning: A survey,\" IEEE Transactions on Big Data, 2018."}, {"ref": "[30] H. Cai, V. W. Zheng, and K. Chang, \"A comprehensive survey of graph embedding: problems, techniques and applications,\" IEEE Transactions on Knowledge and Data Engineering, 2018."}, {"ref": "[31] P. Goyal and E. Ferrara, \"Graph embedding techniques, applications, and performance: A survey,\" Knowledge-Based Systems, vol. 151, pp. 78\u201394, 2018."}, {"ref": "[32] S. Pan, J. Wu, X. Zhu, C. Zhang, and Y. Wang, \"Tri-party deep network representation,\" in Proc. of IJCAI, 2016, pp. 1895\u20131901."}, {"ref": "[33] X. Shen, S. Pan, W. Liu, Y.-S. Ong, and Q.-S. Sun, \"Discrete network embedding,\" in Proc. of IJCAI, 7 2018, pp. 3549\u20133555."}, {"ref": "[34] H. Yang, S. Pan, P. Zhang, L. Chen, D. Lian, and C. Zhang, \"Binarized attributed network embedding,\" in IEEE International Conference on Data Mining. IEEE, 2018."}, {"ref": "[35] B. Perozzi, R. Al-Rfou, and S. Skiena, \"Deepwalk: Online learning of social representations,\" in Proc. of KDD. ACM, 2014, pp. 701\u2013710."}, {"ref": "[36] S. V. N. Vishwanathan, N. N. Schraudolph, R. Kondor, and K. M. Borgwardt, \"Graph kernels,\" Journal of Machine Learning Research, vol. 11, no. Apr, pp. 1201\u20131242, 2010."}, {"ref": "[37] N. Shervashidze, P. Schweitzer, E. J. v. Leeuwen, K. Mehlhorn, and K. M. Borgwardt, \"Weisfeiler-lehman graph kernels,\" Journal of Machine Learning Research, vol. 12, no. Sep, pp. 2539\u20132561, 2011."}, {"ref": "[38] N. Navarin and A. Sperduti, \"Approximated neighbours minhash graph node kernel.\" in ESANN, 2017."}, {"ref": "[39] N. M. Kriege, F. D. Johansson, and C. Morris, \"A survey on graph kernels,\" arXiv preprint arXiv:1903.11835, 2019."}, {"ref": "[40] R. Li, S. Wang, F. Zhu, and J. Huang, \"Adaptive graph convolutional neural networks,\" in Proc. of AAAI, 2018, pp. 3546\u20133553."}, {"ref": "[41] C. Zhuang and Q. Ma, \"Dual graph convolutional networks for graphbased semi-supervised classification,\" in Proc. of WWW, 2018, pp. 499\u2013 508."}, {"ref": "[42] W. Hamilton, Z. Ying, and J. Leskovec, \"Inductive representation learning on large graphs,\" in NIPS, 2017, pp. 1024\u20131034."}, {"ref": "[43] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio, \"Graph attention networks,\" in Proc. of ICLR, 2017."}, {"ref": "[44] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein, \"Geometric deep learning on graphs and manifolds using mixture model cnns,\" in Proc. of CVPR, 2017, pp. 5115\u20135124."}, {"ref": "[45] H. Gao, Z. Wang, and S. Ji, \"Large-scale learnable graph convolutional networks,\" in Proc. of KDD. ACM, 2018, pp. 1416\u20131424."}, {"ref": "[46] D. V. Tran, A. Sperduti et al., \"On filter size in graph convolutional networks,\" in SSCI. IEEE, 2018, pp. 1534\u20131541."}, {"ref": "[47] D. Bacciu, F. Errica, and A. Micheli, \"Contextual graph markov model: A deep and generative approach to graph processing,\" in Proc. of ICML, 2018."}, {"ref": "[48] J. Zhang, X. Shi, J. Xie, H. Ma, I. King, and D.-Y. Yeung, \"Gaan: Gated attention networks for learning on large and spatiotemporal graphs,\" in Proc. of UAI, 2018."}, {"ref": "[49] J. Chen, T. Ma, and C. Xiao, \"Fastgcn: fast learning with graph convolutional networks via importance sampling,\" in Proc. of ICLR, 2018."}, {"ref": "[50] J. Chen, J. Zhu, and L. Song, \"Stochastic training of graph convolutional networks with variance reduction,\" in Proc. of ICML, 2018, pp. 941\u2013949."}, {"ref": "[51] W. Huang, T. Zhang, Y. Rong, and J. Huang, \"Adaptive sampling towards fast graph representation learning,\" in NeurIPS, 2018, pp. 4563\u20134572."}, {"ref": "[52] M. Zhang, Z. Cui, M. Neumann, and Y. Chen, \"An end-to-end deep learning architecture for graph classification,\" in Proc. of AAAI, 2018."}, {"ref": "[53] Q. Li, Z. Han, and X.-M. Wu, \"Deeper insights into graph convolutional networks for semi-supervised learning,\" in Proc. of AAAI, 2018."}, {"ref": "[54] Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec, \"Hierarchical graph representation learning with differentiable pooling,\" in NeurIPS, 2018, pp. 4801\u20134811."}, {"ref": "[55] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, and L. Song, \"Geniepath: Graph neural networks with adaptive receptive paths,\" in Proc. of AAAI, 2019."}, {"ref": "[56] P. Velickovi \u02c7 c, W. Fedus, W. L. Hamilton, P. Li \u00b4 o, Y. Bengio, and R. D. ` Hjelm, \"Deep graph infomax,\" in Proc. of ICLR, 2019."}, {"ref": "[57] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, \"How powerful are graph neural networks,\" in Proc. of ICLR, 2019."}, {"ref": "[58] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh, \"Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,\" in Proc. of KDD, 2019."}, {"ref": "[59] S. Cao, W. Lu, and Q. Xu, \"Deep neural networks for learning graph representations,\" in Proc. of AAAI, 2016, pp. 1145\u20131152."}, {"ref": "[60] D. Wang, P. Cui, and W. Zhu, \"Structural deep network embedding,\" in Proc. of KDD. ACM, 2016, pp. 1225\u20131234."}, {"ref": "[61] T. N. Kipf and M. Welling, \"Variational graph auto-encoders,\" arXiv preprint arXiv:1611.07308, 2016."}, {"ref": "[62] S. Pan, R. Hu, G. Long, J. Jiang, L. Yao, and C. Zhang, \"Adversarially regularized graph autoencoder for graph embedding.\" in Proc. of IJCAI, 2018, pp. 2609\u20132615."}, {"ref": "[63] K. Tu, P. Cui, X. Wang, P. S. Yu, and W. Zhu, \"Deep recursive network embedding with regular equivalence,\" in Proc. of KDD. ACM, 2018, pp. 2357\u20132366."}, {"ref": "[64] W. Yu, C. Zheng, W. Cheng, C. C. Aggarwal, D. Song, B. Zong, H. Chen, and W. Wang, \"Learning deep network representations with adversarially regularized autoencoders,\" in Proc. of AAAI. ACM, 2018, pp. 2663\u20132671."}, {"ref": "[65] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia, \"Learning deep generative models of graphs,\" in Proc. of ICML, 2018."}, {"ref": "[66] J. You, R. Ying, X. Ren, W. L. Hamilton, and J. Leskovec, \"Graphrnn: A deep generative model for graphs,\" Proc. of ICML, 2018."}, {"ref": "[67] M. Simonovsky and N. Komodakis, \"Graphvae: Towards generation of small graphs using variational autoencoders,\" in ICANN. Springer, 2018, pp. 412\u2013422."}, {"ref": "[68] T. Ma, J. Chen, and C. Xiao, \"Constrained generation of semantically valid graphs via regularizing variational autoencoders,\" in NeurIPS, 2018, pp. 7110\u20137121."}, {"ref": "[69] N. De Cao and T. Kipf, \"Molgan: An implicit generative model for small molecular graphs,\" arXiv preprint arXiv:1805.11973, 2018."}, {"ref": "[70] A. Bojchevski, O. Shchur, D. Zugner, and S. G \u00a8 unnemann, \"Netgan: \u00a8 Generating graphs via random walks,\" in Proc. of ICML, 2018."}, {"ref": "[71] Y. Seo, M. Defferrard, P. Vandergheynst, and X. Bresson, \"Structured sequence modeling with graph convolutional recurrent networks,\" arXiv preprint arXiv:1612.07659, 2016."}, {"ref": "[72] Y. Li, R. Yu, C. Shahabi, and Y. Liu, \"Diffusion convolutional recurrent neural network: Data-driven traffic forecasting,\" in Proc. of ICLR, 2018."}, {"ref": "[73] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena, \"Structural-rnn: Deep learning on spatio-temporal graphs,\" in Proc. of CVPR, 2016, pp. 5308\u20135317."}, {"ref": "[74] B. Yu, H. Yin, and Z. Zhu, \"Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting,\" in Proc. of IJCAI, 2018, pp. 3634\u20133640."}, {"ref": "[75] S. Yan, Y. Xiong, and D. Lin, \"Spatial temporal graph convolutional networks for skeleton-based action recognition,\" in Proc. of AAAI, 2018."}, {"ref": "[76] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang, \"Graph wavenet for deep spatial-temporal graph modeling,\" in Proc. of IJCAI, 2019."}, {"ref": "[77] S. Guo, Y. Lin, N. Feng, C. Song, and H. Wan, \"Attention based spatialtemporal graph convolutional networks for traffic flow forecasting,\" in Proc. of AAAI, 2019."}, {"ref": "[78] S. Pan, J. Wu, X. Zhu, C. Zhang, and P. S. Yu, \"Joint structure feature exploration and regularization for multi-task graph classification,\" IEEE Transactions on Knowledge and Data Engineering, vol. 28, no. 3, pp. 715\u2013728, 2016."}, {"ref": "[79] S. Pan, J. Wu, X. Zhu, G. Long, and C. Zhang, \"Task sensitive feature exploration and learning for multitask graph classification,\" IEEE transactions on cybernetics, vol. 47, no. 3, pp. 744\u2013758, 2017."}, {"ref": "[80] A. Micheli, D. Sona, and A. Sperduti, \"Contextual processing of structured data by recursive cascade correlation,\" IEEE Transactions on Neural Networks, vol. 15, no. 6, pp. 1396\u20131410, 2004."}, {"ref": "[81] K. Cho, B. Van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, \u00a8 H. Schwenk, and Y. Bengio, \"Learning phrase representations using rnn encoder-decoder for statistical machine translation,\" in Proc. of EMNLP, 2014, pp. 1724\u20131734."}, {"ref": "[82] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, \"The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains,\" IEEE Signal Processing Magazine, vol. 30, no. 3, pp. 83\u201398, 2013."}, {"ref": "[83] A. Sandryhaila and J. M. Moura, \"Discrete signal processing on graphs,\" IEEE transactions on signal processing, vol. 61, no. 7, pp. 1644\u20131656, 2013."}, {"ref": "[84] S. Chen, R. Varma, A. Sandryhaila, and J. Kovacevi \u02c7 c, \"Discrete signal \u00b4 processing on graphs: Sampling theory,\" IEEE Transactions on Signal Processing, vol. 63, no. 24, pp. 6510\u20136523, 2015."}, {"ref": "[85] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams, \"Convolutional networks on graphs for learning molecular fingerprints,\" in NIPS, 2015, pp. 2224\u20132232."}, {"ref": "[86] S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley, \"Molecular graph convolutions: moving beyond fingerprints,\" Journal of computer-aided molecular design, vol. 30, no. 8, pp. 595\u2013608, 2016."}, {"ref": "[87] K. T. Schutt, F. Arbabzadah, S. Chmiela, K. R. M \u00a8 uller, and \u00a8 A. Tkatchenko, \"Quantum-chemical insights from deep tensor neural networks,\" Nature communications, vol. 8, p. 13890, 2017."}, {"ref": "[88] J. B. Lee, R. Rossi, and X. Kong, \"Graph classification using structural attention,\" in Proc. of KDD. ACM, 2018, pp. 1666\u20131674."}, {"ref": "[89] S. Abu-El-Haija, B. Perozzi, R. Al-Rfou, and A. A. Alemi, \"Watch your step: Learning node embeddings via graph attention,\" in NeurIPS, 2018, pp. 9197\u20139207."}, {"ref": "[90] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst, \"Geodesic convolutional neural networks on riemannian manifolds,\" in Proc. of CVPR Workshops, 2015, pp. 37\u201345."}, {"ref": "[91] D. Boscaini, J. Masci, E. Rodola, and M. Bronstein, \"Learning shape ` correspondence with anisotropic convolutional neural networks,\" in NIPS, 2016, pp. 3189\u20133197."}, {"ref": "[92] M. Fey, J. E. Lenssen, F. Weichert, and H. Muller, \"Splinecnn: Fast \u00a8 geometric deep learning with continuous b-spline kernels,\" in Proc. of CVPR, 2018, pp. 869\u2013877."}, {"ref": "[93] B. Weisfeiler and A. Lehman, \"A reduction of a graph to a canonical form and an algebra arising during this reduction,\" NauchnoTechnicheskaya Informatsia, vol. 2, no. 9, pp. 12\u201316, 1968."}, {"ref": "[94] B. L. Douglas, \"The weisfeiler-lehman method and graph isomorphism testing,\" arXiv preprint arXiv:1101.5211, 2011."}, {"ref": "[95] T. Pham, T. Tran, D. Q. Phung, and S. Venkatesh, \"Column networks for collective classification,\" in Proc. of AAAI, 2017, pp. 2485\u20132491."}, {"ref": "[96] M. Simonovsky and N. Komodakis, \"Dynamic edgeconditioned filters in convolutional neural networks on graphs,\" in Proc. of CVPR, 2017."}, {"ref": "[97] T. Derr, Y. Ma, and J. Tang, \"Signed graph convolutional network,\" in Proc. of ICDM, 2018."}, {"ref": "[98] F. P. Such, S. Sah, M. A. Dominguez, S. Pillai, C. Zhang, A. Michael, N. D. Cahill, and R. Ptucha, \"Robust spatial filtering with graph convolutional neural networks,\" IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 6, pp. 884\u2013896, 2017."}, {"ref": "[99] X. Wang, H. Ji, C. Shi, B. Wang, C. Peng, Y. P., and Y. Ye, \"Heterogeneous graph attention network,\" in Proc. of WWW, 2019."}, {"ref": "[100] I. S. Dhillon, Y. Guan, and B. Kulis, \"Weighted graph cuts without eigenvectors a multilevel approach,\" IEEE transactions on pattern analysis and machine intelligence, vol. 29, no. 11, pp. 1944\u20131957, 2007."}, {"ref": "[101] O. Vinyals, S. Bengio, and M. Kudlur, \"Order matters: Sequence to sequence for sets,\" arXiv preprint arXiv:1511.06391, 2015."}, {"ref": "[102] J. Lee, I. Lee, and J. Kang, \"Self-attention graph pooling,\" in Prof. ICML, 2019, pp. 3734\u20133743."}, {"ref": "[103] F. Scarselli, A. C. Tsoi, and M. Hagenbuchner, \"The vapnik\u2013 chervonenkis dimension of graph and recursive neural networks,\" Neural Networks, vol. 108, pp. 248\u2013259, 2018."}, {"ref": "[104] H. Maron, H. Ben-Hamu, N. Shamir, and Y. Lipman, \"Invariant and equivariant graph networks,\" in ICLR, 2019."}, {"ref": "[105] K. Hornik, M. Stinchcombe, and H. White, \"Multilayer feedforward networks are universal approximators,\" Neural networks, vol. 2, no. 5, pp. 359\u2013366, 1989."}, {"ref": "[106] B. Hammer, A. Micheli, and A. Sperduti, \"Universal approximation capability of cascade correlation for structures,\" Neural Computation, vol. 17, no. 5, pp. 1109\u20131159, 2005."}, {"ref": "[107] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, \"Computational capabilities of graph neural networks,\" IEEE Transactions on Neural Networks, vol. 20, no. 1, pp. 81\u2013102, 2008."}, {"ref": "[108] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, \"Extracting and composing robust features with denoising autoencoders,\" in Proc. of ICML. ACM, 2008, pp. 1096\u20131103."}, {"ref": "[109] S. Pan, R. Hu, S.-f. Fung, G. Long, J. Jiang, and C. Zhang, \"Learning graph embedding with adversarial training methods,\" IEEE Transactions on Cybernetics, 2019."}, {"ref": "[110] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \"Generative adversarial nets,\" in NIPS, 2014, pp. 2672\u20132680."}, {"ref": "[111] R. Gomez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hern \u00b4 andez- \u00b4 Lobato, B. Sanchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, \u00b4 T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik, \"Automatic chemical design using a data-driven continuous representation of molecules,\" ACS central science, vol. 4, no. 2, pp. 268\u2013276, 2018."}, {"ref": "[112] M. J. Kusner, B. Paige, and J. M. Hernandez-Lobato, \"Grammar \u00b4 variational autoencoder,\" in Proc. of ICML, 2017."}, {"ref": "[113] H. Dai, Y. Tian, B. Dai, S. Skiena, and L. Song, \"Syntax-directed variational autoencoder for molecule generation,\" in Proc. of ICLR, 2018."}, {"ref": "[114] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. van den Berg, I. Titov, and M. Welling, \"Modeling relational data with graph convolutional networks,\" in ESWC. Springer, 2018, pp. 593\u2013607."}, {"ref": "[115] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, \"Improved training of wasserstein gans,\" in NIPS, 2017, pp. 5767\u2013 5777."}, {"ref": "[116] M. Arjovsky, S. Chintala, and L. Bottou, \"Wasserstein gan,\" arXiv preprint arXiv:1701.07875, 2017."}, {"ref": "[117] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. EliassiRad, \"Collective classification in network data,\" AI magazine, vol. 29, no. 3, p. 93, 2008."}, {"ref": "[118] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su, \"Arnetminer: extraction and mining of academic social networks,\" in Proc. of KDD. ACM, 2008, pp. 990\u2013998."}, {"ref": "[119] M. Zitnik and J. Leskovec, \"Predicting multicellular function through multi-layer tissue networks,\" Bioinformatics, vol. 33, no. 14, pp. i190\u2013 i198, 2017."}, {"ref": "[120] N. Wale, I. A. Watson, and G. Karypis, \"Comparison of descriptor spaces for chemical compound retrieval and classification,\" Knowledge and Information Systems, vol. 14, no. 3, pp. 347\u2013375, 2008."}, {"ref": "[121] A. K. Debnath, R. L. Lopez de Compadre, G. Debnath, A. J. Shusterman, and C. Hansch, \"Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity,\" Journal of medicinal chemistry, vol. 34, no. 2, pp. 786\u2013797, 1991."}, {"ref": "[122] P. D. Dobson and A. J. Doig, \"Distinguishing enzyme structures from non-enzymes without alignments,\" Journal of molecular biology, vol. 330, no. 4, pp. 771\u2013783, 2003."}, {"ref": "[123] K. M. Borgwardt, C. S. Ong, S. Schonauer, S. Vishwanathan, A. J. \u00a8 Smola, and H.-P. Kriegel, \"Protein function prediction via graph kernels,\" Bioinformatics, vol. 21, no. suppl 1, pp. i47\u2013i56, 2005."}, {"ref": "[124] H. Toivonen, A. Srinivasan, R. D. King, S. Kramer, and C. Helma, \"Statistical evaluation of the predictive toxicology challenge 2000\u2013 2001,\" Bioinformatics, vol. 19, no. 10, pp. 1183\u20131193, 2003."}, {"ref": "[125] L. Tang and H. Liu, \"Relational learning via latent social dimensions,\" in Proc. of KDD. ACM, 2009, pp. 817\u2013826."}, {"ref": "[126] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner et al., \"Gradient-based learning applied to document recognition,\" Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998."}, {"ref": "[127] H. Jagadish, J. Gehrke, A. Labrinidis, Y. Papakonstantinou, J. M. Patel, R. Ramakrishnan, and C. Shahabi, \"Big data and its technical challenges,\" Communications of the ACM, vol. 57, no. 7, pp. 86\u201394, 2014."}, {"ref": "[128] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hruschka Jr, and T. M. Mitchell, \"Toward an architecture for never-ending language learning.\" in Proc. of AAAI, 2010, pp. 1306\u20131313."}, {"ref": "[129] C. Wang, S. Pan, G. Long, X. Zhu, and J. Jiang, \"Mgae: Marginalized graph autoencoder for graph clustering,\" in Proc. of CIKM. ACM, 2017, pp. 889\u2013898."}, {"ref": "[130] M. Zhang and Y. Chen, \"Link prediction based on graph neural networks,\" in NeurIPS, 2018."}, {"ref": "[131] T. Kawamoto, M. Tsubaki, and T. Obuchi, \"Mean-field theory of graph neural networks in graph partitioning,\" in NeurIPS, 2018, pp. 4362\u2013 4372."}, {"ref": "[132] D. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei, \"Scene graph generation by iterative message passing,\" in Proc. of CVPR, vol. 2, 2017."}, {"ref": "[133] J. Yang, J. Lu, S. Lee, D. Batra, and D. Parikh, \"Graph r-cnn for scene graph generation,\" in ECCV. Springer, 2018, pp. 690\u2013706."}, {"ref": "[134] Y. Li, W. Ouyang, B. Zhou, J. Shi, C. Zhang, and X. Wang, \"Factorizable net: an efficient subgraph-based framework for scene graph generation,\" in ECCV. Springer, 2018, pp. 346\u2013363."}, {"ref": "[135] J. Johnson, A. Gupta, and L. Fei-Fei, \"Image generation from scene graphs,\" in Proc. of CVPR, 2018."}, {"ref": "[136] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon, \"Dynamic graph cnn for learning on point clouds,\" ACM Transactions on Graphics (TOG), 2019."}, {"ref": "[137] L. Landrieu and M. Simonovsky, \"Large-scale point cloud semantic segmentation with superpoint graphs,\" in Proc. of CVPR, 2018."}, {"ref": "[138] G. Te, W. Hu, Z. Guo, and A. Zheng, \"Rgcnn: Regularized graph cnn for point cloud segmentation,\" arXiv preprint arXiv:1806.02952, 2018."}, {"ref": "[139] S. Qi, W. Wang, B. Jia, J. Shen, and S.-C. Zhu, \"Learning humanobject interactions by graph parsing neural networks,\" in ECCV, 2018, pp. 401\u2013417."}, {"ref": "[140] V. G. Satorras and J. B. Estrach, \"Few-shot learning with graph neural networks,\" in Proc. of ICLR, 2018."}, {"ref": "[141] M. Guo, E. Chou, D.-A. Huang, S. Song, S. Yeung, and L. Fei-Fei, \"Neural graph matching networks for fewshot 3d action recognition,\" in ECCV. Springer, 2018, pp. 673\u2013689."}, {"ref": "[142] L. Liu, T. Zhou, G. Long, J. Jiang, L. Yao, and C. Zhang, \"Prototype propagation networks (ppn) for weakly-supervised few-shot learning on category graph,\" in Proc. of IJCAI, 2019."}, {"ref": "[143] X. Qi, R. Liao, J. Jia, S. Fidler, and R. Urtasun, \"3d graph neural networks for rgbd semantic segmentation,\" in Proc. of CVPR, 2017, pp. 5199\u20135208."}, {"ref": "[144] L. Yi, H. Su, X. Guo, and L. J. Guibas, \"Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation.\" in Proc. of CVPR, 2017, pp. 6584\u20136592."}, {"ref": "[145] X. Chen, L.-J. Li, L. Fei-Fei, and A. Gupta, \"Iterative visual reasoning beyond convolutions,\" in Proc. of CVPR, 2018."}, {"ref": "[146] M. Narasimhan, S. Lazebnik, and A. Schwing, \"Out of the box: Reasoning with graph convolution nets for factual visual question answering,\" in NeurIPS, 2018, pp. 2655\u20132666."}, {"ref": "[147] D. Marcheggiani and I. Titov, \"Encoding sentences with graph convolutional networks for semantic role labeling,\" in Proc. of EMNLP, 2017, pp. 1506\u20131515."}, {"ref": "[148] J. Bastings, I. Titov, W. Aziz, D. Marcheggiani, and K. Sima\u2019an, \"Graph convolutional encoders for syntax-aware neural machine translation,\" in Proc. of EMNLP, 2017, pp. 1957\u20131967."}, {"ref": "[149] D. Marcheggiani, J. Bastings, and I. Titov, \"Exploiting semantics in neural machine translation with graph convolutional networks,\" in NAACL, 2018."}, {"ref": "[150] L. Song, Y. Zhang, Z. Wang, and D. Gildea, \"A graph-to-sequence model for amr-to-text generation,\" in Proc. of ACL, 2018."}, {"ref": "[151] D. Beck, G. Haffari, and T. Cohn, \"Graph-to-sequence learning using gated graph neural networks,\" in Proc. of ACL, 2018."}, {"ref": "[152] D. D. Johnson, \"Learning graphical state transitions,\" in Proc. of ICLR, 2016."}, {"ref": "[153] B. Chen, L. Sun, and X. Han, \"Sequence-to-action: End-to-end semantic graph generation for semantic parsing,\" in Proc. of ACL, 2018, pp. 766\u2013777."}, {"ref": "[154] H. Yao, F. Wu, J. Ke, X. Tang, Y. Jia, S. Lu, P. Gong, J. Ye, and Z. Li, \"Deep multi-view spatial-temporal network for taxi demand prediction,\" in Proc. of AAAI, 2018, pp. 2588\u20132595."}, {"ref": "[155] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, \"Line: Large-scale information network embedding,\" in Proceedings of the International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2015, pp. 1067\u20131077."}, {"ref": "[156] R. van den Berg, T. N. Kipf, and M. Welling, \"Graph convolutional matrix completion,\" stat, vol. 1050, p. 7, 2017."}, {"ref": "[157] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec, \"Graph convolutional neural networks for web-scale recommender systems,\" in Proc. of KDD. ACM, 2018, pp. 974\u2013983."}, {"ref": "[158] F. Monti, M. Bronstein, and X. Bresson, \"Geometric matrix completion with recurrent multi-graph neural networks,\" in NIPS, 2017, pp. 3697\u2013 3707."}, {"ref": "[159] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur, \"Protein interface prediction using graph convolutional networks,\" in NIPS, 2017, pp. 6530\u20136539."}, {"ref": "[160] J. You, B. Liu, R. Ying, V. Pande, and J. Leskovec, \"Graph convolutional policy network for goal-directed molecular graph generation,\" in NeurIPS, 2018."}, {"ref": "[161] M. Allamanis, M. Brockschmidt, and M. Khademi, \"Learning to represent programs with graphs,\" in Proc. of ICLR, 2017."}, {"ref": "[162] J. Qiu, J. Tang, H. Ma, Y. Dong, K. Wang, and J. Tang, \"Deepinf: Social influence prediction with deep learning,\" in Proc. of KDD. ACM, 2018, pp. 2110\u20132119."}, {"ref": "[163] D. Zugner, A. Akbarnejad, and S. G \u00a8 unnemann, \"Adversarial attacks on \u00a8 neural networks for graph data,\" in Proc. of KDD. ACM, 2018, pp. 2847\u20132856."}, {"ref": "[164] E. Choi, M. T. Bahadori, L. Song, W. F. Stewart, and J. Sun, \"Gram: graph-based attention model for healthcare representation learning,\" in Proc. of KDD. ACM, 2017, pp. 787\u2013795."}, {"ref": "[165] E. Choi, C. Xiao, W. Stewart, and J. Sun, \"Mime: Multilevel medical embedding of electronic health records for predictive healthcare,\" in NeurIPS, 2018, pp. 4548\u20134558."}, {"ref": "[166] J. Kawahara, C. J. Brown, S. P. Miller, B. G. Booth, V. Chau, R. E. Grunau, J. G. Zwicker, and G. Hamarneh, \"Brainnetcnn: convolutional neural networks for brain networks; towards predicting neurodevelopment,\" NeuroImage, vol. 146, pp. 1038\u20131049, 2017."}, {"ref": "[167] T. H. Nguyen and R. Grishman, \"Graph convolutional networks with argument-aware pooling for event detection,\" in Proc. of AAAI, 2018, pp. 5900\u20135907."}, {"ref": "[168] Z. Li, Q. Chen, and V. Koltun, \"Combinatorial optimization with graph convolutional networks and guided tree search,\" in NeurIPS, 2018, pp. 536\u2013545."}, {"ref": "[169] K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proc. of CVPR, 2016, pp. 770\u2013778."}]}, {"author": ["Bourigault S", "Lagnier C", "Lamprier S"], "title": "7th ACM international conference on Web search and data mining. ACM, 2014: 393-402", "journal": "Learning social network embeddings for predicting information diffusion", "year": 2014, "DOI": "0", "month": 0, "citations(google scholar)": 76, "abstract": "Analyzing and modeling the temporal diffusion of information on social media has mainly been treated as a diffusion process on known graphs or proximity structures. The underlying phenomenon results however from the interactions of several actors and media and is more complex than what these models can account for and cannot be explained using such limiting assumptions. We introduce here a new approach to this problem whose goal is to learn a mapping of the observed temporal dynamic onto a continuous space. Nodes participating to diffusion cascades are projected in a latent representation space in such a way that information diffusion can be modeled efficiently using a heat diffusion process. This amounts to learning a diffusion kernel for which the proximity of nodes in the projection space reflects the proximity of their infection time in cascades. The proposed approach possesses several unique characteristics compared to existing ones. Since its parameters are directly learned from cascade samples without requiring any additional information, it does not rely on any pre-existing diffusion structure. Because the solution to the diffusion equation can be expressed in a closed form in the projection space, the inference time for predicting the diffusion of a new piece of information is greatly reduced compared to discrete models. Experiments and comparisons with baselines and alternative models have been performed on both synthetic networks and real datasets. They show the effectiveness of the proposed method both in terms of prediction quality and of inference speed.", "keywords": ["0"], "reference_count": 29, "ccfClass": "0", "important": true, "references": [{"ref": "[1] F. M. Bass. A new product growth for model consumer durables. Management Science, 15:215{227, 1969."}, {"ref": "[2] A. Bordes, J. Weston, R. Collobert, and Y. Bengio. Learning structured embeddings of knowledge bases.  In AAAI, 2011."}, {"ref": "[3] R. Bosagh Zadeh, A. Goel, K. Munagala, and A. Sharma. On the precision of social and information networks. In Proceedings of the first ACM conference on Online social networks, pages 63{74. ACM, 2013."}, {"ref": "[4] K. Burton, A. Java, and I. Soboroff. The icwsm 2009 spinn3r dataset. In Proceedings of the Third Annual Conference on Weblogs and Social Media, May 2009."}, {"ref": "[5] M. Chen, Q. Yang, and X. Tang. Directed graph embedding. In IJCAI, pages 2707{2712, 2007."}, {"ref": "[6] W. Feng and J. Wang. Retweet or not?: personalized tweet re-ranking. In Proceedings of the sixth ACM international conference on Web search and data mining, WSDM \u201913. ACM, 2013."}, {"ref": "[7] J. Goldenberg, B. Libai, and E. Muller. Talk of the network: A complex systems look at the underlying process of word-of-mouth. Marketing letters, 12(3):211{223, 2001."}, {"ref": "[8] M. Gomez-Rodriguez, D. Balduzzi, and B. Sch\u00a8 olkopf. Uncovering the temporal dynamics of diffusion networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML 11, pages 561{568. ACM, 2011."}, {"ref": "[9] M. Gomez Rodriguez, J. Leskovec, and A. Krause. Inferring networks of diffusion and influence. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD 10, New York, NY, USA, 2010. ACM."}, {"ref": "[10] M. Gomez-Rodriguez, J. Leskovec, and B. Sch\u00a8 olkopf. Modeling information propagation with survival theory. In ICML, 2013."}, {"ref": "[11] A. Grigoryan. Heat Kernel and Analysis on Manifolds. AMS/IP Studies in Advanced Mathematics. American Mathematical Society, 2009."}, {"ref": "[12] A. Guille and H. Hacid. A predictive model for the temporal dynamics of information diffusion in online social networks. In Proceedings of the 21st international conference companion on World Wide Web, WWW \u201912 Companion. ACM, 2012."}, {"ref": "[13] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of influence through a social network. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD \u201903, pages 137{146. ACM, 2003."}, {"ref": "[14] R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete input spaces. In ICML, volume 2, pages 315{322, 2002."}, {"ref": "[15] C. Lagnier, L. Denoyer, E. Gaussier, and P. Gallinari. Predicting information diffusion in social networks using content and user\u2019s profiles. In European Conference on Information Retrieval, ECIR \u201913, 2013."}, {"ref": "[16] J. Leskovec, L. Backstrom, and J. Kleinberg. Meme-tracking and the dynamics of the news cycle. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD \u201909, pages 497{506, New York, NY, USA, 2009. ACM."}, {"ref": "[17] H. Ma, H. Yang, M. R. Lyu, and I. King. Mining social networks using heat diffusion processes for marketing candidates selection. In Proceedings of the 17th ACM conference on Information and knowledge management, CIKM \u201908, pages 233{242, New York, NY, USA, 2008. ACM."}, {"ref": "[18] S. A. Myers and J. Leskovec. On the convexity of latent social network inference. CoRR, abs/1010.5504, 2010."}, {"ref": "[19] A. Najar, L. Denoyer, and P. Gallinari. Predicting information diffusion on social networks with partial knowledge. In Proceedings of the 21st international conference companion on World Wide Web, WWW \u201912 Companion, pages 1197{1204, New York, NY, USA, 2012. ACM."}, {"ref": "[20] D. M. Romero, B. Meeder, and J. Kleinberg. Differences in the mechanics of information diffusion across topics: idioms, political hashtags, and complex contagion on twitter. In Proceedings of the 20th international conference on World wide web, pages 695{704. ACM, 2011."}, {"ref": "[21] K. Saito, M. Kimura, K. Ohara, and H. Motoda. Generative models of information diffusion with asynchronous timedelay. Journal of Machine Learning Research - Proceedings Track, 13:193{208, 2010."}, {"ref": "[22] K. Saito, R. Nakano, and M. Kimura. Prediction of information diffusion probabilities for independent cascade model. In Proceedings of the 12th international conference on Knowledge-Based Intelligent Information and Engineering Systems, Part III, KES \u201908, pages 67{75. Springer-Verlag, 2008."}, {"ref": "[23] K. Saito, K. Ohara, Y. Yamagishi, M. Kimura, and H. Motoda. Learning diffusion probability based on node attributes in social networks. In M. Kryszkiewicz, H. Rybinski, A. Skowron, and Z. W. Ras, editors, ISMIS, volume 6804 of Lecture Notes in Computer Science, pages 153{162. Springer, 2011."}, {"ref": "[24] G. Szabo and B. A. Huberman. Predicting the popularity of online content. Communications of the ACM, 53(8):80{88, 2010."}, {"ref": "[25] G. Ver Steeg and A. Galstyan. Information-theoretic measures of influence based on content dynamics. In Proceedings of the sixth ACM international conference on Web search and data mining, WSDM \u201913, pages 3{12, New York, NY, USA, 2013. ACM."}, {"ref": "[26] L. Wang, S. Ermon, and J. E. Hopcroft. Feature-enhanced probabilistic models for diffusion network inference. In Proceedings of the 2012 European conference on Machine Learning and Knowledge Discovery in Databases - Volume Part II, ECML PKDD\u201912, pages 499{514. Springer-Verlag, 2012."}, {"ref": "[27] K. Q. Weinberger, F. Sha, and L. K. Saul. Learning a kernel matrix for nonlinear dimensionality reduction. In Proceedings of the twenty-first international conference on Machine learning, page 106. ACM, 2004."}, {"ref": "[28] H. Yang, I. King, and M. R. Lyu. Diffusionrank: a possible penicillin for web spamming. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 431{438. ACM, 2007."}, {"ref": "[29] J. Yang and J. Leskovec. Modeling information diffusion in implicit networks. In Proceedings of the 2010 IEEE International Conference on Data Mining, ICDM \u201910, pages 599{608, Washington, DC, USA, 2010. IEEE Computer Society."}]}, {"author": ["Wang D", "Cui P", "Zhu W."], "title": "Structural Deep Network Embedding", "journal": "22nd ACM SIGKDD international conference on Knowledge discovery and data mining. ACM", "year": 2016, "DOI": "0", "month": 0, "citations(google scholar)": 752, "abstract": "Network embedding is an important method to learn low-dimensional representations of vertexes in networks, aiming to capture and preserve the network structure. Almost all the existing network embedding methods adopt shallow models. However, since the underlying network structure is complex, shallow models cannot capture the highly non-linear network structure, resulting in sub-optimal network representations. Therefore, how to find a method that is able to effectively capture the highly non-linear network structure and preserve the global and local structure is an open yet important problem. To solve this problem, in this paper we propose a Structural Deep Network Embedding method, namely SDNE. More specifically, we first propose a semi-supervised deep model, which has multiple layers of non-linear functions, thereby being able to capture the highly non-linear network structure. Then we propose to exploit the first-order and second-order proximity jointly to preserve the network structure. The second-order  roximity is used by the unsupervised component to capture the global network structure. While the first-order proximity is used as the supervised information in the supervised component to preserve the local network structure. By jointly optimizing them in the semi-supervised deep model, our method can preserve both the local and global network structure and is robust to sparse networks. Empirically, we conduct the experiments on five real-world networks, including a language network, a citation network and three social networks. The results show that compared to the baselines, our method can reconstruct the original network significantly better and achieves substantial gains in three applications, i.e. multi-label classification, link prediction and visualization.", "keywords": ["0"], "reference_count": 36, "ccfClass": "A", "important": true, "references": [{"ref": "[1] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation, 15(6):1373\u20131396, 2003."}, {"ref": "[2] Y. Bengio. Learning deep architectures for ai. Foundations and trends R in Machine Learning, 2(1):1\u2013127, 2009."}, {"ref": "[3] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798\u20131828, 2013."}, {"ref": "[4] S. Cao, W. Lu, and Q. Xu. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 891\u2013900. ACM, 2015."}, {"ref": "[5] S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang. Heterogeneous network embedding via deep architectures. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 119\u2013128. ACM, 2015"}, {"ref": "[6] N. S. Dash. Context and contextual word meaning. SKASE Journal o Theoretical Linguistics, 5(2):21\u201331, 2008."}, {"ref": "[7] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio. Why does unsupervised pre-training help deep learning? The Journal of Machine Learning Research, 11:625\u2013660, 2010."}, {"ref": "[8] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871\u20131874, 2008."}, {"ref": "[9] K. Georgiev and P. Nakov. A non-iid framework for collaborative filtering with restricted boltzmann machines. In ICML-13, pages 1148\u20131156, 2013."}, {"ref": "[10] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neura networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6):82\u201397, 2012."}, {"ref": "[11] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527\u20131554, 2006."}, {"ref": "[12] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 2006."}, {"ref": "[13] M. Jamali and M. Ester. A matrix factorization technique with trust propagation for recommendation in social networks. In Proceedings of the fourth ACM conference on Recommender systems, pages 135\u2013142. ACM, 2010."}, {"ref": "[14] E. M. Jin, M. Girvan, and M. E. Newman. Structure of growing social networks. Physical review E, 64(4):046132, 2001."}, {"ref": "[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances neural information processing systems, pages 1097\u20131105, 2012."}, {"ref": "[16] J. Leskovec, J. Kleinberg, and C. Faloutsos. Graph evolution: Densification and shrinking diameters. ACM Transactions on Knowledge Discovery from Data (TKDD), 1(1):2, 2007."}, {"ref": "[17] D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019\u20131031, 2007."}, {"ref": "[18] T. Liu and D. Tao. Classification with noisy labels by importance reweighting. TPAMI, (1):1\u20131."}, {"ref": "[19] D. Luo, F. Nie, H. Huang, and C. H. Ding. Cauchy graph embedding. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 553\u2013560, 2011."}, {"ref": "[20] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 2:849\u2013856, 2002."}, {"ref": "[21] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701\u2013710. ACM, 2014."}, {"ref": "[22] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323\u20132326, 2000."}, {"ref": "[23] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate Reasoning, 50(7):969\u2013978, 2009."}, {"ref": "[24] B. Shaw and T. Jebara. Structure preserving embedding. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 937\u2013944. ACM, 2009."}, {"ref": "[25] R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP), volume 1631, page 1642. Citeseer, 2013."}, {"ref": "[26] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067\u20131077. International World Wide Web Conferences Steering Committee, 2015."}, {"ref": "[27] L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2009."}, {"ref": "[28] L. Tang and H. Liu. Scalable learning of collective behavior based on sparse social dimensions. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 1107\u20131116. ACM, 2009."}, {"ref": "[29] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319\u20132323, 2000."}, {"ref": "[30] F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu. Learning deep representations for graph clustering. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pages 1293\u20131299, 2014."}, {"ref": "[31] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(2579-2605):85, 2008."}, {"ref": "[32] S. V. N. Vishwanathan, N. N. Schraudolph, R. Kondor, and K. M. Borgwardt. Graph kernels. The Journal of Machine Learning Research, 11:1201\u20131242, 2010."}, {"ref": "[33] D. Wang, P. Cui, M. Ou, and W. Zhu. Deep multimodal hashing with orthogonal regularization. In Proceedings of the 24th International Conference on Artificial Intelligence, pages 2291\u20132297. AAAI Press, 2015."}, {"ref": "[34] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In Advances in neural information processing systems, pages 1753\u20131760, 2009."}, {"ref": "[35] C. Xu, D. Tao, and C. Xu. Multi-view intact space learning. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 37(12):2531\u20132544, 2015."}, {"ref": "[36] J. Zhuang, I. W. Tsang, and S. Hoi. Two-layer multiple kernel learning. In International conference on artificial intelligence and statistics, pages 909\u2013917, 2011."}]}, {"author": ["Wang S", "Tang J"], "title": "2017 SIAM international conference on data mining", "journal": "Signed network embedding in social media", "year": 2017, "DOI": "0", "month": 0, "citations(google scholar)": 86, "abstract": "Network embedding is to learn low-dimensional vector representations for nodes of a given social network, facilitating many tasks in social network analysis such as link prediction. The vast majority of existing embedding algorithms are designed for unsigned social networks or social networks with only positive links. However, networks in social media could have both positive and negative links, and little work exists for signed social networks. From recent findings of signed network analysis, it is evident that negative links have distinct properties and added value besides positive links, which brings about both challenges and opportunities for signed network embedding. In this paper, we propose a deep learning framework SiNE for signed network embedding. The framework optimizes an objective function guided by social theories that provide a fundamental understanding of signed social networks. Experimental results on two realworld datasets of social media demonstrate the effectiveness of the proposed framework SiNE.", "keywords": ["0"], "reference_count": 37, "ccfClass": "0", "important": true, "references": [{"ref": "[1] D. Liben-Nowell and J. Kleinberg, \"The link-prediction problem for social networks,\" Journal of the American society for information science and technology, vol. 58, no. 7, pp. 1019\u20131031, 2007."}, {"ref": "[2] S. Papadopoulos, Y. Kompatsiaris, A. Vakali, and P. Spyridonos, \"Community detection in social media,\" DMKD, vol. 24, no. 3, pp. 515\u2013554, 2012."}, {"ref": "[3] S. Bhagat, G. Cormode, and S. Muthukrishnan, \"Node classification in social networks,\" in Social network data analytics. Springer, 2011, pp. 115\u2013148."}, {"ref": "[4] S. Wang, J. Tang, F. Morstatter, and H. Liu, \"Paired restricted boltzmann machine for linked data,\" in CIKM. ACM, 2016, pp. 1753\u20131762."}, {"ref": "[5] S. Wang, J. Tang, C. Aggarwal, and H. Liu, \"Linked document embedding for classification,\" in CIKM. ACM, 2016, pp. 115\u2013124."}, {"ref": "[6] L. Van der Maaten and G. Hinton, \"Visualizing data using t-sne,\" JMLR, vol. 9, no. 2579-2605, p. 85, 2008."}, {"ref": "[7] J. Leskovec, D. Huttenlocher, and J. Kleinberg, \"Signed networks in social media,\" in SIGCHI, 2010."}, {"ref": "[8] J. Tang, Y. Chang, C. Aggarwal, and H. Liu, \"A survey of signed network mining in social media,\" ACM Comput. Surv., vol. 49, no. 3, 2016."}, {"ref": "[9] J. Tang, X. Hu, and H. Liu, \"Is distrust the negation of trust?: the value of distrust in social media,\" in Hypertext. ACM, 2014, pp. 148\u2013157."}, {"ref": "[10] J. Leskovec, D. Huttenlocher, and J. Kleinberg, \"Predicting positive and negative links in online social networks,\" in WWW. ACM, 2010, pp. 641\u2013650."}, {"ref": "[11] H. Ma, M. R. Lyu, and I. King, \"Learning to recommend with trust and distrust relationships,\" in RecSys. ACM, 2009, pp. 189\u2013196."}, {"ref": "[12] J. Tang, Y. Chang, and H. Liu, \"Mining social media with social theories: A survey,\" ACM SIGKDD Explorations Newsletter, vol. 15, no. 2, pp. 20\u201329, 2014."}, {"ref": "[13] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, \"Cnn features off-the-shelf: an astounding baseline for recognition,\" in CVPRW. IEEE, 2014."}, {"ref": "[14] B. Perozzi, R. Al-Rfou, and S. Skiena, \"Deepwalk: Online learning of social representations,\" in SIGKDD. ACM, 2014, pp. 701\u2013710."}, {"ref": "[15] S. T. Roweis and L. K. Saul, \"Nonlinear dimensionality reduction by locally linear embedding,\" science, vol. 290, no. 5500, pp. 2323\u20132326, 2000."}, {"ref": "[16] M. Belkin and P. Niyogi, \"Laplacian eigenmaps and spectral techniques for embedding and clustering.\" in NIPS, vol. 14, 2001, pp. 585\u2013591."}, {"ref": "[17] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, \"Line: Large-scale information network embedding,\" in WWW. ACM, 2015, pp. 1067\u20131077."}, {"ref": "[18] A. Grover and J. Leskovec, \"node2vec: Scalable feature learning for network,\" in SIGKDD. ACM, 2016."}, {"ref": "[19] M. Ou, P. Cui, J. Pei, and W. Zhu, \"Asymmetric transitive preserving graph embedding,\" in KDD, 2016."}, {"ref": "[20] J. Tang, S. Chang, C. Aggarwal, and H. Liu, \"Negative link prediction in social media,\" in WSDM, 2015."}, {"ref": "[21] J. Kunegis, S. Schmidt, A. Lommatzsch, J. Lerner, E. W. De Luca, and S. Albayrak, \"Spectral analysis of signed graphs for clustering, prediction and visualization.\" in SDM, vol. 10. SIAM, 2010, pp. 559\u2013559."}, {"ref": "[22] M. Szell, R. Lambiotte, and S. Thurner, \"Multirelational organization of large-scale social networks in an online world,\" Proceedings of the National Academy of Sciences, vol. 107, no. 31, 2010."}, {"ref": "[23] K.-Y. Chiang, J. J. Whang, and I. S. Dhillon, \"Scalable clustering of signed networks using balance normalized cut,\" in SIGIR. ACM, 2012, pp. 615\u2013624."}, {"ref": "[24] S. Wasserman, Social network analysis: Methods and applications. Cambridge university press, 1994, vol. 8."}, {"ref": "[25] J. C. Turner, Social influence. Thomson Brooks/Cole Publishing Co, 1991."}, {"ref": "[26] F. Heider, \"Attitudes and cognitive organization,\" The Journal of psychology, vol. 21, no. 1, pp. 107\u2013112, 1946."}, {"ref": "[27] D. Cartwright and F. Harary, \"Structural balance: a generalization of heider\u2019s theory.\" p. 277, 1956."}, {"ref": "[28] J. A. Davis, \"Clustering and structural balance in graphs,\" Social networks. A developing paradigm, pp. 27\u201334, 1977."}, {"ref": "[29] M. Cygan, M. Pilipczuk, M. Pilipczuk, and J. O. Wojtaszczyk, \"Sitting closer to friends than enemies, revisited,\" in Mathematical Foundations of Computer Science. Springer, 2012, pp. 296\u2013307."}, {"ref": "[30] Y. Bengio, A. Courville, and P. Vincent, \"Representation learning: A review and new perspectives,\" TPAMI, vol. 35, no. 8, pp. 1798\u20131828, 2013."}, {"ref": "[31] R. Socher, A. Perelygin, J. Y. Wu et al., \"Recursive deep models for semantic compositionality over a sentiment treebank,\" in EMNLP, 2013."}, {"ref": "[32] T. Mikolov, K. Chen, G. Corrado, and J. Dean, \"Efficient estimation of word representations in vector space,\" arXiv preprint arXiv:1301.3781, 2013."}, {"ref": "[33] X. Glorot and Y. Bengio, \"Understanding the difficulty of training deep feedforward neural networks,\" in AISTATS, 2010, pp. 249\u2013256."}, {"ref": "[34] G. Beigi, J. Tang, S. Wang, and H. Liu, \"Exploiting emotional information for trust/distrust prediction,\" in SDM, 2016."}, {"ref": "[35] C.-J. Hsieh, K.-Y. Chiang, and I. S. Dhillon, \"Low rank modeling of signed networks,\" in SIGKDD, 2012."}, {"ref": "[36] Y. Wang, S. Wang, J. Tang, H. Liu, and B. Li, \"Unsupervised sentiment analysis for social media images.\" in IJCAI, 2015, pp. 2378\u20132379."}, {"ref": "[37] W. Lian, R. Henao, V. Rao, J. Lucas, and L. Carin, \"A multitask point process predictive model,\" in ICML, 2015, pp. 2030\u20132038."}]}, {"author": ["L. Xu", "X. Wei", "J. Cao", "P. S. Yu"], "title": "Embedding of embedding (eoe): Joint embedding for coupled heterogeneous networks", "journal": "10th ACM Int. Conf. Web Search Data Mining", "year": 2017, "DOI": "0", "month": 0, "citations(google scholar)": 54, "abstract": "Network embedding is increasingly employed to assist network analysis as it is effective to learn latent features that encode linkage information. Various network embedding methods have been proposed, but they are only designed for a single network scenario. In the era of big data, different types of related information can be fused together to form a coupled heterogeneous network, which consists of two different but related sub-networks connected by inter-network edges. In this scenario, the inter-network edges can act as complementary information in the presence of intra-network ones. This complementary information is important because it can make latent features more comprehensive and accurate. And it is more important when the intra-network edges are absent, which can be referred to as the cold-start problem. In this paper, we thus propose a method named embedding of embedding (EOE) for coupled heterogeneous networks. In the EOE, latent features encode not only intra-network edges, but also inter-network ones. To tackle the challenge of heterogeneities of two networks, the EOE incorporates a harmonious embedding matrix to further embed the embeddings that only encode intra-network edges. Empirical experiments on a variety of real-world datasets demonstrate the EOE outperforms consistently single network embedding methods in applications including visualization, link prediction multi-class classification, and multi-label classification.", "keywords": ["Information systems", "Data mining"], "reference_count": 26, "ccfClass": "0", "important": true, "references": [{"ref": "[1] A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. In Proceedings of the 22nd international conference on World Wide Web, pages 37\u201348. International World Wide Web Conferences Steering Committee, 2013."}, {"ref": "[2] L. Armijo. Minimization of functions having lipschitz continuous first partial derivatives. Pacific Journal of mathematics, 16(1):1\u20133, 1966."}, {"ref": "[3] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585\u2013591, 2001."}, {"ref": "[4] J. C. Bezdek and R. J. Hathaway. Some notes on alternating optimization. In AFSS International Conference on Fuzzy Systems, pages 288\u2013300. Springer, 2002."}, {"ref": "[5] B. Cao, X. Kong, and S. Y. Philip. Collective prediction of multiple types of links in heterogeneous information networks. In 2014 IEEE International Conference on Data Mining, pages 50\u201359. IEEE, 2014."}, {"ref": "[6] B. Carpenter. Lazy sparse stochastic gradient descent for regularized multinomial logistic regression. Alias-i, Inc., Tech. Rep, pages 1\u201320, 2008."}, {"ref": "[7] B. Chen, Y. Ding, and D. J. Wild. Assessing drug target association using semantic linked data. PLoS Comput Biol, 8(7):e1002574, 2012."}, {"ref": "[8] T. F. Cox and M. A. Cox. Multidimensional scaling. CRC press, 2000."}, {"ref": "[9] S. Fortunato. Community detection in graphs. Physics reports, 486(3):75\u2013174, 2010."}, {"ref": "[10] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. The weka data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10\u201318, 2009."}, {"ref": "[11] J. B. Kruskal. Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. Psychometrika, 29(1):1\u201327, 1964."}, {"ref": "[12] D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019\u20131031, 2007."}, {"ref": "[13] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701\u2013710. ACM, 2014."}, {"ref": "[14] J. Read, P. Reutemann, B. Pfahringer, and G. Holmes. Meka: a multi-label/multi-target extension to weka. Journal of Machine Learning Research, 17(21):1\u20135, 2016."}, {"ref": "[15] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323\u20132326, 2000."}, {"ref": "[16] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008."}, {"ref": "[17] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067\u20131077. International World Wide Web Conferences Steering Committee, 2015."}, {"ref": "[18] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su. Arnetminer: extraction and mining of academic social networks. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 990\u2013998. ACM, 2008."}, {"ref": "[19] L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 817\u2013826. ACM, 2009."}, {"ref": "[20] L. Tang and H. Liu. Leveraging social media networks for classification. Data Mining and Knowledge Discovery, 23(3):447\u2013478, 2011."}, {"ref": "[21] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319\u20132323, 2000."}, {"ref": "[22] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(2579-2605):85, 2008."}, {"ref": "[23] X. Wang, L. Tang, H. Gao, and H. Liu. Discovering overlapping groups in social media. In 2010 IEEE international conference on data mining, pages 569\u2013578. IEEE, 2010."}, {"ref": "[24] X. Wei, B. Cao, W. Shao, C.-T. Lu, and P. S. Yu. Community detection with partially observable links and node attributes. In IEEE International Conference on Big Data, 2016."}, {"ref": "[25] X. Wei, B. Cao, and P. S. Yu. Unsupervised feature selection on networks: A generative view. In AAAI, 2016."}, {"ref": "[26] X. Wei, S. Xie, and P. S. Yu. Efficient partial order preserving unsupervised feature selection on networks. In SDM, pages 82\u201390. SIAM, 2015."}]}, {"author": ["Tang J", "Qu M", "Wang M"], "title": "LINE: Large-scale Information Network Embedding", "journal": "24th international conference on world wide web. International World Wide Web Conferences Steering Committee", "year": 2016, "DOI": "0", "month": 0, "citations(google scholar)": 1638, "abstract": "This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the \\LINE,\" which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very eficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online.", "keywords": ["information network embedding", "scalability", "feature learning", "dimension reduction"], "reference_count": 23, "ccfClass": "0", "important": true, "references": []}, {"author": ["Grover A", "Leskovec J"], "title": "Node2vec: Scalable feature learning for networks", "journal": "22nd ACM SIGKDD international conference on Knowledge discovery and data mining", "year": 2016, "DOI": "0", "month": 0, "citations(google scholar)": 2175, "abstract": "Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node\u2019s network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.", "keywords": ["Information networks", "Feature learning", "Node embeddings", "Graph representations"], "reference_count": 39, "ccfClass": "A", "important": true, "references": [{"ref": "2. Backstrom L, Leskovec J. Supervised random walks: predicting and recommending links in social networks. WSDM. 2011 [Google Scholar]"}, {"ref": "3. Belkin M, Niyogi P. Laplacian eigenmaps and spectral techniques for embedding and clustering. NIPS. 2001 [Google Scholar]"}, {"ref": "4. Bengio Y, Courville A, Vincent P. Representation learning: A review and new perspectives. IEEE TPAMI. 2013;35(8):1798\u20131828. [PubMed] [Google Scholar]"}, {"ref": "5. Breitkreutz B-J, Stark C, Reguly T, Boucher L, Breitkreutz A, Livstone M, Oughtred R, Lackner DH, B\u00e4hler J, Wood V, et al. The BioGRID interaction database. Nucleic acids research. 2008;36:D637\u2013D640. [PMC free article] [PubMed] [Google Scholar]"}, {"ref": "6. Cao S, Lu W, Xu Q. GraRep: Learning Graph Representations with global structural information. CIKM. 2015 [Google Scholar]"}, {"ref": "7. Fortunato S. Community detection in graphs. Physics Reports. 2010;486(3\u20135):75\u2013174. [Google Scholar]"}, {"ref": "8. Gallagher B, Eliassi-Rad T. Lecture Notes in Computer Science: Advances in Social Network Mining and Analysis. Springer; 2009. Leveraging label-independent features for classification in sparsely labeled networks: An empirical study. [Google Scholar]"}, {"ref": "9. Harris ZS. Word. Distributional Structure. 1954;10(23):146\u2013162. [Google Scholar]"}, {"ref": "10. Henderson K, Gallagher B, Eliassi-Rad T, Tong H, Basu S, Akoglu L, Koutra D, Faloutsos C, Li L. RolX: structural role extraction & mining in large graphs. KDD. 2012 [Google Scholar]"}, {"ref": "11. Henderson K, Gallagher B, Li L, Akoglu L, Eliassi-Rad T, Tong H, Faloutsos C. It\u2019s who you know: graph mining using recursive structural features. KDD. 2011 [Google Scholar]"}, {"ref": "12. Hoff PD, Raftery AE, Handcock MS. Latent space approaches to social network analysis. J of the American Statistical Association. 2002 [Google Scholar]"}, {"ref": "13. Knuth DE. The Stanford GraphBase: a platform for combinatorial computing. Vol. 37. Addison-Wesley; Reading: 1993. [Google Scholar]"}, {"ref": "14. Leskovec J, Krevl A. SNAP Datasets: Stanford large network dataset collection. 2014 Jun; http://snap.stanford.edu/data."}, {"ref": "15. Li K, Gao J, Guo S, Du N, Li X, Zhang A. LRBM: A restricted boltzmann machine based approach for representation learning on linked data. ICDM. 2014 [Google Scholar]"}, {"ref": "16. Li X, Du N, Li H, Li K, Gao J, Zhang A. A deep learning approach to link prediction in dynamic networks. ICDM. 2014 [Google Scholar]"}, {"ref": "17. Li Y, Tarlow D, Brockschmidt M, Zemel R. Gated graph sequence neural networks. ICLR. 2016 [Google Scholar]"}, {"ref": "18. Liben-Nowell D, Weinberg J. The link-prediction problem for social networks. J of the American society for information science and technology. 2007;58(7):1019\u20131031. [Google Scholar]"}, {"ref": "19. Liberzon A, Subramanian A, Pinchback R, Thorvaldsd\u00f3ttir H, Tamayo P, Mesirov JP. Molecular signatures database (MSigDB) 3.0. Bioinformatics. 2011;27(12):1739\u20131740. [PMC free article] [PubMed] [Google Scholar]"}, {"ref": "20. Mahoney M. Large text compression benchmark. 2011 www.mattmahoney.net/dc/textdata."}, {"ref": "21. Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in vector space. ICLR. 2013 [Google Scholar]"}, {"ref": "22. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their composiuonality. NIPS. 2013 [Google Scholar]"}, {"ref": "23. Pennington J, Socher R, Manning CD. GloVe: Global vectors for word representation. EMNLP. 2014 [Google Scholar]"}, {"ref": "24. Perozzi B, Al-Rfou R, Skiena S. DeepWalk: Online learning of social representations. KDD. 2014 [Google Scholar]"}, {"ref": "25. Radivojac P, Clark WT, Oron TR, Schnoes AM, Wittkop T, Sokolov A, Graim K, Funk C, Verspoor, et al. A large-scale evaluation of computational protein function prediction. Nature methods. 2013;10(3):221\u2013227. [PMC free article] [PubMed] [Google Scholar]"}, {"ref": "26. Recht B, Re C, Wright S, Niu F. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. NIPS. 2011 [Google Scholar]"}, {"ref": "27. Roweis ST, Saul LK. Nonlinear dimensionality reduction by locally linear embedding. Science. 2000;290(5500):2323\u20132326. [PubMed] [Google Scholar]"}, {"ref": "28. Tang J, Qu M, Wang M, Zhang M, Yan J, Mei Q. LINE: Large-scale Information Network Embedding. WWW. 2015 [Google Scholar]"}, {"ref": "29. Tang L, Liu H. Leveraging social media networks for classification. Data Mining and Knowledge Discovery. 2011;23(3):447\u2013478. [Google Scholar]"}, {"ref": "30. Tenenbaum JB, De Silva V, Langford JC. A global geometric framework for nonlinear dimensionality reduction. Science. 2000;290(5500):2319\u20132323. [PubMed] [Google Scholar]"}, {"ref": "31. Tian F, Gao B, Cui Q, Chen E, Liu TY. Learning deep representations for graph clustering. AAAI. 2014 [Google Scholar]"}, {"ref": "32. Toutanova K, Klein D, Manning CD, Singer Y. Feature-rich part-of-speech tagging with a cyclic dependency network. NAACL. 2003 [Google Scholar]"}, {"ref": "33. Tsoumakas G, Katakis I. Multi-label classification: An overview. Dept of Informatics, Aristotle University of Thessaloniki; Greece: 2006. [Google Scholar]"}, {"ref": "34. Vazquez A, Flammini A, Maritan A, Vespignani A. Global protein function prediction from protein-protein interaction networks. Nature biotechnology. 2003;21(6):697\u2013700. [PubMed] [Google Scholar]"}, {"ref": "35. Yan S, Xu D, Zhang B, Zhang HJ, Yang Q, Lin S. Graph embedding and extensions: a general framework for dimensionality reduction. IEEE TPAMI. 2007;29(1):40\u201351. [PubMed] [Google Scholar]"}, {"ref": "36. Yang J, Leskovec J. Overlapping communities explain core-periphery organization of networks. Proceedings of the IEEE. 2014;102(12):1892\u20131902. [Google Scholar]"}, {"ref": "37. Yang SH, Long B, Smola A, Sadagopan N, Zheng Z, Zha H. Like like alike: joint friendship and interest propagation in social networks. WWW. 2011 [Google Scholar]"}, {"ref": "38. Zafarani R, Liu H. Social computing data repository at ASU. 2009 [Google Scholar]"}, {"ref": "39. Zhai S, Zhang Z. Dropout training of matrix factorization and autoencoder for link prediction in sparse graphs. SDM. 2015 [Google Scholar]"}, {"ref": "38. Zafarani R, Liu H. Social computing data repository at ASU. 2009 [Google Scholar]"}, {"ref": "39. Zhai S, Zhang Z. Dropout training of matrix factorization and autoencoder for link prediction in sparse graphs. SDM. 2015 [Google Scholar]"}]}, {"author": ["Cao S", "Lu W", "Xu Q."], "title": "Grarep: Learning graph representations with global structural information", "journal": "24th international conference on world wide web. International World Wide Web Conferences Steering Committee", "year": 2015, "DOI": "0", "month": 0, "citations(google scholar)": 501, "abstract": "In this paper, we present GraRep, a novel model for learning vertex representations of weighted graphs. This model learns low dimensional vectors to represent vertices appearing in a graph and, unlike existing work, integrates global structural information of the graph into the learning process. We also formally analyze the connections between our work and several previous research efforts, including the DeepWalk model of Perozzi et al. [20] as well as the skip-gram model with negative sampling of Mikolov et al. [18] We conduct experiments on a language network, a social network as well as a citation network and show that our learned global representations can be effectively used as features in tasks such as clustering, classification and visualization. Empirical results demonstrate that our representation significantly outperforms other state-of-the-art methods in such tasks.", "keywords": ["Graph Representation", "Matrix Factorization", "Feature Learning", "Dimension Reduction"], "reference_count": 31, "ccfClass": "A", "important": true, "references": [{"ref": "[1] A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. In WWW, pages 37\u201348. International World Wide Web Conferences Steering Committee, 2013."}, {"ref": "[2] D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In SODA, pages 1027\u20131035. Society for Industrial and Applied Mathematics, 2007."}, {"ref": "[3] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585\u2013591, 2001."}, {"ref": "[4] J. A. Bullinaria and J. P. Levy. Extracting semantic representations from word co-occurrence statistics: A computational study. BRM, 39(3):510\u2013526, 2007."}, {"ref": "[5] J. A. Bullinaria and J. P. Levy. Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd. BRM, 44(3):890\u2013907, 2012."}, {"ref": "[6] J. Caron. Experiments with lsa scoring: Optimal rank and basis. In CIR, pages 157\u2013169, 2001."}, {"ref": "[7] P. Comon. Independent component analysis, a new concept? Signal processing, 36(3):287\u2013314, 1994."}, {"ref": "[8] T. F. Cox and M. A. Cox. Multidimensional scaling. CRC Press, 2000."}, {"ref": "[9] C. Eckart and G. Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211\u2013218, 1936."}, {"ref": "[10] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large linear classification. JMLR, 9:1871\u20131874, 2008."}, {"ref": "[11] M. U. Gutmann and A. Hyv\u00a8arinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. JMLR, 13(1):307\u2013361, 2012."}, {"ref": "[12] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504\u2013507, 2006."}, {"ref": "[13] C. Jutten and J. Herault. Blind separation of sources, part i: An adaptive algorithm based on neuromimetic architecture. Signal processing, 24(1):1\u201310, 1991."}, {"ref": "[14] V. Klema and A. J. Laub. The singular value decomposition: Its computation and some applications. Automatic Control, 25(2):164\u2013176, 1980."}, {"ref": "[15] T. K. Landauer, P. W. Foltz, and D. Laham. An introduction to latent semantic analysis. Discourse processes, 25(2-3):259\u2013284, 1998."}, {"ref": "[16] O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In NIPS, pages 2177\u20132185, 2014."}, {"ref": "[17] K. Lund and C. Burgess. Producing high-dimensional semantic spaces from lexical co-occurrence. BRMIC, 28(2):203\u2013208, 1996."}, {"ref": "[18] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111\u20133119, 2013."}, {"ref": "[19] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. EMNLP, 12, 2014."}, {"ref": "[20] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701\u2013710. ACM, 2014."}, {"ref": "[21] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323\u20132326, 2000."}, {"ref": "[22] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Incremental singular value decomposition algorithms for highly scalable recommender systems. In ICIS, pages 27\u201328. Citeseer, 2002."}, {"ref": "[23] J. Shi and J. Malik. Normalized cuts and image segmentation. PAMI, 22(8):888\u2013905, 2000."}, {"ref": "[24] A. Strehl, J. Ghosh, and R. Mooney. Impact of similarity measures on web-page clustering. In Workshop on Artificial Intelligence for Web Search (AAAI 2000), pages 58\u201364, 2000."}, {"ref": "[25] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW. ACM, 2015."}, {"ref": "[26] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su. Arnetminer: extraction and mining of academic social networks. In SIGKDD, pages 990\u2013998. ACM, 2008."}, {"ref": "[27] L. Tang and H. Liu. Relational learning via latent social dimensions. In SIGKDD, pages 817\u2013826. ACM, 2009."}, {"ref": "[28] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319\u20132323, 2000."}, {"ref": "[29] F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu. Learning deep representations for graph clustering. In AAAI, 2014."}, {"ref": "[30] P. D. Turney. Domain and function: A dual-space model of semantic relations and compositions. JAIR, pages 533\u2013585, 2012."}, {"ref": "[31] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. JMLR, 9(2579-2605):85, 2008."}]}, {"author": ["Mao C", "Xiao W"], "title": "A comprehensive algorithm for evaluating node influences in social networks based on preference analysis and random walk", "journal": "Complexity", "year": 2018, "DOI": "0", "month": 0, "citations(google scholar)": 1, "abstract": "In the era of big data, social network has become an important reflection of human communications and interactions on the Internet. Identifying the influential spreaders in networks plays a crucial role in various areas, such as disease outbreak, virus propagation, and public opinion controlling. Based on the three basic centrality measures, a comprehensive algorithm named PARW-Rank for evaluating node influences has been proposed by applying preference relation analysis and random walk technique. For each basic measure, the preference relation between every node pair in a network is analyzed to construct the partial preference graph (PPG). Then, the comprehensive preference graph (CPG) is generated by combining the preference relations with respect to three basic measures. Finally, the ranking of nodes is determined by conducting random walk on the CPG. Furthermore, five public social networks are used for comparative analysis. The experimental results show that our PARW-Rank algorithm can achieve the higher precision and better stability than the existing methods with a single centrality measure.", "keywords": ["0"], "reference_count": 63, "ccfClass": "0", "important": true, "references": []}, {"author": ["Song A", "Liu Y", "Wu Z"], "title": "A local random walk model for complex networks based on discriminative feature combinations", "journal": "Expert Systems with Applications", "year": 2019, "DOI": "0", "month": 0, "citations(google scholar)": 0, "abstract": "Networks have become increasingly important to model many complex systems. This powerful representation has been employed in different tasks of artificial intelligence including machine learning, expert and intelligent systems. Link prediction, a branch of network pattern recognition, is the most fundamental and essential problem for complex network analysis. However, most existing link-prediction methods only consider a network\u2019s topology structures, and in doing so, these methods miss the opportunity to use nodes\u2019 attribute information. We present a combined approach here that uses nodes\u2019 attribute information and topology structure to direct link prediction. First, we propose a discriminative feature combinations selection method. Specifically, we present a novel mathematics inference to detail discriminative feature combinations. Second, based on the selected feature combinations, we aggregate the network, and further compute each feature combination\u2019s contributing degree to the link\u2019s formation, called  he strength of feature combination. Third, we apply discriminative feature combinations into a local random walk model; in particular, we compute and redistribute the random walk particle\u2019s transfer probability in terms of each feature combination\u2019s strength, which makes the transfer probability depend on feature combinations satisfied by each node\u2019s edges. Finally, we predict links in complex networks based on the improved random walk model. Experimental results on real-life complex network datasets demonstrate that, compared to other baseline methods, using discriminative feature combinations and topology structures in tandem strengthens prediction performance remarkably.", "keywords": ["Link prediction", "Local random walk", "Discriminative feature combination", "The strength of feature combination", "Probability transfer matrix"], "reference_count": 53, "ccfClass": "0", "important": true, "references": []}, {"author": ["Sengupta N", "Bagchi A", "Ramanath M"], "title": "ARROW: Approximating Reachability Using Random Walks Over Web-Scale Graphs", "journal": "2019 IEEE 35th International Conference on Data Engineering (ICDE)", "year": 2019, "DOI": "0", "month": 0, "citations(google scholar)": 1, "abstract": "Efficiently answering reachability queries on a directed graph is a fundamental problem and many solutions \u2013 theoretical and practical \u2013 have been proposed. A common strategy to make reachability query processing efficient, accurate and scalable is to precompute indexes on the graph. However this often becomes impractical, particularly when dealing with large graphs that are highly dynamic or when queries have additional constraints known only at the time of querying. In the former case, indexes become stale very quickly and keeping them upto-date at the same speed as changes to the graph is untenable. For the latter setting, currently proposed indexes are often quite bulky and are highly customized to handle only a small class of constraints. In this paper, we propose a first practical attempt to address these issues by abandoning the traditional indexing approach altogether and operating directly on the graph as it evolves. Our approach, called ARROW, uses random walks to efficiently approximate reachability between vertices, building on ideas that have been prevalent in the theory community but ignored by practitioners. Not only is ARROW well suited for highly dynamic settings \u2013 as it is index-free, but it can also be easily adapted to handle many different forms of ad-hoc constraints while being competitive with custom-made index structures. In this paper, we show that ARROW, despite its simplicity, is near-accurate and scales to graphs with tens of millions of vertices and hundreds of millions of edges. We present extensive empirical evidence to illustrate these advantages.", "keywords": ["0"], "reference_count": 60, "ccfClass": "0", "important": true, "references": [{"ref": "[1] R. Agrawal, A. Borgida, and H. V. Jagadish. Efficient management of transitive relationships in large data and knowledge bases. SIGMOD Record, 18(2), 1989."}, {"ref": "[2] A. Anagnostopoulos, R. Kumar, M. Mahdian, E. Upfal, and F. Vandin. Algorithms on evolving graphs. In ITCS, 2012."}, {"ref": "[3] L. Atzori, A. Iera, and G. Morabito. Siot: Giving a social structure to the internet of things. IEEE communications letters, 15(11):1193\u20131195, 2011."}, {"ref": "[4] C. Avin, M. Koucky, and Z. Lotker. How to explore a fast-changing ` world (cover time of a simple random walk on evolving graphs). In ICALP, 2008."}, {"ref": "[5] A. Barabasi and R. Albert. Emergence of scaling in random networks. \u00b4 science, 286(5439), 1999."}, {"ref": "[6] K. Berberich, S. J. Bedathur, M. Vazirgiannis, and G. Weikum. Buzzrank ... and the trend is your friend. In WWW, 2006."}, {"ref": "[7] K. Berberich, M. Vazirgiannis, and G. Weikum. Time-aware authority ranking. Internet Mathematics, 2(3):301\u2013332, 2005."}, {"ref": "[8] B. Bollobas and O. Riordan. The diameter of a scale-free random graph. \u00b4 Combinatorica, 24(1), 2004."}, {"ref": "[9] R. Bramandia, B. Choi, and W. K. Ng. Incremental maintenance of 2-hop labeling of large graphs. TKDE, 22(5), 2010."}, {"ref": "[10] J. Cai and C. K. Poon. Path-hop: efficiently indexing large graphs for reachability queries. In CIKM, 2010."}, {"ref": "[11] A. Casteigts, P. Flocchini, W. Quattrociocchi, and N. Santoro. Timevarying graphs and dynamic networks. IJPEDS, 27(5), 2012."}, {"ref": "[12] Y. Chen and Y. Chen. An efficient algorithm for answering graph reachability queries. In ICDE, 2008."}, {"ref": "[13] J. Cheng, S. Huang, H. Wu, and A. W. Fu. Tf-label: a topologicalfolding labeling scheme for reachability querying in a large graph. In SIGMOD, 2013."}, {"ref": "[14] F. R. K. Chung. Diameters and eigenvalues. J. Amer. Math. Soc., 2(2), 1989."}, {"ref": "[15] E. Cohen. Size-estimation framework with applications to transitive closure and reachability. J. Comput. Syst. Sci., 55(3):441\u2013453, 1997."}, {"ref": "[16] E. Cohen, E. Halperin, H. Kaplan, and U. Zwick. Reachability and distance queries via 2-hop labels. SIAM J. Comput., 32(5), 2003."}, {"ref": "[17] I. B. Dhia. Access control in social networks: a reachability-based approach. In EDBT/ICDT Workshops, 2012."}, {"ref": "[18] S. Dorogovtsev and J. F. F. Mendes. Scaling properties of scale-free evolving networks: Continuous approach. Phys. Rev. E, 63, 2001."}, {"ref": "[19] W. Eberle, J. Graves, and L. Holder. Insider threat detection using a graph-based approach. Journal of Applied Security Research, 6(1):32\u2013 81, 2010."}, {"ref": "[20] U. Feige. A fast randomized LOGSPACE algorithm for graph connectivity. Theor. Comput. Sci., 169(2), 1996."}, {"ref": "[21] J. A. Fill. Eigenvalue bounds on convergence to stationarity for nonreversible markov chains, with an application to the exclusion process. Ann. Appl. Probab., 1(1), 1991."}, {"ref": "[22] P. Gopalan, R. J. Lipton, and A. Mehta. Randomized time-space tradeoffs for directed graph connectivity. In FSTTCS, 2003."}, {"ref": "[23] I. Gorodezky and I. Pak. Generalized loop-erased random walks and approximate reachability. RSA, 44(2), 2014."}, {"ref": "[24] J. Gubbi, R. Buyya, S. Marusic, and M. Palaniswami. Internet of things (iot): A vision, architectural elements, and future directions. Future generation computer systems, 29(7):1645\u20131660, 2013."}, {"ref": "[25] P. Holme and J. Saramaki. Temporal networks. \u00a8 Phys. rep., 519(3), 2012."}, {"ref": "[26] W. Huo and V. J. Tsotras. Efficient temporal shortest path queries on evolving social graphs. In SSDBM, 2014."}, {"ref": "[27] D. Kempe, J. Kleinberg, and A. Kumar. Connectivity and inference problems for temporal networks. In STOC, 2000."}, {"ref": "[28] U. Khurana and A. Deshpande. Efficient snapshot retrieval over historical graph data. In ICDE, 2013."}, {"ref": "[29] U. Khurana and A. Deshpande. Hinge: enabling temporal network analytics at scale. In SIGMOD, 2013."}, {"ref": "[30] U. Khurana and A. Deshpande. Storing and analyzing historical graph data at scale. In EDBT, 2016."}, {"ref": "[31] V. King. Fully dynamic algorithms for maintaining all-pairs shortest paths and transitive closure in digraphs. In FOCS, 1999."}, {"ref": "[32] V. King and G. Sagert. A fully dynamic algorithm for maintaining the transitive closure. J. Comput. Syst. Sci., 65(1):150\u2013167, 2002."}, {"ref": "[33] G. Koloniari, D. Souravlias, and E. Pitoura. On graph deltas for historical queries. arXiv:1302.5549, 2013."}, {"ref": "[34] I. Krommidas and C. Zaroliagis. An experimental study of algorithms for fully dynamic transitive closure. J. Exp. Algorithmics, 12, 2008."}, {"ref": "[35] J. Kunegis. Konect: The koblenz network collection. In WWW, 2013."}, {"ref": "[36] J. Leskovec, J. Kleinberg, and C. Faloutsos. Graphs over time: densification laws, shrinking diameters and possible explanations. In KDD, 2005."}, {"ref": "[37] D. A. Levin and Y. Peres. Markov Chains and Mixing Times, 2nd Edition. American Mathematical Society (first edition, 2008), 2017."}, {"ref": "[38] O. Michail. An introduction to temporal graphs: An algorithmic perspective. Internet Math., 12(4), 2016."}, {"ref": "[39] V. Z. Moffitt and J. Stoyanovich. Portal: A query language for evolving graphs. CoRR, abs/1602.00773, 2016."}, {"ref": "[40] V. Z. Moffitt and J. Stoyanovich. Towards sequenced semantics for evolving graphs. In EDBT, 2017."}, {"ref": "[41] M. Newman. Networks: an introduction. Oxford university press, 2010."}, {"ref": "[42] V. Nicosia, J. Tang, C. Mascolo, M. Musolesi, G. Russo, and V. Latora. Graph metrics for temporal networks. In Temporal networks, pages 15\u2013 40. Springer, 2013."}, {"ref": "[43] R. K. Pan and J. Saramaki. Path lengths, correlations, and centrality in \u00a8 temporal networks. Phys. Rev. E, 84(1), 2011."}, {"ref": "[44] L. Roditty and U. Zwick. Improved dynamic reachability algorithms for directed graphs. SIAM J. Comput., 37(5):1455\u20131471, 2008."}, {"ref": "[45] L. Roditty and U. Zwick. A fully dynamic reachability algorithm for directed graphs with an almost linear update time. SIAM J. Comput., 45(3):712\u2013733, 2016."}, {"ref": "[46] R. Schenkel, A. Theobald, and G. Weikum. Efficient creation and incremental maintenance of the hopi index for complex xml document collections. In ICDE, 2005."}, {"ref": "[47] K. Semertzidis and E. Pitoura. Time traveling in graphs using a graph database. In EDBT/ICDT Workshops, 2016."}, {"ref": "[48] K. Semertzidis, E. Pitoura, and K. Lillis. Timereach: Historical reachability queries on evolving graphs. In EDBT, 2015."}, {"ref": "[49] N. Sengupta, A. Bagchi, M. Ramanath, and S. Bedathur. Arrow: Approximating reachability using random-walks over web-scale graphs. http://www.cse.iitd.ernet.in/\u223cneha/Reachability.pdf."}, {"ref": "[50] S. Seufert, A. Anand, S. Bedathur, and G. Weikum. Ferrari: Flexible and efficient reachability range assignment for graph indexing. In ICDE, 2013."}, {"ref": "[51] D. Spielman. Lecture 8: Diameter, doubling and applications. Online lecture notes for \"Spectral Graph Theory and Applications\", Fall 2004, Yale University. URL:http://www.cs.yale.edu/homes/spielman/eigs/."}, {"ref": "[52] M. Starnini, A. Baronchelli, A. Barrat, and R. Pastor-Satorras. Random walks on temporal networks. Phys Rev E, 85(5), 2012."}, {"ref": "[53] J. Tang, M. Musolesi, C. Mascolo, and V. Latora. Temporal distance metrics for social network analysis. In WOSN, 2009."}, {"ref": "[54] H. Wu, J. Cheng, S. Huang, Y. Ke, Y. Lu, and Y. Xu. Path problems in temporal graphs. Proc. VLDB Endow., 7(9), 2014."}, {"ref": "[55] H. Wu, Y. Huang, J. Cheng, J. Li, and Y. Ke. Reachability and timebased path queries in temporal graphs. In ICDE, 2016."}, {"ref": "[56] Y. Yano, T. Akiba, Y. Iwata, and Y. Yoshida. Fast and scalable reachability queries on graphs by pruned labeling with landmarks and paths. In CIKM, 2013."}, {"ref": "[57] H. Y\u0131ld\u0131r\u0131m, V. Chaoji, and M. J. Zaki. GRAIL: a scalable index for reachability queries in very large graphs. VLDB J., 21(4):509\u2013534, 2012."}, {"ref": "[58] H. Y\u0131ld\u0131r\u0131m, V. Chaoji, and M. J. Zaki. DAGGER: A scalable index for reachability queries in large dynamic graphs. CoRR, abs/1301.0977, 2013."}, {"ref": "[59] J. X. Yu and J. Cheng. Managing and Mining Graph Data, volume 40 of Advances in Database Systems, chapter 6: Graph Reachability Queries: A Survey. Springer, 2010."}, {"ref": "[60] A. D. Zhu, W. Lin, S. Wang, and X. Xiao. Reachability queries on large dynamic graphs: A total order approach. In SIGMOD, 2014."}]}, {"author": ["Murai S", "Yoshida Y"], "title": "Estimating Walk-Based Similarities Using Random Walk", "journal": "The World Wide Web Conference. ACM, 2019", "year": 2019, "DOI": "0", "month": 0, "citations(google scholar)": 0, "abstract": "Measuring similarities between vertices is an important task in network analysis, which has numerous applications. One major approach to define a similarity between vertices is by accumulating weights of walks between them that encompasses personalized PageRank (PPR) and Katz similarity. Although many effective methods for PPR based on efficient simulation of random walks have been proposed, these techniques cannot be applied to other walkbased similarity notions because the random walk interpretation is only valid for PPR. In this paper, we propose a random-walk reduction method that reduces the computation of any walk-based similarity to the computation of a stationary distribution of a random walk. With this reduction, we can exploit techniques for PPR to compute other walk-based similarities. As a concrete application, we design an indexing method for walk-based similarities with which we can quickly estimate the similarity value of queried pairs of vertices, and theoretically analyze its approximation e ror. Our experimental results demonstrate that the instantiation of our method for Katz similarity is two orders of magnitude faster than existing methods on large real networks, without any deterioration in solution quality.", "keywords": ["0"], "reference_count": 30, "ccfClass": "0", "important": true, "references": [{"ref": "[1] Reid Andersen, Christian Borgs, Jennifer T. Chayes, John E. Hopcroft, Vahab S. Mirrokni, and Shang-Hua Teng. 2007. Local Computation of PageRank Contributions. In Proceedings of the 5th International Workshop on Algorithms and Models for the Web-Graph (WAW). 150\u2013165."}, {"ref": "[2] R. Andersen, F. Chung, and K. Lang. 2006. Local Graph Partitioning using PageRank Vectors. In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS). 475\u2013486."}, {"ref": "[3] Shumeet Baluja, Rohan Seth, D Sivakumar, Yushi Jing, Jay Yagnik, Shankar Kumar, Deepak Ravichandran, and Mohamed Aly. 2008. Video suggestion and discovery for youtube: taking random walks through the view graph. In Proceeding of the 17th International Conference on World Wide Web (WWW). 895\u2013904."}, {"ref": "[4] Siddhartha Banerjee and Peter Lofgren. 2015. Fast Bidirectional Probability Estimation in Markov Models. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems (NIPS). 1423\u20131431."}, {"ref": "[5] Alex Bavelas. 1950. Communication Patterns in Task\u00e2\u0102\u0158Oriented Groups. The Journal of the Acoustical Society of America 22, 6 (1950), 725\u2013730."}, {"ref": "[6] Murray A. Beauchamp. 1965. An improved index of centrality. Behavioral Science 10, 2 (1965), 161\u2013163."}, {"ref": "[7] Michele Benzi, Thomas M Evans, Steven P Hamilton, Massimiliano Lupo Pasini, and Stuart R Slattery. 2017. Analysis of Monte Carlo accelerated iterative methods for sparse linear systems. Numerical Linear Algebra with Applications 24, 3 (2017), e2088"}, {"ref": "[8] Ivan Dimov, Sylvain Maire, and Jean Michel Sellier. 2015. A new Walk on Equations Monte Carlo method for solving systems of linear algebraic equations. Applied Mathematical Modelling 39, 15 (2015), 4494\u20134510."}, {"ref": "[9] Pooya Esfandiar, Francesco Bonchi, David F. Gleich, Chen Greif, Laks V. S. Lakshmanan, and Byung-Won On. 2010. Fast Katz and Commuters: Efficient Estimation of Social Relatedness in Large Networks. In Proceedings of the 7th International Workshop on Algorithms and Models for the Web-Graph (WAW). 132\u2013145."}, {"ref": "[10] D\u00e1niel Fogaras, Bal\u00e1zs R\u00e1cz, K\u00e1roly Csalog\u00e1ny, and Tam\u00e1s Sarl\u00f3s. 2005. Towards Scaling Fully Personalized PageRank: Algorithms, Lower Bounds, and Experiments. Internet Mathematics 2, 3 (2005), 333\u2013358."}, {"ref": "[11] Michel Goemans. 2014. MIT Mathematics 18.310, Lecture Notes: Chernoff bounds, and some applications. https://ocw.mit.edu/courses/mathematics/ 18-310-principles-of-discrete-applied-mathematics-fall-2013/lecture-notes/ MIT18_310F13_Ch4.pdf."}, {"ref": "[12] David Goldberg, David Nichols, Brian M Oki, and Douglas Terry. 1992. Using collaborative filtering to weave an information tapestry. Commun. ACM 35, 12 (1992), 61\u201370."}, {"ref": "[13] Leo Katz. 1953. A new status index derived from sociometric analysis. Psychometrika 18, 1 (1953), 39\u201343."}, {"ref": "[14] Samamon Khemmarat and Lixin Gao. 2016. Fast Top-K Path-Based Relevance Query on Massive Graphs. IEEE Transactions on Knowledge and Data Engineering 28, 5 (2016), 1189\u20131202."}, {"ref": "[15] Joseph A Konstan, Bradley N Miller, David Maltz, Jonathan L Herlocker, Lee R Gordon, and John Riedl. 1997. GroupLens: applying collaborative filtering to Usenet news. Commun. ACM 40, 3 (1997), 77\u201387."}, {"ref": "[16] Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network Dataset Collection. http://snap.stanford.edu/data."}, {"ref": "[17] Peter Lofgren, Siddhartha Banerjee, and Ashish Goel. 2016. Personalized PageRank Estimation and Search: A Bidirectional Approach. In Proceedings of the 9th ACM International Conference on Web Search and Data Mining (WSDM). 163\u2013172."}, {"ref": "[18] Peter Lofgren, Siddhartha Banerjee, Ashish Goel, and Seshadhri Comandur. 2014. FAST-PPR: scaling personalized pagerank estimation for large graphs. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD). 1436\u20131445."}, {"ref": "[19] Peter Lofgren and Ashish Goel. 2013. Personalized PageRank to a Target Node. CoRR abs/1304.4658 (2013)."}, {"ref": "[20] Linyuan Lu and Tao Zhou. 2010. Link Prediction in Complex Networks: A Survey. CoRR abs/1010.0725 (2010)."}, {"ref": "[21] Takanori Maehara, Takuya Akiba, Yoichi Iwata, and Ken-ichi Kawarabayashi. 2014. Computing Personalized PageRank Quickly by Exploiting Graph Structures. PVLDB 7, 12 (2014), 1023\u20131034."}, {"ref": "[22] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The PageRank Citation Ranking: Bringing Order to the Web. Technical Report. Stanford InfoLab."}, {"ref": "[23] Natalia Rosca. 2006. Monte carlo methods for systems of linear equations. Studia Univ. BABES-BOLYAI Mathematica 51 (2006)."}, {"ref": "[24] Nitin Shyamkumar, Siddhartha Banerjee, and Peter Lofgren. 2016. Sublinear estimation of a single element in sparse linear systems. In Proceedings of the 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton). 856\u2013860."}, {"ref": "[25] Han Hee Song, Tae Won Cho, Vacha Dave, Yin Zhang, and Lili Qiu. 2009. Scalable proximity estimation and link prediction in online social networks. In Proceedings of the 9th ACM SIGCOMM Internet Measurement Conference (IMC). 322\u2013335."}, {"ref": "[26] Alastair J. Walker. 1977. An Efficient Method for Generating Discrete Random Variables with General Distributions. ACM Trans. Math. Softw. 3, 3 (Sept. 1977), 253\u2013256."}, {"ref": "[27] Sibo Wang, Youze Tang, Xiaokui Xiao, Yin Yang, and Zengxiang Li. 2016. HubPPR - Effective Indexing for Approximate Personalized PageRank. PVLDB 10, 3 (2016), 205\u2013216."}, {"ref": "[28] Sibo Wang, Renchi Yang, Xiaokui Xiao, Zhewei Wei, and Yin Yang. 2017. FORA: Simple and Effective Approximate Single-Source Personalized PageRank. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD). 505\u2013514."}, {"ref": "[29] Tao Wu and David F Gleich. 2016. Multi-way Monte Carlo Method for Linear Systems. arXiv preprint arXiv:1608.04361 (2016)."}, {"ref": "[30] Yubao Wu, Ruoming Jin, and Xiang Zhang. 2016. Efficient and Exact Local Search for Random Walk Based Top-K Proximity Query in Large Graphs. IEEE Transactions on Knowledge and Data Engineering 28, 5 (2016), 1160\u20131174."}]}, {"author": ["Zhang M", "Hu B", "Shi C"], "title": "Local low-rank matrix approximation with preference selection of anchor points", "journal": "26th International Conference on World Wide Web Companion. International World Wide Web Conferences Steering Committee", "year": 2017, "DOI": "0", "month": 0, "citations(google scholar)": 4, "abstract": "Matrix factorization is widely used in personalized recommender systems, text mining, and computer vision. A general assumption to construct matrix approximation is that the original matrix is of global low rank, while Joonseok Lee et al. proposed that many real matrices may be not globally low rank, and thus a locally low-rank matrix approximation method has been proposed [11]. However, this kind of matrix approximation method still leaves some important issues unsolved, for example, the randomly selecting anchor nodes. In this paper, we study the problem of the selection of anchor nodes to enhance locally low-rank matrix approximation. We propose a new model for local low-rank matrix approximation which selects anchor-points using a heuristic method. Our experiments indicate that the proposed method outperforms many state-of-the-art recommendation methods. Moreover, the proposed method can significantly improve algorithm efficiency, and it is easy to parallelize. These traits make it potential for large scale real-world recommender systems.", "keywords": ["0"], "reference_count": 25, "ccfClass": "0", "important": true, "references": [{"ref": "[1] A. Bellog\u00b4\u0131n, I. Cantador, and P. Castells. A comparative study of heterogeneous item recommendations in social systems. Information Sciences, 221:142\u2013169, 2013."}, {"ref": "[2] J. Bennett and S. Lanning. The netflix prize. In Proceedings of KDD cup and workshop, volume 2007, page 35, 2007."}, {"ref": "[3] I. Cantador, A. Bellog\u00b4\u0131n, and D. Vallet. Content-based recommendation in social tagging systems. In Proceedings of the fourth ACM conference on Recommender systems, pages 237\u2013240. ACM, 2010."}, {"ref": "[4] W. Feng and J. Wang. Incorporating heterogeneous information for personalized tag recommendation in social tagging systems. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1276\u20131284. ACM, 2012."}, {"ref": "[5] D. Goldberg, D. Nichols, B. M. Oki, and D. Terry. Using collaborative filtering to weave an information tapestry. Communications of the ACM, 35(12):61\u201370, 1992."}, {"ref": "[6] S. H. Ha. Helping online customers decide through web personalization. IEEE Intelligent systems, (6):34\u201343, 2002."}, {"ref": "[7] W. Hill, L. Stead, M. Rosenstein, and G. Furnas. Recommending and evaluating choices in a virtual community of use. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 194\u2013201. ACM Press/Addison-Wesley Publishing Co., 1995."}, {"ref": "[8] P. Kazienko and M. Kiewra. Personalized recommendation of web pages. Chapter, 10:163\u2013183, 2004."}, {"ref": "[9] Y. Koren. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 426\u2013434. ACM, 2008."}, {"ref": "[10] J. Lee, S. Bengio, S. Kim, G. Lebanon, and Y. Singer. Local collaborative ranking. In Proceedings of the 23rd international conference on World wide web, pages 85\u201396. ACM, 2014."}, {"ref": "[11] J. Lee, S. Kim, G. Lebanon, and Y. Singer. Local low-rank matrix approximation. In Proceedings of The 30th International Conference on Machine Learning, pages 82\u201390, 2013."}, {"ref": "[12] C. Luo, W. Pang, Z. Wang, and C. Lin. Hete-cf: Social-based collaborative filtering recommendation using heterogeneous relations. In Data Mining (ICDM), 2014 IEEE International Conference on, pages 917\u2013922. IEEE, 2014."}, {"ref": "[13] H. Ma, I. King, and M. R. Lyu. Learning to recommend with social trust ensemble. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 203\u2013210. ACM, 2009."}, {"ref": "[14] H. Ma, H. Yang, M. R. Lyu, and I. King. Sorec: socia recommendation using probabilistic matrix factorization. In Proceedings of the 17th ACM conference on Information and knowledge management, pages 931\u2013940. ACM, 2008."}, {"ref": "[15] H. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King. Recommender systems with social regularization. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 287\u2013296. ACM, 2011."}, {"ref": "[16] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl. Grouplens: an open architecture for collaborative filtering of netnews. In Proceedings of th 1994 ACM conference on Computer supported cooperative work, pages 175\u2013186. ACM, 1994."}, {"ref": "[17] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. Citeseer, 2011."}, {"ref": "[18] U. Shardanand and P. Maes. Social information filtering: algorithms for automating \"word of mouth\". In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 210\u2013217. ACM Press/Addison-Wesley Publishing Co., 1995."}, {"ref": "[19] C. Shi, J. Liu, F. Zhuang, P. S. Yu, and B. Wu. Integrating heterogeneous information via flexible regularization framework for recommendation. arXiv preprint arXiv:1511.03759, 2015."}, {"ref": "[20] C. Shi, Z. Zhang, P. Luo, P. S. Yu, Y. Yue, and B. Wu. Semantic path based personalized recommendation on weighted heterogeneous information networks. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 453\u2013462. ACM, 2015."}, {"ref": "[21] M. P. Wand and M. C. Jones. Kernel smoothing. Crc Press, 1994."}, {"ref": "[22] X. Yang, H. Steck, and Y. Liu. Circle-based recommendation in online social networks. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1267\u20131275. ACM, 2012."}, {"ref": "[23] X. Yu, H. Ma, B.-J. P. Hsu, and J. Han. On building entity recommender systems using user click log and freebase knowledge. In Proceedings of the 7th ACM international conference on Web search and data mining, pages 263\u2013272. ACM, 2014."}, {"ref": "[24] X. Yu, X. Ren, Y. Sun, Q. Gu, B. Sturt, U. Khandelwal, B. Norick, and J. Han. Personalized entity recommendation: A heterogeneous information network approach. In Proceedings of the 7th ACM international conference on Web search and data mining, pages 283\u2013292. ACM, 2014."}, {"ref": "[25] X. Yu, X. Ren, Y. Sun, B. Sturt, U. Khandelwal, Q. Gu, B. Norick, and J. Han. Recommendation in heterogeneous information networks with implicit user feedback. In Proceedings of the 7th ACM conference on Recommender systems, pages 347\u2013350. ACM, 2013."}]}, {"author": ["W\u0105s T", "Rahwan T", "Skibski O"], "title": "Random Walk Decay Centrality", "journal": "AAAI Conference on Artificial Intelligence. 2019", "year": 2019, "DOI": "0", "month": 0, "citations(google scholar)": 0, "abstract": "We propose a new centrality measure, called the Random Walk Decay centrality. While most centralities in the literature are based on the notion of shortest paths, this new centrality measure stems from the random walk on the network. We provide an axiomatic characterization and show that the new centrality is closely related to PageRank. More in detail, we show that replacing only one axiom, called Lack of SelfImpact, with another one, called Edge Swap, results in the new axiomatization of PageRank. Finally, we argue that Lack of Self-Impact is desirable in various settings and explain why violating Edge Swap may be beneficial and may contribute to promoting diversity in the centrality measure.", "keywords": ["0"], "reference_count": 24, "ccfClass": "0", "important": true, "references": []}, {"author": ["Oliveira R I", "Peres Y"], "title": "Random walks on graphs: new bounds on hitting, meeting, coalescing and returning", "journal": "2019 Proceedings of the Sixteenth Workshop on Analytic Algorithmics and Combinatorics (ANALCO). Society for Industrial and Applied Mathematics", "year": 2019, "DOI": "0", "month": 0, "citations(google scholar)": 5, "abstract": "We prove new results on lazy random walks on finite graphs. To start, we obtain new estimates on return probabilities P t(x; x) and the maximum expected hitting time thit, both in terms of the relaxation time. We also prove a discretetime version of the first-named author\u2019s \\Meeting time lemma\" that bounds the probability of a random walk hitting a deterministic trajectory in terms of hitting times of static vertices. The meeting time result is then used to bound the expected full coalescence time of multiple random walks over a graph. This last theorem is a discretetime version of a result by the first-named author, which had been previously conjectured by Aldous and Fill. Our bounds improve on recent results by Lyons and OveisGharan; Kanade et al; and (in certain regimes) Cooper et al", "keywords": ["0"], "reference_count": 12, "ccfClass": "0", "important": true, "references": [{"ref": "[1] D. Aldous and J. A. Fill, Reversible Markov Chains and Random Walks on Graphs, manuscript available from www.stat.berkeley.edu/\u223caldous/RWG/book.html (1994)."}, {"ref": "[2] A. Ben-Hamou, R. I. Oliveira, and Y. Peres. Estimating graph parameters via random walks with restarts, in Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms (2018), pp. 1702{ 1714."}, {"ref": "[3] G. Brightwell and P. Winkler, Maximum hitting time for random walks on graphs, Random Structures and Algorithms, 1 (1990), pp. 263 { 276."}, {"ref": "[4] C. Cooper, R. Elssser, H. Ono, and T. Radzik, Coalescing random walks and voting on connected graphs, SIAM Journal of Discrete Mathematics, 27 (2013), pp. 1748 { 1758."}, {"ref": "[5] V. Kanade, F. Mallman-Trenn, T. Sauerwald, On coalescence time in graphs { When is coalescing as fast as meeting?, arXiv:1611.02460 (2016), 78 pp."}, {"ref": "[6] S. Griffiths, R. Kang, R. I. Oliveira, V. Patel, Tight inequalities among set hitting times in Markov chains, Proceedings of the American Mathematical Society, 142 (2014), pp. 3285 { 3298."}, {"ref": "[7] D. Levin and Y. Peres, Markov chains and mixing times (2nd edition), American Mathematical Society (2017)."}, {"ref": "[8] R. Lyons, S. Oveis-Gharan, Sharp bounds on random walk eigenvalues via spectral embedding, International Mathematics Research Notices, rnx082 (2017)."}, {"ref": "[9] R. I. Oliveira, Mixing and hitting times for finite Markov chains, Electronic Journal of Probability 17 (2012), paper no. 70, 12 pp."}, {"ref": "[10] R. I. Oliveira, On the coalescence time of reversible random walks, Transactions of the American Mathematical Society, 364 (2012), pp. 2109 { 2128."}, {"ref": "[11] Y. Peres, T. Sauerwald, P. Sousi, and A. Stauffer, Intersection and mixing times for reversible chains, Electronic Journal of Probability, 22 (2017), paper no. 12, 16 pp."}, {"ref": "[12] Y. Peres and P. Sousi, Mixing times are hitting times of large sets, Journal of Theoretical Probability, 28 (2015), pp. 488 { 519 ."}]}, {"author": ["Ding M C", "Szeto K Y"], "title": "Selection of random walkers that optimizes the global mean first-passage time for search in complex networks", "journal": "Procedia Computer Science.2017", "year": 2017, "DOI": "0", "month": 0, "citations(google scholar)": 1, "abstract": "We design a method to optimize the global mean first-passage time (GMFPT) of multiple random walkers searching in complex networks for a general target, without specifying the property of the target node. According to the Laplace transformed formula of the GMFPT, we can equivalently minimize the overlap between the probability distribution of sites visited by the random walkers. We employ a mutation only genetic algorithm to solve this optimization problem using a population of walkers with different starting positions and a corresponding mutation matrix to modify them. The numerical experiments on two kinds of random networks (WS and BA) show satisfactory results in selecting the origins for the walkers to achieve mini We design a method to optimize the global mean first-passage time (GMFPT) of multiple random walkers searching in complex networks for a general target, without specifying the property of the target node. According to the Laplace transformed formula of the GMFPT, we can equivalently minimize the overlap between the probability distribution of sites visited by the random walkers. We employ a mutation only genetic algorithm to solve this optimization problem using a population of walkers with different starting positions and a corresponding mutation matrix to modify them. The numerical experiments on two kinds of random networks (WS and BA) show satisfactory results in selecting the origins for the walkers to achieve minimum overlap. Our method thus provides guidance for setting up the search process by multiple random walkers on complex networks.", "keywords": ["Random Walk", "First-Passage Time", "Complex Network", "Genetic Algorithm", "Search"], "reference_count": 9, "ccfClass": "0", "important": true, "references": [{"ref": "[1] R. Albert and A.L. Barab\u00b4asi. Statistical mechanics of complex networks. Rev. Mod. Phys., 74:47\u201397, Jan 2002."}, {"ref": "[2] B.D. Hughes. Random walks and random environments. Vol. 1. Random walks. Oxford, 1995."}, {"ref": "[3] S. Condamin, O. B\u00b4enichou, and M. Moreau. Random walks and brownian motion. Phys. Rev. E, 75:021111, Feb 2007."}, {"ref": "[4] J. D. Noh and H. Rieger. Random walks on complex networks. Phys. Rev. Lett., 92:118701, Mar 2004."}, {"ref": "[5] H. W. Lau and K. Y. Szeto. Asymptotic analysis of first passage time in complex networks. EPL (Europhysics Letters), 90(4):40005, 2010."}, {"ref": "[6] John H Holland. Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence. U Michigan Press, 1975."}, {"ref": "[7] K. Y. Szeto and J. Zhang. Adaptive Genetic Algorithm and Quasi-parallel Genetic Algorithm: Application to Knapsack Problem, pages 189\u2013196. Springer, Berlin, Heidelberg, 2006."}, {"ref": "[8] N. L. Law and K. Y. Szeto. Adaptive genetic algorithm with mutation and crossover matrices. In 12th Intl. Joint Conf. on Artificial Intelligence (IJCAI\u201907), pages 2330\u20132333, 2007."}, {"ref": "[9] D.G. Wu and K. Y. Szeto. Applications of genetic algorithm on optimal sequence for parrondo games. In 6th Intl. Joint Conf. on Computational Intelligence (IJCCI\u201914), page 30, 2014. 5"}]}, {"author": ["Peng Cui", "Xiao Wang", "Jian Pei", "Wenwu Zhu"], "title": "A Survey on Network Embedding", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": 2018, "DOI": "10.1109/TKDE.2018.2849727", "month": 0, "citations(google scholar)": 139, "abstract": "Network embedding assigns nodes in a network to low-dimensional representations and effectively preserves the network structure. Recently, a signi\ufb01cant amount of progresses have been made toward this emerging network analysis paradigm. In this survey, we focus on categorizing and then reviewing the current development on network embedding methods, and point out its future research directions. We \ufb01rst summarize the motivation of network embedding. We discuss the classical graph embedding algorithms and their relationship with network embedding. Afterwards and primarily, we provide a comprehensive overview of a large number of network embedding methods in a systematic manner, covering the structure- and property-preserving network embedding methods, the network embedding methods with side information, and the advanced information preserving network embedding methods. Moreover, several evaluation approaches for network embedding and some useful online resources, including the network data sets and softwares, are reviewed, too. Finally, we discuss the framework of exploiting these network embedding methods to build an effective system and point out some potential future directions.", "keywords": ["Network embedding", "graph embedding", "network analysis", "data science"], "reference_count": 96, "ccfClass": "A", "important": true, "references": [{"ref": "[1] C. L. Staudt, A. Sazonovs, and H. Meyerhenke, \"Networkit: A tool suite for large-scale network analysis,\" Netw. Sci., Cambridge University Press, vol. 4, pp. 508\u2013530, 2016."}, {"ref": "[2] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad, \"Collective classi\ufb01cation in network data,\" AI Mag., vol. 29, no. 3, 2008, Art. no. 93."}, {"ref": "[3] B. Perozzi, R. Al-Rfou, and S. Skiena, \"Deepwalk: Online learning of social representations,\" in Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2014, pp. 701\u2013710."}, {"ref": "[4] X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang, \"Community preserving network embedding,\" in Proc. 31th AAAI Conf. Artif. Intell., AAAI Press, 2017, pp. 203\u2013209."}, {"ref": "[5] I. Herman, G. Melanc\u00b8on, and M. S. Marshall, \"Graph visualization and navigation in information visualization: A survey,\" IEEE Trans. Vis. Comput. Graph., vol. 6, no. 1, pp. 24\u201343, Jan.-Mar. 2000."}, {"ref": "[6] D. Wang, P. Cui, and W. Zhu, \"Structural deep network embedding,\" in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1225\u20131234."}, {"ref": "[7] D. Liben-Nowell and J. Kleinberg, \"The link-prediction problem for social networks,\" J. Assoc. Inf. Sci. Technol., vol. 58, no. 7, pp. 1019\u20131031, 2007."}, {"ref": "[8] M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu, \"Asymmetric transitivity preserving graph embedding,\" in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 672\u2013681."}, {"ref": "[9] Y. Fu and Y. Ma, Graph Embedding for Pattern Analysis. New York, NY, USA: Springer Science & Business Media, 2012."}, {"ref": "[10] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, \"Line: Large-scale information network embedding,\" in Proc. 24th Int. Conf. World Wide Web, 2015, pp. 1067\u20131077."}, {"ref": "[11] H. Huang, J. Tang, S. Wu, L. Liu, et al., \"Mining triadic closure patterns in social networks,\" in Proc. 23rd Int. Conf. World Wide Web, 2014, pp. 499\u2013504."}, {"ref": "[12] D. Cartwright and F. Harary, \"Structural balance: A generalization of heider\u2019s theory,\" Psychological Rev., vol. 63, no. 5, 1956, Art. no. 277."}, {"ref": "[13] S. Wang, J. Tang, C. Aggarwal, Y. Chang, and H. Liu, \"Signed network embedding in social media,\" in Proc. SIAM Int. Conf. Data Mining, 2017, pp. 327\u2013335."}, {"ref": "[14] C. Tu, W. Zhang, Z. Liu, and M. Sun, \"Max-margin deepwalk: Discriminative learning of network representation,\" in Proc. 25th Int. Joint Conf. Artif. Intell., 2016, pp. 3889\u20133895."}, {"ref": "[15] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, \"Network representation learning with rich text information,\" in Proc. 24th Int. Joint Conf. Artif. Intell., 2015, pp. 2111\u20132117."}, {"ref": "[16] S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang, \"Heterogeneous network embedding via deep architectures,\" in Proc. 21th ACM SIGKDD Int. Conf. Knowl Discovery Data Mining, 2015, pp. 119\u2013128."}, {"ref": "[17] N. Natarajan and I. S. Dhillon, \"Inductive matrix completion for predicting gene\u2013disease associations,\" Bioinf., vol. 30, no. 12, pp. i60\u2013i68, 2014."}, {"ref": "[18] C. Li, J. Ma, X. Guo, and Q. Mei, \"Deepcas: An end-to-end predictor of information cascades,\" in Proc. 26th Int. Conf. World Wide Web, 2017, pp. 577\u2013586."}, {"ref": "[19] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei, \"End-to-end learning of action detection from frame glimpses in videos,\" in Proc. IEEE Conf. Comput. Vision Pattern Recognit., 2016, pp. 2678\u2013 2687."}, {"ref": "[20] X. Yang, Y.-N. Chen, D. Hakkani-T\u20acur, P. Crook, X. Li, J. Gao, and L. Deng, \"End-to-end joint learning of natural language understanding and dialogue manager,\" in Proc. IEEE Int. Conf. Acoust. Speech Signal Process., 2017, pp. 5690\u20135694."}, {"ref": "[21] R. Hu, C. C. Aggarwal, S. Ma, and J. Huai, \"An embedding approach to anomaly detection,\" in Proc. IEEE 32nd Int. Conf. Data Eng., 2016, pp. 385\u2013396."}, {"ref": "[22] T. Man, H. Shen, S. Liu, X. Jin, and X. Cheng, \"Predict anchor links across social networks via an embedding approach,\" in Proc. 25th Int. Joint Conf. Artif. Intell., 2016, pp. 1823\u20131829."}, {"ref": "[23] T. Chen and Y. Sun, \"Task-guided and path-augmented heterogeneous network embedding for author identi\ufb01cation,\" in Proc. 10th ACM Int. Conf. Web Search Data Mining, 2017, pp. 295\u2013304."}, {"ref": "[24] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, \"Distributed representations of words and phrases and their compositionality,\" in Proc. Adv. Neural Inf. Process. Syst., 2013, pp. 3111\u20133119."}, {"ref": "[25] A. Grover and J. Leskovec, \"node2vec: Scalable feature learning for networks,\" in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1225\u20131234."}, {"ref": "[26] S. Cao, W. Lu, and Q. Xu, \"Deep neural networks for learning graph representations,\" in Proc. 30th AAAI Conf. Artif. Intell., 2016, pp. 1145\u20131152."}, {"ref": "[27] S. Yan, D. Xu, B. Zhang, and H.-J. Zhang, \"Graph embedding: A general framework for dimensionality reduction,\" in Proc. IEEE Comput. Soc. Conf. Comput. Vision Pattern Recognit., 2005, vol. 2, pp. 830\u2013837."}, {"ref": "[28] J. B. Tenenbaum, V. De Silva, and J. C. Langford, \"A global geometric framework for nonlinear dimensionality reduction,\" Sci., vol. 290, no. 5500, pp. 2319\u20132323, 2000."}, {"ref": "[29] S. T. Roweis and L. K. Saul, \"Nonlinear dimensionality reduction by locally linear embedding,\" Sci., vol. 290, no. 5500, pp. 2323\u2013 2326, 2000."}, {"ref": "[30] M. Belkin and P. Niyogi, \"Laplacian eigenmaps and spectral techniques for embedding and clustering,\" in Proc. Adv. Neural Inf. Process. Syst., 2002, pp. 585\u2013591."}, {"ref": "[31] N. Berline, E. Getzler, and M. Vergne, Heat Kernels and Dirac Operators. New York, NY, USA: Springer Science & Business Media, 2003."}, {"ref": "[32] X. He and P. Niyogi, \"Locality preserving projections,\" in Proc. Adv. Neural Inf. Process. Syst., 2004, pp. 153\u2013160."}, {"ref": "[33] T. Mikolov, K. Chen, G. Corrado, and J. Dean, \"Ef\ufb01cient estimation of word representations in vector space,\" in Proc. ICLR Workshops Track, 2013."}, {"ref": "[34] S. Cao, W. Lu, and Q. Xu, \"Grarep: Learning graph representations with global structural information,\" in Proc. 24th ACM Int. Conf. Inf. Knowl. Manage, 2015, pp. 891\u2013900."}, {"ref": "[35] M. Girvan and M. E. Newman, \"Community structure in social and biological networks,\" Proc. Nat. Academy Sci. USA, vol. 99, no. 12, pp. 7821\u20137826, 2002."}, {"ref": "[36] D. D. Lee and H. S. Seung, \"Algorithms for non-negative matrix factorization,\" in Proc. Adv. Neural Inf. Process. Syst., 2001, pp. 556\u2013 562."}, {"ref": "[37] M. E. Newman, \"Finding community structure in networks using the eigenvectors of matrices,\" Phys. Rev. E, vol. 74, no. 3, 2006, Art. no. 036104."}, {"ref": "[38] O. Levy and Y. Goldberg, \"Neural word embedding as implicit matrix factorization,\" in Proc. Adv. Neural Inf. Process. Syst., 2014, pp. 2177\u20132185."}, {"ref": "[39] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,\" J. Mach. Learn. Res., vol. 11, no. Dec, pp. 3371\u20133408, 2010."}, {"ref": "[40] S. Chen, S. Niu, L. Akoglu, J. Kova\u0003cevi\u0004c, and C. Faloutsos, \"Fast, warped graph embedding: Unifying framework and one-click algorithm,\" arXiv preprint arXiv:1702.05764, 2017."}, {"ref": "[41] P. Yanardag and S. Vishwanathan, \"Deep graph kernels,\" in Proc. 21th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2015, pp. 1365\u20131374."}, {"ref": "[42] M. Niepert, M. Ahmed, and K. Kutzkov, \"Learning convolutional neural networks for graphs,\" in Proc. Int. Conf. Mach. Learn., 2016, pp. 2014\u20132023."}, {"ref": "[43] Y. LeCun, Y. Bengio, and G. Hinton, \"Deep learning,\" Nature, vol. 521, no. 7553, 2015, Art. no. 436."}, {"ref": "[44] M. Ou, P. Cui, F. Wang, J. Wang, and W. Zhu, \"Non-transitive hashing with latent similarity components,\" in Proc. 21th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2015, pp. 895\u2013 904."}, {"ref": "[45] L. Katz, \"A new status index derived from sociometric analysis,\" Psychometrika, vol. 18, no. 1, pp. 39\u201343, 1953."}, {"ref": "[46] L. A. Adamic and E. Adar, \"Friends and neighbors on the web,\" Soc. Netw., vol. 25, no. 3, pp. 211\u2013230, 2003."}, {"ref": "[47] C. C. Paige and M. A. Saunders, \"Towards a generalized singular value decomposition,\" SIAM J. Numerical Anal., vol. 18, no. 3, pp. 398\u2013405, 1981."}, {"ref": "[48] M. Cygan, M. Pilipczuk, M. Pilipczuk, and J. O. Wojtaszczyk, \"Sitting closer to friends than enemies, revisited,\" Theory Comput. Syst., vol. 56, no. 2, pp. 394\u2013405, 2015."}, {"ref": "[49] M. A. Hearst, S. T. Dumais, E. Osuna, J. Platt, and B. Scholkopf, \"Support vector machines,\" IEEE Intell. Syst. Appl., vol. 13, no. 4, pp. 18\u201328, July-Aug. 1988."}, {"ref": "[50] T. M. Le and H. W. Lauw, \"Probabilistic latent document network embedding,\" in Proc. IEEE Int. Conf. Data Mining, 2014, pp. 270\u2013 279."}, {"ref": "[51] J. Chang and D. M. Blei, \"Relational topic models for document networks,\" in Proc. Int. Conf. Artif. Intell. Statist., 2009, pp. 81\u201388."}, {"ref": "[52] X. Sun, J. Guo, X. Ding, and T. Liu, \"A general framework for content-enhanced network representation learning,\" arXiv:1610.02906, 2016."}, {"ref": "[53] S. Pan, J. Wu, X. Zhu, C. Zhang, and Y. Wang, \"Tri-party deep network representation,\" Netw., vol. 11, no. 9, 2016, Art. no. 12."}, {"ref": "[54] X. Huang, J. Li, and X. Hu, \"Label informed attributed network embedding,\" in Proc. 10th ACM Int. Conf. Web Search Data Mining, 2017, pp. 731\u2013739."}, {"ref": "[55] F. R. K. Chung and F. C. Graham, \"Spectral graph theory,\" American Mathematical Soc., no. 92, 1997."}, {"ref": "[56] X. Huang, J. Li, and X. Hu, \"Accelerated attributed network embedding,\" in Proc. SIAM Int. Conf. Data Mining, 2017, pp. 633\u2013 641."}, {"ref": "[57] X. Wei, L. Xu, B. Cao, and P. S. Yu, \"Cross view link prediction by learning noise-resilient representation consensus,\" in Proc. 26th Int. Conf. World Wide Web, 2017, pp. 1611\u20131619."}, {"ref": "[58] Y. Jacob, L. Denoyer, and P. Gallinari, \"Learning latent representations of nodes for classifying in heterogeneous social networks,\" in Proc. 7th ACM Int. Conf. Web Search Data Mining, 2014, pp. 373\u2013 382."}, {"ref": "[59] Z. Huang and N. Mamoulis, \"Heterogeneous information network embedding for meta path based proximity,\" arXiv: 1701.05291, 2017."}, {"ref": "[60] Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu, \"Pathsim: Meta pathbased top-k similarity search in heterogeneous information networks,\" Proc. VLDB Endowment, vol. 4, no. 11, pp. 992\u20131003, 2011."}, {"ref": "[61] L. Xu, X. Wei, J. Cao, and P. S. Yu, \"Embedding of embedding (eoe): Joint embedding for coupled heterogeneous networks,\" in Proc. 10th ACM Int. Conf. Web Search Data Mining, 2017, pp. 741\u2013 749."}, {"ref": "[62] A. Guille, H. Hacid, C. Favre, and D. A. Zighed, \"Information diffusion in online social networks: A survey,\" ACM SIGMOD Record, vol. 42, no. 2, pp. 17\u201328, 2013."}, {"ref": "[63] S. Bourigault, C. Lagnier, S. Lamprier, L. Denoyer, and P. Gallinari, \"Learning social network embeddings for predicting information diffusion,\" in Proc. 7th ACM Int. Conf. Web search Data Mining, 2014, pp. 393\u2013402."}, {"ref": "[64] S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\" Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, 1997."}, {"ref": "[65] T. Mikolov, M. Kara\ufb01\u0004at, L. Burget, J. Cernock\u0004y, and S. Khudanpur, \"Recurrent neural network based language model,\" Interspeech, vol.2,2010,Art.no.3."}, {"ref": "[66] D. W. Ruck, S. K. Rogers, M. Kabrisky, M. E. Oxley, and B. W. Suter, \"The multilayer perceptron as an approximation to a bayes optimal discriminant function,\" IEEE Trans. Neural Netw., vol. 1, no. 4, pp. 296\u2013298, Dec. 1990."}, {"ref": "[67] L. Akoglu, H. Tong, and D. Koutra, \"Graph based anomaly detection and description: A survey,\" Data Mining Knowl. Discovery, vol. 29, no. 3, pp. 626\u2013688, 2015."}, {"ref": "[68] R. S. Burt, \"Structural holes and good ideas,\" Amer. J. Sociology, vol. 110, no. 2, pp. 349\u2013399, 2004."}, {"ref": "[69] M. Bayati, M. Gerritsen, D. F. Gleich, A. Saberi, and Y. Wang, \"Algorithms for large, sparse network alignment problems,\" in Proc. 9th IEEE Int. Conf. Data Mining, 2009, pp. 705\u2013710."}, {"ref": "[70] L. Tang and H. Liu, \"Relational learning via latent social dimensions,\" in Proc. 15th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2009, pp. 817\u2013826."}, {"ref": "[71] L. tang and H. Liu, \"Scalable learning of collective behavior based on sparse social dimensions,\" in Proc. 18th ACM Conf. Inf. Knowl. Manage., 2009, pp. 1107\u20131116."}, {"ref": "[72] M. De Choudhury, Y.-R. Lin, H. Sundaram, K. S. Candan, L. Xie, A. Kelliher et al., \"How does the data sampling strategy impact the discovery of information diffusion in social media?\" ICWSM, vol. 10, pp. 34\u201341, 2010."}, {"ref": "[73] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su, \"Arnetminer: extraction and mining of academic social networks,\" in Proc. 14th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2008, pp. 990\u2013998."}, {"ref": "[74] A. K. McCallum, K. Nigam, J. Rennie, and K. Seymore, \"Automating the construction of internet portals with machine learning,\" Inf. Retrieval, vol. 3, no. 2, pp. 127\u2013163, 2000."}, {"ref": "[75] J. Leskovec, J. Kleinberg, and C. Faloutsos, \"Graph evolution: Densi\ufb01cation and shrinking diameters,\" ACM Trans. Knowl. Discovery Data, vol. 1, no. 1, 2007, Art. no. 2."}, {"ref": "[76] J. Leskovec and A. Krevl, \"Snap datasets: Stanford large network dataset collection (2014),\" 2016. [Online]. Available: http://snap. stanford.edu/data"}, {"ref": "[77] M. Mahoney, \"Large text compression benchmark,\" 2011, http:// www. mattmahoney.net/text/text.html"}, {"ref": "[78] B.-J. Breitkreutz, C. Stark, T. Reguly, L. Boucher, A. Breitkreutz, M. Livstone, R. Oughtred, D. H. Lackner, J. B\u20acahler, V. Wood, et al., \"The biogrid interaction database: 2008 update,\" Nucleic Acids Res., vol. 36, no. suppl_1, pp. D637\u2013D640, 2007."}, {"ref": "[79] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, \"Liblinear: A library for large linear classi\ufb01cation,\" J. Mach. Learn. Res., vol. 9, no. Aug, pp. 1871\u20131874, 2008."}, {"ref": "[80] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng, \"Nuswide: A real-world web image database from national university of singapore,\" in Proc. ACM Int. Conf. Image Video Retrieval, 2009, Art. no. 48."}, {"ref": "[81] M. Belkin and P. Niyogi, \"Laplacian eigenmaps for dimensionality reduction and data representation,\" Neural Comput., vol. 15, no. 6, pp. 1373\u20131396, 2003."}, {"ref": "[82] L. L\u20acu and T. Zhou, \"Link prediction in complex networks: A survey,\" Physica A: Statist. Mech. Appl., vol. 390, no. 6, pp. 1150\u2013 1170, 2011."}, {"ref": "[83] L. Getoor and C. P. Diehl, \"Link mining: a survey,\" ACM SIGKDD Explorations Newsletter, vol. 7, no. 2, pp. 3\u201312, 2005."}, {"ref": "[84] J. MacQueen, et al., \"Some methods for classi\ufb01cation and analysis of multivariate observations,\" in Proc.e 5th Berkeley Symp. Math. Statist. Probability, 1967, pp. 281\u2013297."}, {"ref": "[85] D. Cai, X. He, J. Han, and T. S. Huang, \"Graph regularized nonnegative matrix factorization for data representation,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 8, pp. 1548\u20131560, Aug. 2011."}, {"ref": "[86] A. L. Traud, P. J. Mucha, and M. A. Porter, \"Social structure of facebook networks,\" Physica A: Statist. Mech. Appl., vol. 391, no. 16, pp. 4165\u20134180, 2012."}, {"ref": "[87] F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu, \"Learning deep representations for graph clustering\" in Proc. AAAI, 2014, pp. 1293\u20131299."}, {"ref": "[88] Z. Huang, Y. Zheng, R. Cheng, Y. Sun, N. Mamoulis, and X. Li, \"Meta structure: Computing relevance in large heterogeneous information networks,\" in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1595\u20131604."}, {"ref": "[89] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor, \"Freebase: A collaboratively created graph database for structuring human knowledge,\" in Proc. ACM SIGMOD Int. Conf. Manage. Data, 2008, pp. 1247\u20131250."}, {"ref": "[90] M. Laurens van der and G. Hinton, \"Visualizing data using t-sne,\" J. Mach. Learn. Res., vol. 9, pp. 2579\u20132605, 2008."}, {"ref": "[91] K. W. Lim and W. Buntine, \"Bibliographic analysis with the citation network topic model,\" arXiv:1609.06826, 2016."}, {"ref": "[92] A. R. Benson, D. F. Gleich, and J. Leskovec, \"Higher-order organization of complex networks,\" Sci., vol. 353, no. 6295, pp. 163\u2013166, 2016."}, {"ref": "[93] E. Seo, P. Mohapatra, and T. Abdelzaher, \"Identifying rumors and their sources in social networks,\" SPIE Defense Secur. Sens., pp. 83 891I\u201383 891I, 2012."}, {"ref": "[94] Q. Zhang, S. Zhang, J. Dong, J. Xiong, and X. Cheng, \"Automatic detection of rumor on social network,\" in Proc. Natural Lang. Process. Chinese Comput., 2015, pp. 113\u2013122."}, {"ref": "[95] J. Tang, T. Lou, and J. Kleinberg, \"Inferring social ties across heterogenous networks,\" in Proc. 5th ACM Int. Conf. Web Search Data Mining, 2012, pp. 743\u2013752."}, {"ref": "[96] D. Krioukov, F. Papadopoulos, M. Kitsak, A. Vahdat, and M. Bogun\u0004a, \"Hyperbolic geometry of complex networks,\" Phys. Rev. E, vol. 82, no. 3, 2010, Art. no. 036106."}]}, {"author": ["Palash Goyal", "Di Huang", "Sujit Rokka Chhetri", "Arquimedes Canedo", "Jaya Shree", "Evan Patterson"], "title": "Graph Representation Ensemble Learning", "journal": "arXiv preprint arXiv:1909.02811", "year": 2019, "DOI": "", "month": 0, "citations(google scholar)": 1, "abstract": "Representation learning on graphs has been gaining attention due to its wide applicability in predicting missing links, and classifying and recommending nodes. Most embedding methods aim to preserve certain properties of the original graph in the low dimensional space. However, real world graphs have a combination of several properties which are dif\ufb01cult to characterize and capture by a single approach. In thiswork,weintroducetheproblemofgraphrepresentation ensemble learning and provide a \ufb01rst of its kind framework to aggregate multiple graph embedding methods ef\ufb01ciently. We provide analysis of our framework and analyze \u2013 theoreticallyandempirically\u2013thedependencebetweenstate-ofthe-artembeddingmethods.Wetestourmodelsonthenode classi\ufb01cation task on four real world graphs and show that proposed ensemble approaches can outperform the state-ofthe-art methods by up to 8% on macro-F1. We further show that the approach is even more bene\ufb01cial for underrepresented classes providing an improvement of up to 12%.", "keywords": [""], "reference_count": 40, "ccfClass": "", "important": true, "references": [{"ref": "[Ahmed et al. 2013a] Ahmed, A.; Shervashidze, N.; Narayanamurthy,S.;Josifovski,V.;andSmola,A.J. 2013a. Distributed large-scale natural graph factorization. In Proceedings of the 22nd international conference on World Wide Web, 37\u201348. ACM."}, {"ref": "[Ahmed et al. 2013b] Ahmed, A.; Shervashidze, N.; Narayanamurthy,S.;Josifovski,V.;andSmola,A.J. 2013b. Distributed large-scale natural graph factorization. In Proceedings of the 22nd international conference on World Wide Web, 37\u201348. ACM."}, {"ref": "[Belkin and Niyogi 2001] Belkin, M., and Niyogi, P. 2001. Laplacianeigenmapsandspectraltechniquesforembedding and clustering. In NIPS, volume 14, 585\u2013591."}, {"ref": "[Belkin and Niyogi 2002] Belkin, M., and Niyogi, P. 2002. Laplacianeigenmapsandspectraltechniquesforembedding and clustering. In Advances in neural information processing systems, 585\u2013591."}, {"ref": "[Benesty et al. 2009] Benesty, J.; Chen, J.; Huang, Y.; and Cohen, I. 2009. Pearson correlation coef\ufb01cient. In Noise reduction in speech processing. Springer. 1\u20134."}, {"ref": "[Breitkreutz et al. 2008] Breitkreutz,B.-J.;Stark,C.;Reguly, T.;Boucher,L.;Breitkreutz,A.;Livstone,M.;Oughtred,R.; Lackner,D.H.;B\u00a8ahler,J.;Wood,V.;etal. 2008. Thebiogrid interaction database: 2008 update. Nucleic acids research 36(suppl 1):D637\u2013D640."}, {"ref": "[Bruna et al. 2013] Bruna, J.; Zaremba, W.; Szlam, A.; and LeCun, Y. 2013. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203."}, {"ref": "[Cao, Lu, and Xu 2016] Cao, S.; Lu, W.; and Xu, Q. 2016. Deep neural networks for learning graph representations. In Proceedings of the Thirtieth AAAI Conference on Arti\ufb01cial Intelligence, 1145\u20131152. AAAI Press."}, {"ref": "[Dietterich and others 2002] Dietterich, T. G., et al. 2002. Ensemble learning. The handbook of brain theory and neural networks 2:110\u2013125."}, {"ref": "[Freeman 1978] Freeman, L. C. 1978. Centrality in social networks conceptual clari\ufb01cation. Social networks 1(3):215\u2013239."}, {"ref": "[Freeman 2000] Freeman, L. C. 2000. Visualizing social networks. Journal of social structure 1(1):4."}, {"ref": "[Gehrke, Ginsparg, and Kleinberg 2003] Gehrke, J.; Ginsparg, P.; and Kleinberg, J. 2003. Overview of the 2003 kdd cup. ACM SIGKDD Explorations 5(2)."}, {"ref": "[Goyal and Ferrara 2018] Goyal, P., and Ferrara, E. 2018. Graph embedding techniques, applications, and performance: A survey. Knowledge-Based Systems."}, {"ref": "[Goyal, Sapienza, and Ferrara 2018] Goyal,P.;Sapienza,A.; and Ferrara, E. 2018. Recommending teammates with deep neural networks. In Proceedings of the 29th on Hypertext and Social Media, 57\u201361. ACM."}, {"ref": "[Grover and Leskovec 2016a] Grover, A., and Leskovec, J. 2016a. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining, 855\u2013864. ACM."}, {"ref": "[Grover and Leskovec 2016b] Grover, A., and Leskovec, J. 2016b. node2vec:Scalablefeaturelearningfornetworks. In Proceedings of the 22nd ACM SIGKDD international conferenceonKnowledgediscoveryanddatamining,855\u2013864. ACM."}, {"ref": "[Hansen and Salamon 1990] Hansen, L. K., and Salamon, P. 1990. Neural network ensembles. IEEE Transactions on Pattern Analysis & Machine Intelligence (10):993\u20131001."}, {"ref": "[Henaff, Bruna, and LeCun 2015] Henaff,M.;Bruna,J.;and LeCun, Y. 2015. Deep convolutional networks on graphstructured data. arXiv preprint arXiv:1506.05163."}, {"ref": "[Kipf and Welling 2016a] Kipf, T. N., and Welling, M. 2016a. Semi-supervised classi\ufb01cation with graph convolutional networks. arXiv preprint arXiv:1609.02907."}, {"ref": "[Kipf and Welling 2016b] Kipf, T. N., and Welling, M. 2016b. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308."}, {"ref": "[Liaw, Wiener, and others 2002] Liaw, A.; Wiener, M.; et al. 2002. Classi\ufb01cation and regression by random forest. Rnews 2(3):18\u201322."}, {"ref": "[Lu and Getoor 2003] Lu, Q., and Getoor, L. 2003. Linkbased classi\ufb01cation. In ICML, volume 3, 496\u2013503."}, {"ref": "[Mahoney 2011] Mahoney,M. 2011. Large text compression benchmark. URL: http://www. mattmahoney. net/text/text. html."}, {"ref": "[Opitz 1999] Opitz, D. W. 1999. Feature selection for ensembles. AAAI/IAAI 379:384."}, {"ref": "[Ou et al. 2016a] Ou,M.;Cui,P.;Pei,J.;Zhang,Z.;andZhu, W. 2016a. Asymmetric transitivity preserving graph embedding. In Proc. of ACM SIGKDD, 1105\u20131114."}, {"ref": "[Ou et al. 2016b] Ou,M.;Cui,P.;Pei,J.;Zhang,Z.;andZhu, W. 2016b. Asymmetric transitivity preserving graph embedding. In Proceedings of the 22nd ACM SIGKDD internationalconferenceonKnowledgediscoveryanddatamining, 1105\u20131114. ACM."}, {"ref": "[R\u00a8atsch, Onoda, and M\u00a8uller 2001] R\u00a8atsch, G.; Onoda, T.; and M\u00a8uller, K.-R. 2001. Soft margins for adaboost. Machine learning 42(3):287\u2013320."}, {"ref": "[Ricci and Aha 1997] Ricci, F., and Aha, D. W. 1997. Extending local learners with error-correcting output codes. Naval Center for Applied Research in Arti\ufb01cial Intelligence, Washington, DC."}, {"ref": "[Robert and Escou\ufb01er 1976] Robert, P., and Escou\ufb01er, Y. 1976. A unifying tool for linear multivariate statistical methods: the rv-coef\ufb01cient. Journal of the Royal Statistical Society: Series C (Applied Statistics) 25(3):257\u2013265."}, {"ref": "[Sz\u00b4ekely et al. 2007] Sz\u00b4ekely, G. J.; Rizzo, M. L.; Bakirov, N. K.; et al. 2007. Measuring and testing dependence by correlationofdistances. The annals of statistics 35(6):2769\u2013 2794."}, {"ref": "[Sz\u00b4ekely, Rizzo, and others 2009] Sz\u00b4ekely, G. J.; Rizzo, M. L.; et al. 2009. Brownian distance covariance. The annals of applied statistics 3(4):1236\u20131265."}, {"ref": "[Tang and Liu 2009] Tang, L., and Liu, H. 2009. Relational learning via latent social dimensions. In Proceedings of the 15th international conference on Knowledge discovery and data mining, 817\u2013826. ACM."}, {"ref": "[Theocharidis et al. 2009] Theocharidis,A.;VanDongen,S.; Enright, A.; and Freeman, T. 2009. Network visualization and analysis of gene expression data using biolayout express3d. Nature protocols 4:1535\u20131550."}, {"ref": "[Wang, Cui, and Zhu 2016a] Wang, D.; Cui, P.; and Zhu, W. 2016a. Structural deep network embedding. In Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining, 1225\u20131234. ACM."}, {"ref": "[Wang, Cui, and Zhu 2016b] Wang, D.; Cui, P.; and Zhu, W. 2016b. Structural deep network embedding. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, 1225\u20131234. ACM."}, {"ref": "[Zhou 2012] Zhou, Z.-H. 2012. Ensemble methods: foundations and algorithms. Chapman and Hall/CRC."}]}, {"author": ["Tiago A. Schieber", "Laura Carpi", "Albert D\u00edaz-Guilera", "Panos M. Pardalos", "Cristina Masoller", "Mart\u00edn G. Ravetti"], "title": "Quantification of network structural dissimilarities", "journal": "Nature communications", "year": 2017, "DOI": "DOI: 10.1038/ncomms13928", "month": 0, "citations(google scholar)": 72, "abstract": "Identifying and quantifying dissimilarities among graphs is a fundamental and challenging problem of practical importance in many \ufb01elds of science. Current methods of network comparison are limited to extract only partial information or are computationally very demanding. Here we propose an ef\ufb01cient and precise measure for network comparison, which is based on quantifying differences among distance probability distributions extracted from the networks. Extensive experiments on synthetic and real-world networks show that this measure returns non-zero values only when the graphs are non-isomorphic. Most importantly, the measure proposed here can identify and quantify structural topological differences that have a practical impact on the information \ufb02ow through the network, such as the presence or absence of critical links that connect or disconnect connected components.", "keywords": [""], "reference_count": 56, "ccfClass": "", "important": true, "references": [{"ref": "1. Kelmans, A. K. Comparison of graphs by their number of spanning trees. Discrete Math. 16, 241\u2013261 (1976)."}, {"ref": "2. Garey, M. R. & Johnson, D. S. Computers and Intractability: a Guide to the Theory of NP-Completeness (W. H. Freeman & Co., 1979)."}, {"ref": "3. IEEE, T., Pattern Anal Bunke, H. & Shearer, K. A graph distance metric based on the maximal common subgraph. Pattern. Recogn. Lett. 19, 255\u2013259 (1998)."}, {"ref": "4. Fernandez, M. L. & Valiente, G. A graph distance metric combining maximum common subgraph and minimum common supergraph. Pattern. Recogn. Lett. 22, 753\u2013758 (2001)."}, {"ref": "5. Luo, B. & Hancock, E. R. Structural graph matching using the EM algorithm and singular value decomposition. IEEE T. Pattern. Anal. 23, 1120\u20131136 (2001)."}, {"ref": "6. Raymond, J. W., Gardiner, E. J. & Willett, P. Heuristics for similarity searching of chemical graphs using a maximum common edge subgraph algorithm. J. Chem. Inf. Comp. Sci. 42, 305\u2013316 (2002)."}, {"ref": "7. Conte, D. et al. Thirty years of graph matching in pattern recognition. Int. J. Pattern Recogn. 18, 265\u2013298 (2004)."}, {"ref": "8. Dehmer, M. et al. A similarity measure for graphs with low computational complexity. Appl. Math. Comput. 182, 447\u2013459 (2006)."}, {"ref": "9. Przulj, N. Biological network comparison using graphlet degree distribution. Bioinformatics 23, E177\u2013E182 (2007)."}, {"ref": "10. Zager, L. A. & Verghese, G. C. Graph similarity scoring and matching. Appl. Math. Lett. 21, 86\u201394 (2008)."}, {"ref": "11. Gao, X., Xiao, B., Tao, D. & Li, X. A survey of graph edit distance. Pattern Anal. Appl. 13, 113\u2013129 (2010)."}, {"ref": "12. Soundarajan, S., Eliassi-Rad, T. & Gallagher, B. in Proceedings of the 2014 SIAM International Conference on Data Mining, 1037\u20131045 (2014)."}, {"ref": "13. Fischer, A. et al. Approximation of graph edit distance based on Hausdorff matching. Pattern Recogn. 48, 331\u2013343 (2015)."}, {"ref": "14. Aliakbary, S. et al. Distance metric learning for complex networks: towards size-independent comparison of network structures. Chaos 25, 023111 (2015)."}, {"ref": "15. Bougleux, S. et al. A quadratic assignment formulation of the graph edit distance. Preprint at https://arxiv.org/abs/1512.07494v1 (2015)."}, {"ref": "16. Babai, L. Graph isomorphism in quasipolynomial time. Preprint at https://arxiv.org/abs/1512.03547v2 (2016)."}, {"ref": "17. Savage, N. Graph matching in theory and practice. Commun. ACM 59, 12\u201314 (2016)."}, {"ref": "18. Borgwardt, K. M. Graph Kernels. (PhD Thesis, Fakulta \u00a8t fu\u00a8r Mathematik, Informatik und Statistikder LudwigMaximiliansUniversitat, 2007)."}, {"ref": "19. Boccaletti, S. et al. Complex networks: structure and dynamics. Phys. Rep. 424, 175\u2013308 (2006)."}, {"ref": "20. Arenas, A. et al. Synchronization in complex networks. Phys. Rep. 469, 93\u2013153 (2008)."}, {"ref": "21. Barabasi, A. L., Gulbahce, N. & Loscalzo, J. Network medicine: a network-based approach to human disease. Nat. Rev. Genet. 12, 56\u201368 (2011)."}, {"ref": "22. Carpi, L. et al. Structural evolution of the Tropical Paci\ufb01c climate network. Eur. Phys. J. B 85, 1434\u20136028 (2012)."}, {"ref": "23. Schieber, T. A. & Ravetti, M. G. Simulating the dynamics of scale-free networks via optimization. PLoS ONE 8, e80783 (2013)."}, {"ref": "24. Taylor, D. et al. Topological data analysis of contagion maps for examining spreading processes on networks. Nat. Commun. 6, 7723 (2015)."}, {"ref": "25. Orsini, C. et al. Quantifying randomness in real networks. Nat. Commun. 6, 8627 (2015)."}, {"ref": "26. De Domenico, M., Nicosia, V., Arenas, A. & Latora, V. Structural reducibility of multilayer networks. Nat. Commun. 6, 6864 (2015)."}, {"ref": "27. Menche, J. et al. Uncovering disease-disease relationships through the incomplete interactome. Science 347, 1257601 (2015)."}, {"ref": "28. Schieber, T. A. et al. Information theory perspective on network robustness. Phys. Lett. A 380, 359\u2013364 (2016)."}, {"ref": "29. Verma, T., Russmann, F., Arau \u00b4jo, N. A. M., Nagler, J. & Herrmann, H. J. Emergence of coreperipheries in networks. Nat. Commun. 7, 10441 (2016)."}, {"ref": "30. \u00c7olak, S., Lima, A. & Gonza \u00b4lez, M. C. Understanding congested travel in urban areas. Nat. Commun. 7, 10793 (2016)."}, {"ref": "31. Calderone, A. et al. Comparing Alzheimers and Parkinsons diseases networks using graph communities structure. BMC Syst. Biol. 10, 1\u201310 (2016)."}, {"ref": "32. Morrow, J. K., Tian, L. & Zhang, S. Molecular Networks in Drug Discovery. Crit. Rev. Biomed. Eng. 38, 143\u2013156 (2010)."}, {"ref": "33. Costa, L. et al. Characterization of complex networks: a survey of measurements. Adv. Phys. 56, 167\u2013242 (2007)."}, {"ref": "34. Hamming, R. W. Binary codes capable of correcting deletions, insertions, and reversals. AT&T Tech. J. 10, 147\u2013160 (1950)."}, {"ref": "35. Sanfeliu, A. & Fu, K. S. A distance measure between attributed relational graphs for pattern recognition. IEEE T. Syst. Man Cyb. 13, 353\u2013363 (1983)."}, {"ref": "36. Lin, J. Divergence measures based on the Shannon entropy. IEEE T. Inform. Theory 37, 145\u2013151 (1991)."}, {"ref": "37. Erdo \u00a8s, P. & Re \u00b4nyi, A. On random graphs. Publ. Math. 6, 290\u2013297 (1959)."}, {"ref": "38. Watts, D. J. & Strogatz, S. H. Collective dynamics of small-world networks. Nature 393, 440\u2013442 (1998)."}, {"ref": "39. Bonacich, P. Power and centrality: a family of measures. Am. J. Sociol. 92, 1170\u20131182 (1987)."}, {"ref": "40. Dijkstra, E. W. A note on two problems in connexion with graphs. Numer. Math. 1, 269\u2013271 (1959)."}, {"ref": "41. Fredman, M. L. & Tarjan, R. E. Fibonacci Heaps and Their Uses in Improved Network Optimization Algorithms. J. ACM 34, 596\u2013615 (1987)."}, {"ref": "42. Albert, R. & Baraba \u00b4si, A. Statistical mechanics of complex networks. Rev. Mod. Phys. 74, 47\u201397 (2002)."}, {"ref": "43. Carpi, L. et al. Analyzing complex networks evolution through Information theory quanti\ufb01ers. Phys. Lett. A 375, 801\u2013804 (2011)."}, {"ref": "44. Newman, M. E. J. & Ziff, R. M. Ef\ufb01cient Monte Carlo algorithm and high-precision results for percolation. Phys. Rev. Lett. 85, 4101 (2000)."}, {"ref": "45. Radicchi, F. Predicting percolation thresholds in networks. Phys. Rev. E 91, 010801 (2015)."}, {"ref": "46. Molloy, M. & Reed, B. The size of the giant component of a random graph with a given degree sequence. Comb. Probab. Comput. 7, 295\u2013305 (1998)."}, {"ref": "47. Maslov, S. & Sneppen, K. Speci\ufb01city and stability in topology of protein networks. Science 296, 910\u2013913 (2002)."}, {"ref": "48. Jamakovic, A. et al. How small are building blocks of complex networks. Preprint at https://arxiv.org/abs/0908.1143v2 (2015)."}, {"ref": "49. Kunegis, J. KONECT\u2014The Koblenz Network Collection. In Proc. Int. Conf. on World Wide Web Companion, 1343\u20131350 (2013)."}, {"ref": "50. Newman, M. E. J. & Park, J. Why social networks are different from other types of networks. Phys. Rev. E 68, 036122 (2003)."}, {"ref": "51. Subelj, L. & Bajec, M. Robust network community detection using balanced propagation. Eur. Phys. J. B 81, 353\u2013362 (2011)."}, {"ref": "52. Watts., D. J. Small Worlds: The Dynamics of Networks between Order and Randomness (Princeton Univ. Press, 2003)."}, {"ref": "53. Luque, B., Lacasa, L., Ballesteros, F. & Luque, J. Horizontal visibility graphs: exact results for random time series. Phys. Rev. E 80, 046103 (2009)."}, {"ref": "54. Gonc \u00b8alves, B. A., Carpi, L., Rosso, O. A. & Ravetti, M. G. Time series characterization via horizontal visibility graph and information theory. Phys. A 464, 93\u2013102 (2016)."}, {"ref": "55. Begleiter, H. EEG Database Data Set https://archive.ics.uci.edu/ml/datasets/ EEG+Database (1995)."}, {"ref": "56. Joudaki, A., Salehi, N., Jalili, M. & Knyazeva, M. G. EEG-based functional brain networks: does the network size matter? PLoS ONE 7, e35673 (2012)."}]}, {"author": ["Matthias Fey", "Jan Eric Lenssen"], "title": "Fast graph representation learning with PyTorch Geometric", "journal": "International Conference on Learning Representations", "year": 2019, "DOI": "", "month": 0, "citations(google scholar)": 53, "abstract": "We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.", "keywords": [""], "reference_count": 49, "ccfClass": "", "important": true, "references": [{"ref": "Automatic differentiation in PyTorch"}, {"ref": "Dynamic Graph CNN for Learning on Point Clouds"}, {"ref": "Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs"}, {"ref": "Graph Attention Networks"}, {"ref": "Hierarchical Graph Representation Learning with Differentiable Pooling"}, {"ref": "Inductive Representation Learning on Large Graphs"}, {"ref": "Neural Message Passing for Quantum Chemistry"}, {"ref": "PointCNN: Convolution On X-Transformed Points"}, {"ref": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space"}, {"ref": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank"}, {"ref": "Representation Learning on Graphs with Jumping Knowledge Networks"}, {"ref": "Semi-Supervised Classification with Graph Convolutional Networks"}, {"ref": "A GPU Algorithm for Greedy Graph Matching"}, {"ref": "A simple yet effective baseline for non-attribute graph classification"}, {"ref": "Adversarially Regularized Graph Autoencoder for Graph Embedding"}, {"ref": "An End-to-End Deep Learning Architecture for Graph Classification"}, {"ref": "Attention-based Graph Neural Network for Semi-supervised Learning"}, {"ref": "Benchmark data sets for graph kernels"}, {"ref": "Collective Classification in Network Data"}, {"ref": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering"}, {"ref": "Deep Gaussian Embedding of Attributed Graphs: Unsupervised Inductive Learning via Ranking"}, {"ref": "Deep Graph Infomax"}, {"ref": "Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs"}, {"ref": "Edge Weight Prediction in Weighted Signed Networks"}, {"ref": "Gated Graph Sequence Neural Networks"}, {"ref": "GDELT: Global data on events, location, and tone"}, {"ref": "Generating 3D faces using Convolutional Mesh Autoencoders"}, {"ref": "Geometric Deep Learning: Going beyond Euclidean data"}, {"ref": "Graph Neural Networks with convolutional ARMA filters"}, {"ref": "How Powerful are Graph Neural Networks?"}, {"ref": "Just Jump: Dynamic Neighborhood Aggregation in Graph Neural Networks"}, {"ref": "Machine learning of molecular electronic properties in chemical compound space"}, {"ref": "Modeling Relational Data with Graph Convolutional Networks"}, {"ref": "Order Matters: Sequence to sequence for sets"}, {"ref": "PCPNET: Learning Local Shape Properties from Raw Point Clouds"}, {"ref": "Pitfalls of Graph Neural Network Evaluation"}, {"ref": "Quantum chemistry structures and properties of 134 kilo molecules"}, {"ref": "Recurrent Event Network for Reasoning over Temporal Knowledge Graphs"}, {"ref": "Relational inductive biases, deep learning, and graph networks"}, {"ref": "ShapeNet: An Information-Rich 3D Model Repository"}, {"ref": "Signed Graph Convolutional Networks"}, {"ref": "Simplifying Graph Convolutional Networks"}, {"ref": "SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels"}, {"ref": "Towards Sparse Hierarchical Graph Classifiers"}, {"ref": "Variational Graph Auto-Encoders"}, {"ref": "Weighted Graph Cuts without Eigenvectors A Multilevel Approach"}, {"ref": "Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks"}]}, {"author": ["Daixin Wang", "Peng Cui", "Wenwu Zhu"], "title": "Structural Deep Network Embedding", "journal": "Conference on Knowledge Discovery and Data Mining", "year": 2016, "DOI": "10.1145/2939672.2939753", "month": 0, "citations(google scholar)": 733, "abstract": "Network embedding is an important method to learn low-dimensional representations of vertexes in networks, aiming to capture and preserve the network structure. Almost all the existing network embedding methods adopt shallow models. However, since the underlying network structure is complex, shallow models cannot capture the highly non-linear network structure, resulting in sub-optimal network representations. Therefore, how to find a method that is able to effectively capture the highly non-linear network structure and preserve the global and local structure is an open yet important problem. To solve this problem, in this paper we propose a Structural Deep Network Embedding method, namely SDNE. More specifically, we first propose a semi-supervised deep model, which has multiple layers of non-linear functions, thereby being able to capture the highly non-linear network structure. Then we propose to exploit the first-order and second-order proximity jointly to preserve the network structure. The second-order proximity is used by the unsupervised component to capture the global network structure. While the first-order proximity is used as the supervised information in the supervised component to preserve the local network structure. By jointly optimizing them in the semi-supervised deep model, our method can preserve both the local and global network structure and is robust to sparse networks. Empirically, we conduct the experiments on five real-world networks, including a language network, a citation network and three social networks. The results show that compared to the baselines, our method can reconstruct the original network significantly better and achieves substantial gains in three applications, i.e. multi-label classification, link prediction and visualization.", "keywords": ["Network Embedding", "Deep Learning", "Network Analysis"], "reference_count": 36, "ccfClass": "A", "important": true, "references": [{"ref": "[1] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation, 15(6):1373\u20131396, 2003."}, {"ref": "[2] Y. Bengio. Learning deep architectures for ai. Foundations and trends in Machine Learning, 2(1):1-127, 2009."}, {"ref": "[3] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798\u20131828, 2013."}, {"ref": "[4] S. Cao, W. Lu, and Q. Xu. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 891\u2013900. ACM, 2015."}, {"ref": "[5] S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang. Heterogeneous network embedding via deep architectures. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 119\u2013128. ACM, 2015."}, {"ref": "[6] N. S. Dash. Context and contextual word meaning. SKASE Journal of Theoretical Linguistics, 5(2):21\u201331, 2008."}, {"ref": "[7] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio. Why does unsupervised pre-training help deep learning? The Journal of Machine Learning Research, 11:625\u2013660, 2010."}, {"ref": "[8] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large linear classi\ufb01cation. The Journal of Machine Learning Research, 9:1871\u20131874, 2008."}, {"ref": "[9] K. Georgiev and P. Nakov. A non-iid framework for collaborative \ufb01ltering with restricted boltzmann machines. In ICML-13, pages 1148\u20131156, 2013."}, {"ref": "[10] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6):82\u201397, 2012."}, {"ref": "[11] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527\u20131554, 2006."}, {"ref": "[12] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 2006."}, {"ref": "[13] M. Jamali and M. Ester. A matrix factorization technique with trust propagation for recommendation in social networks. In Proceedings of the fourth ACM conference on Recommender systems, pages 135\u2013142. ACM, 2010."}, {"ref": "[14] E. M. Jin, M. Girvan, and M. E. Newman. Structure of growing social networks. Physical review E, 64(4):046132, 2001."}, {"ref": "[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classi\ufb01cation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012."}, {"ref": "[16] J. Leskovec, J. Kleinberg, and C. Faloutsos. Graph evolution: Densi\ufb01cation and shrinking diameters. ACM Transactions on Knowledge Discovery from Data (TKDD), 1(1):2, 2007."}, {"ref": "[17] D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019\u20131031, 2007."}, {"ref": "[18] T. Liu and D. Tao. Classi\ufb01cation with noisy labels by importance reweighting. TPAMI, (1):1\u20131."}, {"ref": "[19] D. Luo, F. Nie, H. Huang, and C. H. Ding. Cauchy graph embedding. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 553\u2013560, 2011."}, {"ref": "[20] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 2:849\u2013856, 2002."}, {"ref": "[21] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701\u2013710. ACM, 2014."}, {"ref": "[22] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323\u20132326, 2000."}, {"ref": "[23] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate Reasoning, 50(7):969\u2013978, 2009."}, {"ref": "[24] B. Shaw and T. Jebara. Structure preserving embedding. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 937\u2013944. ACM, 2009."}, {"ref": "[25] R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP), volume 1631, page 1642. Citeseer, 2013."}, {"ref": "[26] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067\u20131077. International World Wide Web Conferences Steering Committee, 2015."}, {"ref": "[27] L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2009."}, {"ref": "[28] L. Tang and H. Liu. Scalable learning of collective behavior based on sparse social dimensions. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 1107\u20131116. ACM, 2009."}, {"ref": "[29] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319\u20132323, 2000."}, {"ref": "[30] F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu. Learning deep representations for graph clustering. In Proceedings of the Twenty-Eighth AAAI Conference on Arti\ufb01cial Intelligence, pages 1293\u20131299, 2014."}, {"ref": "[31] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(2579-2605):85, 2008."}, {"ref": "[32] S. V. N. Vishwanathan, N. N. Schraudolph, R. Kondor, and K. M. Borgwardt. Graph kernels. The Journal of Machine Learning Research, 11:1201\u20131242, 2010."}, {"ref": "[33] D. Wang, P. Cui, M. Ou, and W. Zhu. Deep multimodal hashing with orthogonal regularization. In Proceedings of the 24th International Conference on Arti\ufb01cial Intelligence, pages 2291\u20132297. AAAI Press, 2015."}, {"ref": "[34] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In Advances in neural information processing systems, pages 1753\u20131760, 2009."}, {"ref": "[35] C. Xu, D. Tao, and C. Xu. Multi-view intact space learning. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 37(12):2531\u20132544, 2015."}, {"ref": "[36] J. Zhuang, I. W. Tsang, and S. Hoi. Two-layer multiple kernel learning. In International conference on arti\ufb01cial intelligence and statistics, pages 909\u2013917, 2011."}]}, {"author": ["Shaosheng Cao", "Wei Lu", "Qiongkai Xu"], "title": "GraRep: Learning Graph Representations with Global Structural Information", "journal": "Conference on Information and Knowledge Management", "year": 2015, "DOI": "10.1145/2806416.2806512", "month": 0, "citations(google scholar)": 507, "abstract": "In this paper, we present GraRep, a novel model for learning vertex representations of weighted graphs. This model learns low dimensional vectors to represent vertices appearing in a graph and, unlike existing work, integrates global structural information of the graph into the learning process. We also formally analyze the connections between our work and several previous research efforts, including the DeepWalk model of Perozzi et al. as well as the skip-gram model with negative sampling of Mikolov et al. We conduct experiments on a language network, a social network as well as a citation network and show that our learned global representations can be effectively used as features in tasks such as clustering, classification and visualization. Empirical results demonstrate that our representation significantly outperforms other state-of-the-art methods in such tasks.", "keywords": ["Graph Representation", "Matrix Factorization", "Feature Learning", "Dimension Reduction"], "reference_count": 31, "ccfClass": "B", "important": true, "references": [{"ref": "[1] A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. In WWW, pages 37\u201348. International World Wide Web Conferences Steering Committee, 2013."}, {"ref": "[2] D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In SODA, pages 1027\u20131035. Society for Industrial and Applied Mathematics, 2007."}, {"ref": "[3] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585\u2013591, 2001."}, {"ref": "[4] J. A. Bullinaria and J. P. Levy. Extracting semantic representations from word co-occurrence statistics: A computational study. BRM, 39(3):510\u2013526, 2007."}, {"ref": "[5] J. A. Bullinaria and J. P. Levy. Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd. BRM, 44(3):890\u2013907, 2012."}, {"ref": "[6] J. Caron. Experiments with lsa scoring: Optimal rank and basis. In CIR, pages 157\u2013169, 2001."}, {"ref": "[7] P. Comon. Independent component analysis, a new concept? Signal processing, 36(3):287\u2013314, 1994."}, {"ref": "[8] T. F. Cox and M. A. Cox. Multidimensional scaling. CRC Press, 2000."}, {"ref": "[9] C. Eckart and G. Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211\u2013218, 1936."}, {"ref": "[10] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large linear classi\ufb01cation. JMLR, 9:1871\u20131874, 2008."}, {"ref": "[11] M. U. Gutmann and A. Hyv\u00a8arinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. JMLR, 13(1):307\u2013361, 2012."}, {"ref": "[12] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504\u2013507, 2006."}, {"ref": "[13] C. Jutten and J. Herault. Blind separation of sources, part i: An adaptive algorithm based on neuromimetic architecture. Signal processing, 24(1):1\u201310, 1991."}, {"ref": "[14] V. Klema and A. J. Laub. The singular value decomposition: Its computation and some applications. Automatic Control, 25(2):164\u2013176, 1980."}, {"ref": "[15] T. K. Landauer, P. W. Foltz, and D. Laham. An introduction to latent semantic analysis. Discourse processes, 25(2-3):259\u2013284, 1998."}, {"ref": "[16] O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In NIPS, pages 2177\u20132185, 2014."}, {"ref": "[17] K. Lund and C. Burgess. Producing high-dimensional semantic spaces from lexical co-occurrence. BRMIC, 28(2):203\u2013208, 1996."}, {"ref": "[18] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111\u20133119, 2013."}, {"ref": "[19] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. EMNLP, 12, 2014."}, {"ref": "[20] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701\u2013710. ACM, 2014."}, {"ref": "[21] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323\u20132326, 2000."}, {"ref": "[22] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Incremental singular value decomposition algorithms for highly scalable recommender systems. In ICIS, pages 27\u201328. Citeseer, 2002."}, {"ref": "[23] J. Shi and J. Malik. Normalized cuts and image segmentation. PAMI, 22(8):888\u2013905, 2000."}, {"ref": "[24] A. Strehl, J. Ghosh, and R. Mooney. Impact of similarity measures on web-page clustering. In Workshop on Arti\ufb01cial Intelligence for Web Search (AAAI 2000), pages 58\u201364, 2000."}, {"ref": "[25] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW. ACM, 2015."}, {"ref": "[26] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su. Arnetminer: extraction and mining of academic social networks. In SIGKDD, pages 990\u2013998. ACM, 2008."}, {"ref": "[27] L. Tang and H. Liu. Relational learning via latent social dimensions. In SIGKDD, pages 817\u2013826. ACM, 2009."}, {"ref": "[28] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319\u20132323, 2000."}, {"ref": "[29] F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu. Learning deep representations for graph clustering. In AAAI, 2014."}, {"ref": "[30] P. D. Turney. Domain and function: A dual-space model of semantic relations and compositions. JAIR, pages 533\u2013585, 2012."}, {"ref": "[31] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. JMLR, 9(2579-2605):85, 2008."}]}, {"author": ["Shuicheng Yan", "Dong Xu", "Benyu Zhang", "Hong-Jiang Zhang", "Qiang Yang", "Stephen Lin"], "title": "Graph Embedding and Extensions: A general framework for dimensionality reduction", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2007, "DOI": "10.1109/CVPR.2005.170", "month": 0, "citations(google scholar)": 1394, "abstract": "Over the past few decades, a large family of algorithms - supervised or unsupervised; stemming from statistics or geometry theory - has been designed to provide different solutions to the problem of dimensionality reduction. Despite the different motivations of these algorithms, we present in this paper a general formulation known as graph embedding to unify them within a common framework. In graph embedding, each algorithm can be considered as the direct graph embedding or its linear/kernel/tensor extension of a specific intrinsic graph that describes certain desired statistical or geometric properties of a data set, with constraints from scale normalization or a penalty graph that characterizes a statistical or geometric property that should be avoided. Furthermore, the graph embedding framework can be used as a general platform for developing new dimensionality reduction algorithms. By utilizing this framework as a tool, we propose a new supervised dimensionality reduction algorithm called Marginal Fisher Analysis in which the intrinsic graph characterizes the intraclass compactness and connects each data point with its neighboring points of the same class, while the penalty graph connects the marginal points and characterizes the interclass separability. We show that MFA effectively overcomes the limitations of the traditional Linear Discriminant Analysis algorithm due to data distribution assumptions and available projection directions. Real face recognition experiments show the superiority of our proposed MFA in comparison to LDA, also for corresponding kernel and tensor extensions.", "keywords": ["Dimensionality reduction", "manifold learning", "subspace learning", "graph embedding framework"], "reference_count": 33, "ccfClass": "A", "important": true, "references": [{"ref": "[1] A. Batur and M. Hayes, \"Linear Subspaces for Illumination Robust Face Recognition,\" Proc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recognition, vol. 2, pp. 296-301, Dec. 2001."}, {"ref": "[2] P. Belhumeur, J. Hespanha, and D. Kriegman, \"Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection,\" IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 19, no. 7, pp. 711-720, July 1997."}, {"ref": "[3] M. Belkin and P. Niyogi, \"Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering,\" Advances in Neural Information Processing System, vol. 14, pp. 585-591, 2001."}, {"ref": "[4] Y. Bengio, J. Paiement, P. Vincent, O. Delalleau, N. Roux, and M. Ouimet, \"Out-of-Sample Extensions for LLE, ISPMAP, MDS, Eigenmaps, and Spectral Clustering,\" Advances in Neural Information Processing Systems, vol. 16, 2004."}, {"ref": "[5] M. Brand, \"Continuous Nonlinear Dimensionality Reduction by Kernel Eigenmaps,\" Technical Report 2003-21, Mitsubishi Electric Research Labs, Apr. 2003."}, {"ref": "[6] F. Chung, \"Spectral Graph Theory,\" Regional Conf. Series in Math., no. 92, 1997."}, {"ref": "[7] Y. Fu and T. Huang, \"Locally Linear Embedded Eigenspace Analysis,\" IFP-TR, Univ. of Illinois at Urbana-Champaign, Jan. 2005."}, {"ref": "[8] K. Fukunnaga, Introduction to Statistical Pattern Recognition, second ed. Academic Press, 1991."}, {"ref": "[9] D. Hand, Kernel Discriminant Analysis. Research Studies Press, 1982."}, {"ref": "[10] X. He, S. Yan, Y. Hu, P. Niyogi, and H. Zhang, \"Face Recognition Using Laplacianfaces,\" IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 27, no. 3, pp. 328-340, Mar. 2005."}, {"ref": "[11] I. Joliffe, Principal Component Analysis. Springer-Verlag, 1986."}, {"ref": "[12] J. Luettin and G. Maitre, \"Evaluation Protocol for the Extended M2VTS Database (XM2VTS),\" DMI for Perceptual Artificial Intelligence, 1998."}, {"ref": "[13] J. Ham, D. Lee, S. Mika, and B. Scho \u00a8lkopf, \"A Kernel View of the Dimensionality Reduction of Manifolds,\" Proc. Int\u2019l Conf. Machine Learning, pp. 47-54, 2004."}, {"ref": "[14] A.M. Martinez and A.C. Kak, \"PCA versus LDA,\" IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 23, no. 2, pp. 228-233, Feb. 2001."}, {"ref": "[15] B. Moghaddam, T. Jebara, and A. Pentland, \"Bayesian Face Recognition,\" Pattern Recognition, vol. 33, pp. 1771-1782, 2000."}, {"ref": "[16] K. Muller, S. Mika, G. Riitsch, K. Tsuda, and B. Scho \u00a8lkopf, \"An Introduction to Kernel-Based Learning Algorithms,\" IEEE Trans. Neural Networks, vol. 12, pp. 181-201, 2001."}, {"ref": "[17] Olivetti & Oracle Research Laboratory, The Olivetti & Oracle Research Laboratory Face Database of Faces, http:// www.cam-orl.co.uk/facedatabase.html, 1994."}, {"ref": "[18] S. Roweis and L. Saul, \"Nonlinear Dimensionality Reduction by Locally Linear Embedding,\" Science, vol. 290, no. 22, pp. 23232326, Dec. 2000."}, {"ref": "[19] T. Sim, S. Baker, and M. Bsat, \"The CMU Pose, Illumination, and Expression Database,\" IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 25, no. 12, pp. 1615-1618, Dec. 2003."}, {"ref": "[20] J. Tenenbaum, V. Silva, and J. Langford, \"A Global Geometric Framework for Nonlinear Dimensionality Reduction,\" Science, vol. 290, no. 22, pp. 2319-2323, Dec. 2000."}, {"ref": "[21] Y. Teh and S. Roweis, \"Automatic Alignment of Hidden Representations,\" Advances in Neural Information Processing System, vol. 15, pp. 841-848, 2002."}, {"ref": "[22] M. Turk and A. Pentland, \"Face Recognition Using Eigenfaces,\" Proc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 586591, 1991."}, {"ref": "[23] M. Vasilescu and D. Terzopoulos, \"TensorTextures: Multilinear Image-Based Rendering,\" ACM Trans. Graphics, vol. 23, no. 3, pp. 336-342, 2004."}, {"ref": "[24] X. Wang and X. Tang, \"Dual-Space Linear Discriminant Analysis for Face Recognition,\" Proc. Computer Vision and Pattern Recognition, vol. 2, pp. 564-569, 2004."}, {"ref": "[25] D. Xu, S. Yan, L. Zhang, Z. Liu, and H. Zhang, \"Coupled Subspaces Analysis,\" Microsoft Research Technical Report, MSRTR-2004-106, Oct. 2004."}, {"ref": "[26] S. Yan, D. Xu, Q. Yang, L. Zhang, X. Tang, and H. Zhang, \"Discriminant Analysis with Tensor Representation,\" Proc. Int\u2019l Conf. Computer Vision and Pattern Recognition, vol. 1, pp. 526-532, 2005."}, {"ref": "[27] S. Yan, D. Xu, L. Zhang, B. Zhang, and H. Zhang, \"Coupled Kernel-Based Subspace Learning,\" Proc. Int\u2019l Conf. Computer Vision and Pattern Recognition, vol. 1, pp. 645-650, 2005."}, {"ref": "[28] S. Yan, H. Zhang, Y. Hu, B. Zhang, and Q. Cheng, \"Discriminant Analysis on Embedded Manifold,\" Proc. Eighth European Conf. Computer Vision, vol. 1, pp. 121-132, May 2004."}, {"ref": "[29] J. Yang, D. Zhang, A. Frangi, and J. Yang, \"Two-Dimensional PCA: A New Approach to Appearance-Based Face Representation and Recognition,\" IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 26, no. 1, pp. 131-137, Jan. 2004."}, {"ref": "[30] J. Ye, \"Generalized Low Rank Approximations of Matrices,\" Proc. Int\u2019l Conf. Machine Learning, pp. 895-902, 2004."}, {"ref": "[31] J. Ye, R. Janardan, and Q. Li, \"Two-Dimensional Linear Discriminant Analysis,\" Advances in Neural Information Processing Systems, vol. 17, pp. 1569-1576, 2005."}, {"ref": "[32] J. Ye, R. Janardan, C. Park, and H. Park, \"An Optimization Criterion for Generalized Discriminant Analysis on Undersampled Problems,\" IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 26, no. 8, pp. 982-994, Aug. 2004."}, {"ref": "[33] H. Yu and J. Yang, \"A Direct LDA Algorithm for High Dimensional Data-with Application to Face Recognition,\" Pattern Recognition, vol. 34, pp. 2067-2070, 2001."}]}, {"author": ["Yann Jacob", "Ludovic Denoyer", "Patrick Gallinari"], "title": "Learning latent representations of nodes for classifying in heterogeneous social networks", "journal": "International conference on Web search and data mining", "year": 2014, "DOI": "10.1145/2556195.2556225", "month": 0, "citations(google scholar)": 43, "abstract": "Social networks are heterogeneous systems composed of different types of nodes (e.g. users, content, groups, etc.) and relations (e.g. social or similarity relations). While learning and performing inference on homogeneous networks have motivated a large amount of research, few work exists on heterogeneous networks and there are open and challenging issues for existing methods that were previously developed for homogeneous networks. We address here the specific problem of nodes classification and tagging in heterogeneous social networks, where different types of nodes are considered, each type with its own label or tag set. We propose a new method for learning node representations onto a latent space, common to all the different node types. Inference is then performed in this latent space. In this framework, two nodes connected in the network will tend to share similar representations regardless of their types. This allows bypassing limitations of the methods based on direct extensions of homogenous frameworks and exploiting the dependencies and correlations between the different node types. The proposed method is tested on two representative datasets and compared to state-of-the-art methods and to baselines.", "keywords": ["Machine learning", "Classification", "Social networks"], "reference_count": 28, "ccfClass": "B", "important": true, "references": [{"ref": "[1] Jacob Abernethy, Olivier Chapelle, and Carlos Castillo. Witch: A new approach to web spam detection. In In Proceedings of the 4th International Workshop on Adversarial Information Retrieval on the Web (AIRWeb), 2008."}, {"ref": "[2] Ralitsa Angelova, Gjergji Kasneci, and Gerhard Weikum. Gra\ufb03ti: graph-based classi\ufb01cation in heterogeneous networks. World Wide Web, 15(2):139\u2013170, 2012."}, {"ref": "[3] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. J. Mach. Learn. Res., 7:2399\u20132434, December 2006."}, {"ref": "[4] A. Bordes, J. Weston, R. Collobert, and Y. Bengio. Learning structured embeddings of knowledge bases. In AAAI, 2011."}, {"ref": "[5] Darcy Davis, Ryan Lichtenwalter, and N.V. Chawla. Multi-Relational Link Prediction in Heterogeneous Information Networks. In ASONAM, 2011."}, {"ref": "[6] Ludovic Denoyer and Patrick Gallinari. A ranking based model for automatic image annotation in a social network. In ICWSM, 2010."}, {"ref": "[7] Sheng Gao, Ludovic Denoyer, and Patrick Gallinari. Link pattern prediction with tensor decomposition in multi-relational networks. In CIDM, 2011."}, {"ref": "[8] Taehyun Hwang and Rui Kuang. A heterogeneous label propagation algorithm for disease gene discovery. In SDM, page 12, 2010."}, {"ref": "[9] Yann Jacob, Ludovic Denoyer, and Patrick Gallinari. Classi\ufb01cation and annotation in social corpora using multiple relations. In CIKM, pages 1215\u20131220, 2011."}, {"ref": "[10] Ming Ji, Jiawei Han, and Marina Danilevsky. Ranking-based classi\ufb01cation of heterogeneous information networks. In KDD, pages 1298\u20131306. ACM, 2011."}, {"ref": "[11] Ming Ji, Yizhou Sun, Marina Danilevsky, Jiawei Han, and Jing Gao. Graph regularized transductive classi\ufb01cation on heterogeneous information networks. In ECML PKDD, volume 53, pages 570\u2013586, 2010."}, {"ref": "[12] T. Kato, H. Kashima, and M. Sugiyama. Integration of multiple networks for robust label propagation. In SIAM Conf. on Data Mining, pages 716\u2013726, 2008."}, {"ref": "[13] Xiangnan Kong, Bokai Cao, and Philip S. Yu. Multi-label classi\ufb01cation by mining label and instance correlations from heterogeneous information networks. KDD \u201913, pages 614\u2013622, 2013."}, {"ref": "[14] Xiangnan Kong, Philip S. Yu, Ying Ding, and David J. Wild. Meta path-based collective classi\ufb01cation in heterogeneous information networks. In CIKM, pages 1567\u20131571, 2012."}, {"ref": "[15] Zhenzhen Kou. Stacked graphical models for e\ufb03cient inference in markov random \ufb01elds. In In Proc. of the 2007 SIAM International Conf. on Data Mining, 2007."}, {"ref": "[16] Lu Liu, Jie Tang, Jiawei Han, and Meng Jiang. Mining topic-level in\ufb02uence in heterogeneous networks. In CIKM, 2010."}, {"ref": "[17] Sofus A. Macskassy and Foster Provost. A simple relational classi\ufb01er. In Proceedings of the Second Workshop on Multi-Relational Data Mining (MRDM-2003) at KDD-2003, pages 64\u201376, 2003."}, {"ref": "[18] Francis Maes, Stephane Peters, Ludovic Denoyer, and Patrick Gallinari. Simulated iterative classi\ufb01cation a new learning procedure for graph labeling. Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases Part II, II:47\u201362, 2009."}, {"ref": "[19] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad. Collective classi\ufb01cation in network data. AI Magazine, 29(3):93\u2013106, 2008."}, {"ref": "[20] Yizhou Sun, Charu C. Aggarwal, and Jiawei Han. Relation strength-aware clustering of heterogeneous information networks with incomplete attributes. PVLDB, 5(5):394\u2013405, 2012."}, {"ref": "[21] Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and Tianyi Wu. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. PVLDB, 4(11):992\u20131003, 2011."}, {"ref": "[22] Yizhou Sun, Yintao Yu, and Jiawei Han. Ranking-based clustering of heterogeneous information networks with star network schema. In KDD, pages 797\u2013806, 2009."}, {"ref": "[23] Lei Tang, Xufei Wang, and Huan Liu. Community detection via heterogeneous interaction analysis. Data Min. Knowl. Discov., 25(1):1\u201333, 2012."}, {"ref": "[24] Chi Wang, Rajat Raina, David Fong, Ding Zhou, Jiawei Han, and Greg J. Badros. Learning relevance from heterogeneous social network and its application in online targeting. In SIGIR, pages 655\u2013664, 2011."}, {"ref": "[25] Meng Wang, Xian-Sheng Hua, Richang Hong, Jinhui Tang, Guo-Jun Qi, and Yan Song. Uni\ufb01ed video annotation via multigraph learning. Circuits and Systems for Video Technology, IEEE Transactions on, 19(5):733 \u2013746, may 2009."}, {"ref": "[26] Xiao Yu, Yizhou Sun, Brandon Norick, Tiancheng Mao, and Jiawei Han. User guided entity similarity search using meta-path selection in heterogeneous information networks. CIKM \u201912, pages 2025\u20132029, New York, NY, USA, 2012. ACM."}, {"ref": "[27] Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Sch\u00a8olkopf. Learning with local and global consistency. In Sebastian Thrun, Lawrence Saul, and Bernhard Sch\u00a8olkopf, editors, Advances in Neural Inform. Process. Systems 16. 2004."}, {"ref": "[28] Dengyong Zhou, Jiayuan Huang, and Bernhard Sch\u00a8olkopf. Learning from labeled and unlabeled data on a directed graph. In Proc. of the 22nd intern. conf. on Mach. learn., ICML \u201905, pages 1036\u20131043, 2005."}]}, {"author": ["Linchuan Xu", "Xiaokai Wei", "Jiannong Cao", "Philip S. Yu"], "title": "Embedding of Embedding (EOE): Joint Embedding for Coupled Heterogeneous Networks", "journal": "International conference on Web search and data mining", "year": 2017, "DOI": "10.1145/3018661.3018723", "month": 0, "citations(google scholar)": 47, "abstract": "Network embedding is increasingly employed to assist network analysis as it is effective to learn latent features that encode linkage information. Various network embedding methods have been proposed, but they are only designed for a single network scenario. In the era of big data, different types of related information can be fused together to form a coupled heterogeneous network, which consists of two different but related sub-networks connected by inter-network edges. In this scenario, the inter-network edges can act as comple- mentary information in the presence of intra-network ones. This complementary information is important because it can make latent features more comprehensive and accurate. And it is more important when the intra-network edges are ab- sent, which can be referred to as the cold-start problem. In this paper, we thus propose a method named embedding of embedding (EOE) for coupled heterogeneous networks. In the EOE, latent features encode not only intra-network edges, but also inter-network ones. To tackle the challenge of heterogeneities of two networks, the EOE incorporates a harmonious embedding matrix to further embed the em- beddings that only encode intra-network edges. Empirical experiments on a variety of real-world datasets demonstrate the EOE outperforms consistently single network embedding methods in applications including visualization, link prediction multi-class classification, and multi-label classification.", "keywords": ["Network Embedding", "Coupled Heterogeneous Networks", "Data Mining"], "reference_count": 26, "ccfClass": "B", "important": true, "references": [{"ref": "[1] A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. In Proceedings of the 22nd international conference on World Wide Web, pages 37\u201348. International World Wide Web Conferences Steering Committee, 2013."}, {"ref": "[2] L. Armijo. Minimization of functions having lipschitz continuous \ufb01rst partial derivatives. Paci\ufb01c Journal of mathematics, 16(1):1\u20133, 1966."}, {"ref": "[3] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585\u2013591, 2001."}, {"ref": "[4] J. C. Bezdek and R. J. Hathaway. Some notes on alternating optimization. In AFSS International Conference on Fuzzy Systems, pages 288\u2013300. Springer, 2002."}, {"ref": "[5] B. Cao, X. Kong, and S. Y. Philip. Collective prediction of multiple types of links in heterogeneous information networks. In 2014 IEEE International Conference on Data Mining, pages 50\u201359. IEEE, 2014."}, {"ref": "[6] B. Carpenter. Lazy sparse stochastic gradient descent for regularized multinomial logistic regression. Alias-i, Inc., Tech. Rep, pages 1\u201320, 2008."}, {"ref": "[7] B. Chen, Y. Ding, and D. J. Wild. Assessing drug target association using semantic linked data. PLoS Comput Biol, 8(7):e1002574, 2012."}, {"ref": "[8] T. F. Cox and M. A. Cox. Multidimensional scaling. CRC press, 2000."}, {"ref": "[9] S. Fortunato. Community detection in graphs. Physics reports, 486(3):75\u2013174, 2010."}, {"ref": "[10] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. The weka data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10\u201318, 2009."}, {"ref": "[11] J. B. Kruskal. Multidimensional scaling by optimizing goodness of \ufb01t to a nonmetric hypothesis. Psychometrika, 29(1):1\u201327, 1964."}, {"ref": "[12] D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019\u20131031, 2007."}, {"ref": "[13] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701\u2013710. ACM, 2014."}, {"ref": "[14] J. Read, P. Reutemann, B. Pfahringer, and G. Holmes. Meka: a multi-label/multi-target extension to weka. Journal of Machine Learning Research, 17(21):1\u20135, 2016."}, {"ref": "[15] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323\u20132326, 2000."}, {"ref": "[16] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classi\ufb01cation in network data. AI magazine, 29(3):93, 2008."}, {"ref": "[17] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067\u20131077. International World Wide Web Conferences Steering Committee, 2015."}, {"ref": "[18] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su. Arnetminer: extraction and mining of academic social networks. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 990\u2013998. ACM, 2008."}, {"ref": "[19] L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 817\u2013826. ACM, 2009."}, {"ref": "[20] L. Tang and H. Liu. Leveraging social media networks for classi\ufb01cation. Data Mining and Knowledge Discovery, 23(3):447\u2013478, 2011."}, {"ref": "[21] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319\u20132323, 2000."}, {"ref": "[22] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(2579-2605):85, 2008."}, {"ref": "[23] X. Wang, L. Tang, H. Gao, and H. Liu. Discovering overlapping groups in social media. In 2010 IEEE international conference on data mining, pages 569\u2013578. IEEE, 2010."}, {"ref": "[24] X. Wei, B. Cao, W. Shao, C.-T. Lu, and P. S. Yu. Community detection with partially observable links and node attributes. In IEEE International Conference on Big Data, 2016."}, {"ref": "[25] X. Wei, B. Cao, and P. S. Yu. Unsupervised feature selection on networks: A generative view. In AAAI, 2016."}, {"ref": "[26] X. Wei, S. Xie, and P. S. Yu. E\ufb03cient partial order preserving unsupervised feature selection on networks. In SDM, pages 82\u201390. SIAM, 2015."}]}, {"author": ["Jure Leskovec", "Rok Sosic"], "title": "SNAP: A General Purpose Network Analysis and Graph Mining Library", "journal": "Transactions on Intelligent Systems and Technology", "year": 2016, "DOI": "10.1145/2898361", "month": 0, "citations(google scholar)": 277, "abstract": "Large networks are becoming a widely used abstraction for studying complex systems in a broad set of disciplines, ranging from social-network analysis to molecular biology and neuroscience. Despite an increasing need to analyze and manipulate large networks, only a limited number of tools are available for this task. Here, we describe the Stanford Network Analysis Platform (SNAP), a general-purpose, high-performance system that provides easy-to-use, high-level operations for analysis and manipulation of large networks. We present SNAP functionality, describe its implementational details, and give performance benchmarks. SNAP has been developed for single big-memory machines, and it balances the trade-off between maximum performance, compact in-memory graph representation, and the ability to handle dynamic graphs in which nodes and edges are being added or removed over time. SNAP can process massive networks with hundreds of millions of nodes and billions of edges. SNAP offers over 140 different graph algorithms that can efficiently manipulate large graphs, calculate structural properties, generate regular and random graphs, and handle attributes and metadata on nodes and edges. Besides being able to handle large graphs, an additional strength of SNAP is that networks and their attributes are fully dynamic; they can be modified during the computation at low cost. SNAP is provided as an open-source library in C++ as well as a module in Python. We also describe the Stanford Large Network Dataset, a set of social and information real-world networks and datasets, which we make publicly available. The collection is a complementary resource to our SNAP software and is widely used for development and benchmarking of graph analytics algorithms.", "keywords": ["Networks", "Graphs", "Graph Analytics", "Open-Source Software", "Data Mining"], "reference_count": 55, "ccfClass": "", "important": true, "references": [{"ref": "1. Arifuzzaman S, Khan M, Marathe M. PATRIC: A parallel algorithm for counting triangles in massive networks; ACM International Conference on Information and Knowledge Management (CIKM); 2013. pp. 529\u2013538."}, {"ref": "2. Barab\u00e1si A-L, Albert R. Emergence of scaling in random networks. Science. 1999;286(5439):509\u2013512. (1999)"}, {"ref": "3. Batagelj V, Mrvar A. Pajek-program for large network analysis. Connections. 1998;21(2):47\u201357. (1998)"}, {"ref": "4. Batagelj V, Zaver\u0161nik M. Generalized cores. ArXiv cs.DS/0202039. 2002 Feb; 2002."}, {"ref": "5. Benczur AA, Csalogany K, Sarlos T, Uher M. Spamrank\u2013fully automatic link spam detection. International Workshop on Adversarial Information Retrieval on the Web. 2005"}, {"ref": "6. Bollob\u00e1s B. A probabilistic proof of an asymptotic formula for the number of labelled regular graphs. European Journal of Combinatorics. 1980;1(4):311\u2013316. (1980)"}, {"ref": "7. Chakrabarti D, Zhan Y, Faloutsos C. SIAM International Conference on Data Mining (SDM) Vol. 4. SIAM; 2004. R-MAT: A recursive model for graph mining; pp. 442\u2013446."}, {"ref": "8. Csardi G, Nepusz T. The igraph software package for complex network research. Inter Journal, Complex Systems. 2006;1695(5) (2006)"}, {"ref": "9. Easley D, Kleinberg J. Networks, crowds, and markets: Reasoning about a highly connected world. Cambridge University Press; 2010."}, {"ref": "10. Flaxman AD, Frieze AM, Vera J. A geometric preferential attachment model of networks. Internet Mathematics. 2006;3(2):187\u2013205. (2006)"}, {"ref": "11. Gomez-Rodriguez M, Leskovec J, Balduzzi D, Sch\u00f6lkopf B. Uncovering the structure and temporal dynamics of information propagation. Network Science. 2014;2(01):26\u201365. (2014)"}, {"ref": "12. Gomez-Rodriguez M, Leskovec J, Krause A. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) ACM; 2010. Inferring networks of diffusion and influence; pp. 1019\u20131028."}, {"ref": "13. Gomez-Rodriguez M, Leskovec J, Krause A. Inferring networks of diffusion and influence. ACM Transactions on Knowledge Discovery from Data. 2012 Feb.5(4):37. Article 21 2012."}, {"ref": "14. Gomez-Rodriguez M, Leskovec J, Sch\u00f6lkopf B. ACM International Conference on Web Search and Data Mining (WSDM) ACM; 2013. Structure and dynamics of information pathways in online media; pp. 23\u201332."}, {"ref": "15. Gonzalez JE, Low Y, Gu H, Bickson D, Guestrin C. PowerGraph: Distributed graph-parallel computation on natural graphs. USENIX Symposium on Operating Systems Design and Implementation (OSDI) 2012;12:2."}, {"ref": "16. Gregor D, Lumsdaine A. The parallel BGL: A generic library for distributed graph computations. Parallel Object-Oriented Scientific Computing (POOSC) 2005;2:1\u201318. (2005)"}, {"ref": "17. Hagberg A, Swart P, Chult DS. Technical Report. Los Alamos National Laboratory (LANL); 2008. Exploring network structure, dynamics, and function using NetworkX."}, {"ref": "18. Hallac D, Leskovec J, Boyd S. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) ACM; 2015. Network lasso: Clustering and optimization in large graphs; pp. 387\u2013396."}, {"ref": "19. Jackson MO. Social and economic networks. Vol. 3. Princeton: Princeton university press; 2008."}, {"ref": "20. Kang U, Tsourakakis CE, Faloutsos C. IEEE International Conference on Data Mining (ICDM) IEEE; 2009. Pegasus: A peta-scale graph mining system implementation and observations; pp. 229\u2013238."}, {"ref": "21. Kim J, Han W-S, Lee S, Park K, Yu H. ACM SIGMOD International Conference on Management of Data (SIGMOD) ACM; 2014. OPT: a new framework for overlapped and parallel triangulation in large-scale graphs; pp. 637\u2013648."}, {"ref": "22. Kim M, Leskovec J. Modeling social networks with node attributes using the multiplicative attribute graph model; Conference on Uncertainty in Artificial Intelligence (UAI); 2011a."}, {"ref": "23. Kim M, Leskovec J. The network completion problem: inferring missing nodes and edges in networks; SIAM International Conference on Data Mining (SDM); 2011b. pp. 47\u201358."}, {"ref": "24. Kim M, Leskovec J. Latent multi-group membership graph model; International Conference on Machine Learning (ICML); 2012a."}, {"ref": "25. Kim M, Leskovec J. Multiplicative attribute graph model of real-world networks. Internet Mathematics. 2012b;8(1\u20132):113\u2013160. (2012)"}, {"ref": "26. Kim M, Leskovec J. Nonparametric multi-group membership model for dynamic networks. Advances in Neural Information Processing Systems (NIPS) 2013:1385\u20131393."}, {"ref": "27. Kumar R, Raghavan P, Rajagopalan S, Sivakumar D, Tomkins A, Upfal E. Annual Symposium on Foundations of Computer Science. IEEE; 2000. Stochastic models for the web graph; pp. 57\u201365."}, {"ref": "28. Kwak H, Lee C, Park H, Moon S. What is Twitter, a social network or a news media? WWW \u201910."}, {"ref": "29. Kyrola A, Blelloch G, Guestrin C. GraphChi: Large-scale graph computation on just a PC; USENIX Symposium on Operating Systems Design and Implementation (OSDI); 2012. pp. 31\u201346."}, {"ref": "30. Leskovec J, Backstrom L, Kleinberg J. Meme-tracking and the dynamics of the news cycle; ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD); 2009. pp. 497\u2013506."}, {"ref": "31. Leskovec J, Chakrabarti D, Kleinberg J, Faloutsos C, Ghahramani Z. Kronecker graphs: An approach to modeling networks. Journal of Machine Learning Research. 2010;11:985\u20131042. (2010)"}, {"ref": "32. Leskovec J, Horvitz E. Geospatial structure of a planetary-scale social network. IEEE Transactions on Computational Social Systems. 2014;1(3):156\u2013163. (2014)"}, {"ref": "33. Leskovec J, Kleinberg J, Faloutsos C. ACM SIGKDD International Conference on Knowledge Discovery in Data Mining (KDD) ACM; 2005. Graphs over time: densification laws, shrinking diameters and possible explanations; pp. 177\u2013187."}, {"ref": "34. Leskovec J, Krevl A. SNAP Datasets: Stanford Large Network Dataset Collection. 2014 Jun; http://snap.stanford.edu/data 2014."}, {"ref": "35. Lofgren P, Banerjee S, Goel A. ACM International Conference on Web Search and Data Mining (WSDM) ACM; 2016. Personalized PageRank estimation and search: a bidirectional approach."}, {"ref": "36. Lofgren PA, Banerjee S, Goel A, Seshadhri C. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) ACM; 2014. Fast-ppr: Scaling personalized pagerank estimation for large graphs; pp. 1436\u20131445."}, {"ref": "37. Malewicz G, Austern MH, Bik AJC, Dehnert JC, Horn I, Leiser N, Czajkowski G. ACM SIGMOD International Conference on Management of data (SIGMOD) ACM; 2010. Pregel: a system for large-scale graph processing; pp. 135\u2013146."}, {"ref": "38. McAuley J, Leskovec J. Learning to discover social circles in ego networks. Advances in Neural Information Processing Systems (NIPS) 2012"}, {"ref": "39. McAuley J, Leskovec J. Discovering social circles in ego networks. ACM Transactions on Knowledge Discovery from Data. 2014 Feb.8(1):28. Article 4 2014."}, {"ref": "40. Milo R, Kashtan N, Itzkovitz S, Newman MEJ, Alon U. On the uniform generation of random graphs with prescribed degree sequences. arXiv preprint cond-mat/0312028. 2003 (2003)"}, {"ref": "41. Newman M. The structure and function of complex networks. SLAM Rev. 2003;45(2):167\u2013256. (2003)"}, {"ref": "42. Newman M. Networks: An introduction. Oxford: OUP; 2010."}, {"ref": "43. O\u2019Madadhain J, Fisher D, Smyth P, White S, Boey Y. Analysis and visualization of network data using JUNG. Journal of Statistical Software. 2005;10(2):1\u201335. (2005)"}, {"ref": "44. Page L, Brin S, Motwani R, Winograd T. Technical Report. Stanford InfoLab; 1999. Nov, The pagerank citation ranking: Bringing order to the web."}, {"ref": "45. Perez Y, Sosi\u010d R, Banerjee A, Puttagunta R, Raison M, Shah P, Leskovec J. Ringo: Interactive graph analytics on big-memory machines; ACM SIGMOD International Conference on Management of Data (SIGMOD); 2015. pp. 1105\u20131110."}, {"ref": "46. Ravasz E, Barabasi A-L. Hierarchical organization in complex networks. Physical Review E. 2003;67(2):026112. (2003)"}, {"ref": "47. Salihoglu S, Widom J. International Conference on Scientific and Statistical Database Management. ACM; 2013. GPS: A graph processing system; p. 22."}, {"ref": "48. Suen C, Huang S, Eksombatchai C, Sosi\u010d R, Leskovec J. International conference on World Wide Web (WWW) International World Wide Web Conferences Steering Committee; 2013. NIFTY: A system for large scale information flow tracking and clustering; pp. 1237\u20131248."}, {"ref": "49. Watts DJ, Strogatz SH. Collective dynamics of small-world networks. Nature. 1998;393(6684):440\u2013442. (1998)"}, {"ref": "50. Xin RS, Gonzalez JE, Franklin MJ, Stoica I. ACM International Workshop on Graph Data Management Experiences and Systems. ACM; 2013. GraphX: A resilient distributed graph system on Spark; p. 2."}, {"ref": "51. Yang J, Leskovec J. IEEE International Conference on Data Mining (ICDM) IEEE; 2012. Community-affiliation graph model for overlapping network community detection; pp. 1170\u20131175."}, {"ref": "52. Yang J, Leskovec J. ACM International Conference on Web Search and Data Mining (WSDM) ACM; 2013. Overlapping community detection at scale: A nonnegative matrix factorization approach; pp. 587\u2013596."}, {"ref": "53. Yang J, Leskovec J. Overlapping communities explain core-periphery organization of networks. Proc IEEE. 2014 Dec;102(12):1892\u20131902. 2014."}, {"ref": "54. Yang J, McAuley J, Leskovec J. Community detection in networks with node attributes; IEEE International Conference on Data Mining (ICDM); 2013."}, {"ref": "55. Yang J, McAuley J, Leskovec J. Detecting cohesive and 2-mode communities in directed and undirected networks; ACM International Conference on Web Search and Data Mining (WSDM); 2014."}]}, {"author": ["C Tu", "W Zhang", "Z Liu"], "title": "Max-margin deepwalk: Discriminative learning of network representation.", "journal": "Proc. 25th Int. Joint Conf. Artif. Intell.", "year": 2016, "DOI": "0", "month": 0, "citations(google scholar)": 154, "abstract": "DeepWalk is a typical representation learning method that learns low-dimensional representations for vertices in social networks. Similar to other network representation learning (NRL) models, it encodes the network structure into vertex representations and is learnt in unsupervised form. However, the learnt representations usually lack the ability of discrimination when applied to machine learning tasks, such as vertex classification. In this paper, we overcome this challenge by proposing a novel semisupervised model, max DeepWalk is a typical representation learning method that learns low-dimensional representations for vertices in social networks. Similar to other network representation learning (NRL) models, it encodes the network structure into vertex representations and is learnt in unsupervised form. However, the learnt representations usually lack the ability of discrimination when applied to machine learning tasks, such as vertex classification. In this paper, we overcome this challenge by proposing a novel semisupervised model, max-margin DeepWalk (MMDW). MMDW is a unified NRL framework that jointly optimizes the max-margin classifier and the aimed social representation learning model. Influenced by the max-margin classifier, the learnt representations not only contain the network structure, but also have the characteristic of discrimination. The visualizations of learnt representations indicate that our model is more discriminative than unsupervised ones, and the experimental results on vertex classification demonstrate that our method achieves a significant improvement than other state-of-theart methods. The source code can be obtained from https://github.com/thunlp/MMDW.", "keywords": ["0"], "reference_count": 16, "ccfClass": "0", "important": true, "references": [{"ref": "[Blei et al., 2003] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. JMLR, 3:993\u20131022, 2003."}, {"ref": "[Crammer and Singer, 2002] Koby Crammer and Yoram Singer. On the learnability and design of output codes for multiclass problems. Machine learning, 47(2-3):201\u2013233, 2002."}, {"ref": "[Hearst et al., 1998] Marti A. Hearst, Susan T Dumais, Edgar Osman, John Platt, and Bernhard Scholkopf. Support vector machines. IEEE Intelligent Systems and their Applications, 13(4):18\u201328, 1998."}, {"ref": "[Keerthi et al., 2008] S Sathiya Keerthi, Sellamanickam Sundararajan, Kai-Wei Chang, Cho-Jui Hsieh, and ChihJen Lin. A sequential dual method for large scale multiclass linear svms. In Proceedings of SIGKDD, pages 408\u2013416, 2008."}, {"ref": "[McCallum et al., 2000] Andrew McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the construction of internet portals with machine learning. Information Retrieval Journal, 3:127\u2013163, 2000."}, {"ref": "[Mikolov et al., 2013a] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In Proceedings of ICIR, 2013."}, {"ref": "[Mikolov et al., 2013b] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS, pages 3111\u20133119, 2013."}, {"ref": "[Pei et al., 2014] Wenzhe Pei, Tao Ge, and Baobao Chang. Max-margin tensor neural network for chinese word segmentation. In Proceedings of ACL, pages 293\u2013303, 2014."}, {"ref": "[Perozzi et al., 2014] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of KDD, pages 701\u2013710, 2014."}, {"ref": "[Roller, 2004] Ben Taskar Carlos Guestrin Daphne Roller. Max-margin markov networks. In Proceedings of NIPS, 2004."}, {"ref": "[Sen et al., 2008] Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad. Collective classification in network data. AI Magazine, 29(3):93\u2013106, 2008."}, {"ref": "[Tang et al., 2015] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Largescale information network embedding. In Proceedings of WWW, pages 1067\u20131077, 2015."}, {"ref": "[Taskar et al., 2004] Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and Christopher D Manning. Max-margin parsing. In Proceedings of EMNLP, volume 1, page 3, 2004."}, {"ref": "[Yang et al., 2015] Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. Network representation learning with rich text information. In Proceedings of IJCAI, 2015."}, {"ref": "[Yu et al., 2014] Hsiang-Fu Yu, Prateek Jain, Purushottam Kar, and Inderjit Dhillon. Large-scale multi-label learning with missing labels. In Proceedings of ICML, pages 593\u2013601, 2014."}, {"ref": "[Zhu et al., 2012] Jun Zhu, Amr Ahmed, and Eric P Xing. Medlda: maximum margin supervised topic models. JMLR, 13(1):2237\u20132278, 2012."}]}, {"author": ["M. A. Hearst", "S. T. Dumais", "E. Osuna", "J. Platt", "and B. Scholkopf"], "title": "Support vector machines", "journal": "IEEE Intell. Syst. Appl.", "year": 1988, "DOI": "0", "month": 0, "citations(google scholar)": 583, "abstract": "An Introduction to Mello cristianini John Shawe-Taylor Support Vector Machines and other kernel-based earning methods", "keywords": ["0"], "reference_count": 0, "ccfClass": "0", "important": true, "references": []}, {"author": ["T. M. Le and H. W. Lauw"], "title": "Probabilistic latent document network embedding", "journal": "Proc. IEEE Int. Conf. Data Mining", "year": 2014, "DOI": "0", "month": 0, "citations(google scholar)": 37, "abstract": "A document network refers to a data type that can be represented as a graph of vertices, where each vertex is associated with a text document. Examples of such a data type include hyperlinked Web pages, academic publications with citations, and user profiles in social networks. Such data have very high-dimensional representations, in terms of text as well as network connectivity. In this paper, we study the problem of embedding, or finding a low-dimensional representation of a document network that \"preserves\" the data as much as possible. These embedded representations are useful for various applications driven by dimensionality reduction, such as visualization or feature selection. While previous works in embedding have mostly focused on either the textual aspect or the network aspect, we advocate a holistic approach by finding a unified low-rank representation for both aspects. Moreover, to lend semantic interpretability to the low-rank representation, we further propose to integrate topic modeling and embedding within a joint model. The gist is to join the various representations of a document (words, links, topics, and coordinates) within a generative model, and to estimate the hidden representations through MAP estimation. We validate our model on real-life document networks, showing that it outperforms comparable baselines comprehensively on objective evaluation metrics.", "keywords": ["Visualization", "Nickel", "Joints", "Mathematical model", "Educational institutions", "Data visualization", "Semantics"], "reference_count": 38, "ccfClass": "0", "important": true, "references": [{"ref": "1. I. Jolliffe, Principal Component Analysis, Wiley Online Library, 2005."}, {"ref": "2. L. V. Der Maaten, G. Hinton, \"Visualizing data using t-SNE\", JMLR, vol. 9, 2008."}, {"ref": "3. H. C. Purchase, \"Metrics for graph drawing aesthetics\", JVLC, 2002."}, {"ref": "4. D. M. Blei, A. Y. Ng, M. I. Jordan, \"Latent dirichlet allocation\", JMLR, 2003."}, {"ref": "5. T. Iwata, T. Yamada, N. Ueda, \"Probabilistic latent semantic visualization: topic model for visualizing documents\", KDD, 2008."}, {"ref": "6. J. Choo, C. Lee, C. K. Reddy, H. Park, \"Utopian: User-driven topic modeling based on interactive nonnegative matrix factorization\", TVCG, 2013."}, {"ref": "7. J. B. Kruskal, \"Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis\", Psychometrika, 1964."}, {"ref": "8. S. T. Roweis, L. K. Saul, \"Nonlinear dimensionality reduction by locally linear embedding\", Science, 2000."}, {"ref": "9. J. B. Tenenbaum, V. D. Silva, J. C. Langford, \"A global geometric framework for nonlinear dimensionality reduction\", Science, 2000."}, {"ref": "10. T. Iwata, K. Saito, N. Ueda, S. Stromsten, T. L. Griffiths, J. B. Tenenbaum, \"Parametric embedding for class visualization\", Neural Computation, 2007."}, {"ref": "11. T. M. V. Le, H. W. Lauw, \"Manifold learning for jointly modeling topic and visualization\", AAAI, 2014."}, {"ref": "12. T. M. V. Le, H. W. Lauw, \"Semantic visualization for spherical representation\", KDD, 2014."}, {"ref": "13. G. H. Golub, C. F. V. Loan, Matrix Computations, JHU Press, vol. 3, 2012."}, {"ref": "14. A. Talwalkar, S. Kumar, M. Mohri, H. Rowley, \"Large-scale SVD and manifold learning\", JMLR, 2013."}, {"ref": "15. B. Shaw, T. Jebara, \"Structure preserving embedding\", ICML, 2009."}, {"ref": "16. T. M. Fruchterman, E. M. Reingold, \"Graph drawing by force-directed placement\", Software: Practice and Experience, 1991."}, {"ref": "17. T. Kamada, S. Kawai, \"An algorithm for drawing general undirected graphs\", Information Processing Letters, 1989."}, {"ref": "18. M. Bastian, S. Heymann, M. Jacomy et al., \"Gephi: an open source software for exploring and manipulating networks\", ICWSM, 2009."}, {"ref": "19. J. Ellson, E. Gansner, L. Koutsofios, S. C. North, G. Woodhull, \"Graphviz - open source graph drawing tools\", Graph Drawing, 2002."}, {"ref": "20. A. Goldenberg, A. X. Zheng, S. E. Fienberg, E. M. Airoldi, \"A survey of statistical network models\", FTML, 2010."}, {"ref": "21. E. M. Airoldi, D. M. Blei, S. E. Fienberg, E. P. Xing, \"Mixed membership stochastic blockmodels\", NIPS, 2009."}, {"ref": "22. Q. Mei, D. Cai, D. Zhang, C. Zhai, \"Topic modeling with network regularization\", WWW. ACM, pp. 101-110, 2008."}, {"ref": "23. J. Chang, D. M. Blei, \"Relational topic models for document networks\", AISTATS, 2009."}, {"ref": "24. R. Nallapati, W. W. Cohen, \"Link-PLSA-LDA: A new unsupervised model for topics and influence of blogs\", ICWSM, 2008."}, {"ref": "25. Y. Sun, J. Han, J. Gao, Y. Yu, \"iTopicModel: Information network-integrated topic modeling\", ICDM, 2009."}, {"ref": "26. Y. Liu, A. Niculescu-Mizil, W. Gryc, \"Topic-Link LDA: Joint models of topic and author community\", ICML, 2009."}, {"ref": "27. F. Wei, S. Liu, Y. Song, S. Pan, M. X. Zhou, W. Qian, L. Shi, L. Tan, Q. Zhang, \"Tiara: a visual exploratory text analytic system\", KDD, 2010."}, {"ref": "28. B. Gretarsson, J. O'donovan, S. Bostandjiev, T. H\u00f6llerer, A. Asuncion, D. Newman, P. Smyth, \"TopicNets: Visual analysis of large text corpora with topic modeling\", TIST, 2012."}, {"ref": "29. A. J.-B. Chaney, D. M. Blei, \"Visualizing topic models\", ICWSM, 2012."}, {"ref": "30. J. Chuang, C. D. Manning, J. Heer, \"Termite: visualization techniques for assessing textual topic models\", AVI, 2012."}, {"ref": "31. C. M. Bishop, Neural Networks for Pattern Recognition, Oxford University Press, 1995."}, {"ref": "32. A. P. Dempster, N. M. Laird, D. B. Rubin, \"Maximum likelihood from incomplete data via the EM algorithm\", Journal of the Royal Statistical Society, 1977."}, {"ref": "33. D. C. Liu, J. Nocedal, \"On the limited memory BFGS method for large scale optimization\", Mathematical Programming, vol. 45, 1989."}, {"ref": "34. A. K. McCallum, K. Nigam, J. Rennie, K. Seymore, \"Automating the construction of internet portals with machine learning\", Information Retrieval, 2000."}, {"ref": "35. S. Zhu, K. Yu, Y. Chi, Y. Gong, \"Combining content and link for classification using matrix factorization\", SIGIR, 2007."}, {"ref": "36. D. Newman, S. Karimi, L. Cavedon, \"External evaluation of topic models\", ADCS, 2009."}, {"ref": "37. T. Brants, A. Franz, \"Web 1T 5-gram Version 1\", 2006."}, {"ref": "38. N. Craswell, \"Mean reciprocal rank\", Encyclopedia of Database Systems, 2009."}]}, {"author": ["J. Chang and D. M. Blei"], "title": "Relational topic models for document networks", "journal": "Proc. Int. Conf. Artif. Intell. Statist.", "year": 2009, "DOI": "0", "month": 0, "citations(google scholar)": 493, "abstract": "We develop the relational topic model (RTM), a model of documents and the links between them. For each pair of documents, the RTM models their link as a binary random variable that is conditioned on their contents. The model can be used to summarize a network of documents, predict links between them, and predict words within them. We derive efficient inference and learning algorithms based on variational methods and evaluate the predictive performance of the RTM for large networks of scientific abstracts and web documents.", "keywords": ["0"], "reference_count": 0, "ccfClass": "0", "important": true, "references": []}, {"author": ["C. Yang", "Z. Liu", "D. Zhao", "M. Sun", "and E. Y. Chang"], "title": "Network representation learning with rich text information", "journal": "Proc. 24th Int. Joint Conf. Artif. Intell.", "year": 2015, "DOI": "0", "month": 0, "citations(google scholar)": 413, "abstract": "Representation learning has shown its effectiveness in many tasks such as image classification and text mining. Network representation learning aims at learning distributed vector representation for each vertex in a network, which is also increasingly recognized as an important aspect for network analysis. Most network representation learning methods investigate network structures for learning. In reality, network vertices contain rich information (such as text), which cannot be well applied with algorithmic frameworks of typical representation learning methods. By proving that DeepWalk, a state-of-the-art network representation method, is actually equivalent to matrix factorization (MF), we propose text-associated DeepWalk (TADW). TADW incorporates text features of vertices into network representation learning under the framework of matrix factorization. We evaluate our method and various baseline methods by applying them to the task of multi-class classification of vertices. The experimental results show that, our method outperforms other baselines on all three datasets, especially when networks are noisy and training ratio is small.", "keywords": ["0"], "reference_count": 0, "ccfClass": "0", "important": true, "references": []}, {"author": ["N. Natarajan and I. S. Dhillon"], "title": "Inductive matrix completion for predicting gene\u2013disease associations", "journal": "Bioinf., vol. 30", "year": 2014, "DOI": "0", "month": 0, "citations(google scholar)": 169, "abstract": "Motivation: Most existing methods for predicting causal disease genes rely on specific type of evidence, and are therefore limited in terms of applicability. More often than not, the type of evidence available for diseases varies\u2014for example, we may know linked genes, keywords associated with the disease obtained by mining text, or co-occurrence of disease symptoms in patients. Similarly, the type of evidence available for genes varies\u2014for example, specific microarray probes convey information only for certain sets of genes.", "keywords": ["0"], "reference_count": 0, "ccfClass": "0", "important": true, "references": []}, {"author": ["X. Sun", "J. Guo", "X. Ding", "and T. Liu"], "title": "A general framework for content-enhanced network representation learning", "journal": "arXiv:1610.02906", "year": 2016, "DOI": "0", "month": 0, "citations(google scholar)": 39, "abstract": "This paper investigates the problem of network embedding, which aims at learning low-dimensional vector representation of nodes in networks. Most existing network embedding methods rely solely on the network structure, i.e., the linkage relationships between nodes, but ignore the rich content information associated with it, which is common in real world networks and beneficial to describing the characteristics of a node. In this paper, we propose content-enhanced network embedding (CENE), which is capable of jointly leveraging the network structure and the content information. Our approach integrates text modeling and structure modeling in a general framework by treating the content information as a special kind of node. Experiments on several real world net- works with application to node classification show that our models outperform all existing network embedding methods, demonstrating the merits of content information and joint learning.", "keywords": ["0"], "reference_count": 9, "ccfClass": "0", "important": true, "references": [{"ref": "Xu Z, Su Q, Quan X, et al. A Deep Neural Information Fusion Architecture for Textual Network Embeddings[J]. arXiv preprint arXiv:1908.11057, 2019."}, {"ref": "Tu C, Liu H, Liu Z, et al. Cane: Context-aware network embedding for relation modeling[C]//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2017: 1722-1731."}, {"ref": "Zhuo W, Zhan Q, Liu Y, et al. Context Attention Heterogeneous Network Embedding[J]. Computational intelligence and neuroscience, 2019, 2019."}, {"ref": "Zhang X, Li Y, Shen D, et al. Diffusion maps for textual network embedding[C]//Advances in Neural Information Processing Systems. 2018: 7587-7597."}, {"ref": "Shen D, Zhang X, Henao R, et al. Improved semantic-aware network embedding with fine-grained word alignment[J]. arXiv preprint arXiv:1808.09633, 2018."}, {"ref": "Chen L, Zhang Y, Zhang R, et al. Improving sequence-to-sequence learning via optimal transport[J]. arXiv preprint arXiv:1901.06283, 2019."}, {"ref": "Wu J, Yao L, Liu B. An overview on feature-based classification algorithms for multivariate time series[C]//2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA). IEEE, 2018: 32-38."}, {"ref": "Cui P, Wang X, Pei J, et al. A survey on network embedding[J]. IEEE Transactions on Knowledge and Data Engineering, 2018, 31(5): 833-852."}, {"ref": "Chen H, Perozzi B, Al-Rfou R, et al. A tutorial on network embeddings[J]. arXiv preprint arXiv:1808.02590, 2018."}]}, {"author": ["S. Pan", "J. Wu", "X. Zhu", "C. Zhang", "and Y. Wang"], "title": "Tri-party deep network representation", "journal": "Netw., vol. 11", "year": 2016, "DOI": "0", "month": 0, "citations(google scholar)": 188, "abstract": "Information network mining often requires examination of linkage relationships between nodes for analysis. Recently, network representation has emerged to represent each node in a vector format, embedding network structure, so off-the-shelf machine learning methods can be directly applied for analysis. To date, existing methods only focus onone aspect of node information and cannot leverage node labels. In this paper, we propose TriDNR, a tri-party deep network representation model, using information from three parties: node structure, node content, and node labels (if available) to jointly learn optimal node representation. TriDNR is based on our new coupled deep natural languagemodule, whose learning is enforced at three levels: (1) at the network structure level, TriDNR exploits inter-node relationship by maximizing the probability of observing surrounding nodes given a node in random walks; (2) at the node content level, TriDNR captures node-word correlation by maximizing the co-occurrence of word sequence given anode; and (3) at the node label level, TriDNR models label-word correspondence by maximizing the probability of word sequence given a class label. The tri-party information is jointly fed into the neural network model to mutually enhance each other to learn optimal representation, and results in upto 79% classification accuracy gain, compared to state-of-the-art methods.", "keywords": ["0"], "reference_count": 18, "ccfClass": "0", "important": true, "references": [{"ref": "David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:993\u20131022, 2003."}, {"ref": "Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. Learning structured embeddings of knowledge bases. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011."}, {"ref": "Jonathan Chang and David M Blei. Relational topic models for document networks. In International Conference on Artificial Intelligence and Statistics, pages 81\u201388, 2009."}, {"ref": "Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C Aggarwal, and Thomas S Huang. Heterogeneous network embedding via deep architectures. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 119\u2013128. ACM, 2015."}, {"ref": "Thorsten Joachims. Training linear svms in linear time. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 217\u2013226. ACM, 2006."}, {"ref": "Quoc V Le and Tomas Mikolov. Distributed representations of sentences and documents. In International Conference on Machine Learning, pages 1188\u20131196, 2014."}, {"ref": "Kar Wai Lim and Wray Buntine. Bibliographic analysis with the citation network topic model. In Proceedings of the Sixth Asian Conference on Machine Learning, pages 142\u2013158, 2014."}, {"ref": "Minh-Thang Luong, Richard Socher, and Christopher D Manning. Better word representations with recursive neural networks for morphology. CoNLL-2013, 104, 2013."}, {"ref": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013."}, {"ref": "Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model.In Proceedings of the international workshop on artificial intelligence and statistics, pages 246\u2013252. Citeseer, 2005."}, {"ref": "Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701\u2013710. ACM, 2014."}, {"ref": "Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008."}, {"ref": "Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning with neural tensor networks for knowledge base completion. In Advances in Neural Information Processing Systems, pages 926\u2013934, 2013."}, {"ref": "Lei Tang and Huan Liu. Relationallearning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD international conference on Knowledgediscovery and data mining, pages 817\u2013826. ACM, 2009."}, {"ref": "Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and mining of academic social networks. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 990\u2013998. ACM, 2008."}, {"ref": "] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067\u2013 1077. International World Wide Web Conferences Steering Committee, 2015."}, {"ref": "Fei Tian, Bin Gao, Qing Cui, Enhong Chen, and Tie-Yan Liu. Learning deep representations for graph clustering. In AAAI, pages 1293\u20131299, 2014."}, {"ref": "Cheng Yang, Zhiyuan Liu, Deli Zhao,Maosong Sun, and Edward Y Chang. Network representation learning with rich text information. In International Joint Conference on Artificial Intelligence, 2015."}]}, {"author": ["X. Huang", "J. Li", "and X. Hu"], "title": "Label informed attributed network embedding", "journal": "Proc. 10th ACM Int. Conf. Web Search Data Mining", "year": 2017, "DOI": "0", "month": 0, "citations(google scholar)": 198, "abstract": "Attributed network embedding aims to seek low-dimensional vector representations for nodes in a network, such that original network topological structure and node attribute proximity can be preserved in the vectors.", "keywords": ["0"], "reference_count": 0, "ccfClass": "0", "important": true, "references": []}, {"author": ["F. R. K. Chung and F. C. Graham"], "title": "Spectral graph theory", "journal": "American Mathematical Soc.", "year": 1997, "DOI": "0", "month": 0, "citations(google scholar)": 8095, "abstract": "Beautifully written and elegantly presented, this book is based on 10 lectures given at the CBMS workshop on spectral graph theory in June 1994 at Fresno State University. Chung's well-written exposition can be likened to a conversation with a good teacher", "keywords": ["0"], "reference_count": 0, "ccfClass": "0", "important": true, "references": []}]