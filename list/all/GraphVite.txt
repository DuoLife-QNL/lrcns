[{"author": ["Theo Trouillon", "Johannes Welb", "Sebastian Riedel", "Eric Gaussier", "Guillaume Bouchard"], "title": "Complex Embeddings for Simple Link Prediction", "journal": "International Conference on Machine Learning", "year": 2016, "DOI": "0", "month": 6, "citations(google scholar)": 291, "abstract": "In statistical relational learning, the link prediction problem is key to automatically understand thestructureoflargeknowledgebases. Asinprevious studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.", "keywords": ["keywords"], "reference_count": 26, "ccfClass": "A", "important": true, "references": [{"ref": "[1]Alon, Noga, Moran, Shay, and Yehudayoff, Amir.Sign rank versus vc dimension. arXiv preprint arXiv:1503.07648, 2015."}, {"ref": "[2]Auer, Sren, Bizer, Christian, Kobilarov, Georgi, Lehmann,Jens, and Ives, Zachary. Dbpedia: A nucleus for a web of open data. In In 6th Intl Semantic Web Conference,Busan, Korea, pp. 11\u201315. Springer, 2007."}, {"ref": "[3]Autonne, L. Sur les matrices hypohermitiennes et sur les matrices unitaires. Ann. Univ. Lyons, Nouvelle Srie I, 38:1\u201377, 1915."}, {"ref": "[4]Beltrami, Eugenio. Sulle funzioni bilineari. Giornale di Matematiche ad Uso degli Studenti Delle Universita, 11(2):98\u2013106, 1873."}, {"ref": "[5]Bergstra, James, Breuleux, Olivier, Bastien, Fr\u00b4ed\u00b4eric,Lamblin, Pascal, Pascanu, Razvan, Desjardins, Guillaume,Turian, Joseph, Warde-Farley, David, and Bengio,Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010. Oral Presentation."}, {"ref": "[6]Bollacker, Kurt, Evans, Colin, Paritosh, Praveen, Sturge,Tim, and Taylor, Jamie. Freebase: a collaboratively created graph database for structuring human knowledge.In SIGMOD 08 Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pp.1247\u20131250, 2008."}, {"ref": "[7]Bordes, Antoine, Usunier, Nicolas, Garcia-Duran, Alberto,Weston, Jason, and Yakhnenko, Oksana. Irreflexive and Hierarchical Relations as Translations. In CoRR, 2013a."}, {"ref": "[8]Bordes, Antoine, Usunier, Nicolas, Garcia-Duran, Alberto,Weston, Jason, and Yakhnenko, Oksana. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems, pp.2787\u20132795, 2013b."}, {"ref": "[9]Bouchard, Guillaume, Singh, Sameer, and Trouillon, Theo.On approximate reasoning capabilities of low-rank vector spaces. In AAAI Spring Syposium on Knowledge Representation and Reasoning (KRR): Integrating Symbolic and Neural Approaches, 2015."}, {"ref": "[10]Dong, Xin, Gabrilovich, Evgeniy, Heitz, Geremy, Horn,Wilko, Lao, Ni, Murphy, Kevin, Strohmann, Thomas,Sun, Shaohua, and Zhang, Wei. Knowledge vault: A web-scale approach to probabilistic knowledge fusion.In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,KDD '14, pp. 601\u2013610, 2014."}, {"ref": "[11]Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research,12:2121\u20132159, 2011."}, {"ref": "[12]Getoor, Lise and Taskar, Ben. Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning). The MIT Press, 2007. ISBN 0262072882."}, {"ref": "[13]Hitchcock, F. L. The expression of a tensor or a polyadic as a sum of products. J. Math. Phys, 6(1):164\u2013189, 1927."}, {"ref": "[14]Jenatton, Rodolphe, Bordes, Antoine, Le Roux, Nicolas,and Obozinski, Guillaume. A Latent Factor Model for Highly Multi-relational Data. In Advances in Neural Information Processing Systems 25, pp. 3167\u20133175, 2012."}, {"ref": "[15]Koren, Yehuda, Bell, Robert, and Volinsky, Chris. Matrix factorization techniques for recommender systems.Computer, 42(8):30\u201337, 2009."}, {"ref": "[16]Linial, Nati, Mendelson, Shahar, Schechtman, Gideon, and Shraibman, Adi. Complexity measures of sign matrices.Combinatorica, 27(4):439\u2013463, 2007."}, {"ref": "[17]Nickel, Maximilian, Tresp, Volker, and Kriegel, Hans-Peter. A Three-Way Model for Collective Learning on Multi-Relational Data. In 28th International Conference on Machine Learning, pp. 809\u2014-816, 2011."}, {"ref": "[18]Nickel, Maximilian, Jiang, Xueyan, and Tresp, Volker. Reducing the rank in relational factorization models by including observable patterns. In Advances in Neural Information Processing Systems, pp. 1179\u20131187, 2014."}, {"ref": "[19]Nickel, Maximilian, Murphy, Kevin, Tresp, Volker, and Gabrilovich, Evgeniy. A review of relational machine learning for knowledge graphs. Proceedings of the IEEE,104(1):11\u201333, 2016a."}, {"ref": "[20]Nickel, Maximilian, Rosasco, Lorenzo, and Poggio,Tomaso A. Holographic embeddings of knowledge graphs. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pp. 1955\u20131961, 2016b."}, {"ref": "[21]Riedel, Sebastian, Yao, Limin, McCallum, Andrew, and Marlin, Benjamin M. Relation extraction with matrix factorization and universal schemas. In Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics,Proceedings, pp. 74\u201384, 2013."}, {"ref": "[22]Socher, Richard, Chen, Danqi, Manning, Christopher D,and Ng, Andrew. Reasoning with neural tensor networks for knowledge base completion. In Advances in Neural Information Processing Systems, pp. 926\u2013934, 2013."}, {"ref": "[23]Sutskever, Ilya. Modelling Relational Data using Bayesian Clustered Tensor Factorization. In Advances in Neural Information Processing Systems, volume 22, pp. 1\u20138,2009."}, {"ref": "[24]Trouillon, Th\u00b4eo, Dance, Christopher R., Gaussier, \u00b4 Eric, and Bouchard, Guillaume. Decomposing real square matrices via unitary diagonalization. arXiv:1605.07103,2016."}, {"ref": "[25]Welbl, Johannes, Bouchard, Guillaume, and Riedel, Sebastian.A factorization machine framework for testing bigram embeddings in knowledgebase completion.arXiv:1604.05878, 2016."}, {"ref": "[26]Yang, Bishan, Yih, Wen-tau, He, Xiaodong, Gao, Jianfeng,and Deng, Li. Embedding entities and relations for learning and inference in knowledge bases. In International Conference on Learning Representations, 2015."}]}, {"author": ["Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena"], "title": "DeepWalk: Online Learning of Social Representations", "journal": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining", "year": 2014, "DOI": "10.1145/2623330.2623732", "month": 6, "citations(google scholar)": 2177, "abstract": "We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk\u2019s latent representations on several multi-label network classi\ufb01cation tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk\u2019s representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk\u2019s representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classi\ufb01cation, and anomaly detection.", "keywords": ["keywords"], "reference_count": 44, "ccfClass": "A", "important": true, "references": [{"ref": "[1] R. Andersen, F. Chung, and K. Lang. Local graph partitioning using pagerank vectors. In Foundations of Computer Science, 2006. FOCS\u201906. 47th Annual IEEE Symposium on, pages 475\u2013486. IEEE, 2006."}, {"ref": "[2] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. 2013."}, {"ref": "[3] Y. Bengio, R. Ducharme, and P. Vincent. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137\u20131155, 2003."}, {"ref": "[4] L. Bottou. Stochastic gradient learning in neural networks. In Proceedings of Neuro-N\u02c6\u0131mes 91, Nimes, France, 1991. EC2."}, {"ref": "[5] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM Computing Surveys (CSUR), 41(3):15, 2009."}, {"ref": "[6] R. Collobert and J. Weston. A uni\ufb01ed architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM, 2008."}, {"ref": "[7] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30\u201342, 2012."}, {"ref": "[8] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng. Large scale distributed deep networks. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1232\u20131240. 2012."}, {"ref": "[9] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio. Why does unsupervised pre-training help deep learning? The Journal of Machine Learning Research, 11:625\u2013660, 2010."}, {"ref": "[10] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classi\ufb01cation. Journal of Machine Learning Research, 9:1871\u20131874, 2008."}, {"ref": "[11] F. Fouss, A. Pirotte, J.-M. Renders, and M. Saerens. Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. Knowledge and Data Engineering, IEEE Transactions on, 19(3):355\u2013369, 2007."}, {"ref": "[12] B. Gallagher and T. Eliassi-Rad. Leveraging label-independent features for classi\ufb01cation in sparsely labeled networks: An empirical study. In Advances in Social Network Mining and Analysis, pages 1\u201319. Springer, 2010."}, {"ref": "[13] B. Gallagher, H. Tong, T. Eliassi-Rad, and C. Faloutsos. Using ghost edges for classi\ufb01cation in sparsely labeled networks. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201908, pages 256\u2013264, New York, NY, USA, 2008. ACM."}, {"ref": "[14] S. Geman and D. Geman. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. Pattern Analysis and Machine Intelligence, IEEE Transactions on, (6):721\u2013741, 1984."}, {"ref": "[15] L. Getoor and B. Taskar. Introduction to statistical relational learning. MIT press, 2007."}, {"ref": "[16] K. Henderson, B. Gallagher, L. Li, L. Akoglu, T. Eliassi-Rad, H. Tong, and C. Faloutsos. It\u2019s who you know: Graph mining using recursive structural features. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201911, pages 663\u2013671, New York, NY, USA, 2011. ACM."}, {"ref": "[17] G. E. Hinton. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society, pages 1\u201312. Amherst, MA, 1986."}, {"ref": "[18] R. A. Hummel and S. W. Zucker. On the foundations of relaxation labeling processes. Pattern Analysis and Machine Intelligence, IEEE Transactions on, (3):267\u2013287, 1983."}, {"ref": "[19] U. Kang, H. Tong, and J. Sun. Fast random walk graph kernel. In SDM, pages 828\u2013838, 2012."}, {"ref": "[20] R. I. Kondor and J. La\ufb00erty. Di\ufb00usion kernels on graphs and other discrete input spaces. In ICML, volume 2, pages 315\u2013322, 2002."}, {"ref": "[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classi\ufb01cation with deep convolutional neural networks. In NIPS, volume 1, page 4, 2012."}, {"ref": "[22] D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019\u20131031, 2007."}, {"ref": "[23] F. Lin and W. Cohen. Semi-supervised classi\ufb01cation of network data using very few labels. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on, pages 192\u2013199, Aug 2010."}, {"ref": "[24] S. A. Macskassy and F. Provost. A simple relational classi\ufb01er. In Proceedings of the Second Workshop on Multi-Relational Data Mining (MRDM-2003) at KDD-2003, pages 64\u201376, 2003."}, {"ref": "[25] S. A. Macskassy and F. Provost. Classi\ufb01cation in networked data: A toolkit and a univariate case study. The Journal of Machine Learning Research, 8:935\u2013983, 2007."}, {"ref": "[26] T. Mikolov, K. Chen, G. Corrado, and J. Dean. E\ufb03cient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013."}, {"ref": "[27] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages 3111\u20133119. 2013."}, {"ref": "[28] T. Mikolov, W.-t. Yih, and G. Zweig. Linguistic regularities in continuous space word representations. In Proceedings of NAACL-HLT, pages 746\u2013751, 2013."}, {"ref": "[29] A. Mnih and G. E. Hinton. A scalable hierarchical distributed language model. Advances in neural information processing systems, 21:1081\u20131088, 2009."}, {"ref": "[30] F. Morin and Y. Bengio. Hierarchical probabilistic neural network language model. In Proceedings of the international workshop on arti\ufb01cial intelligence and statistics, pages 246\u2013252, 2005."}, {"ref": "[31] J. Neville and D. Jensen. Iterative classi\ufb01cation in relational data. In Proc. AAAI-2000 Workshop on Learning Statistical Models from Relational Data, pages 13\u201320, 2000."}, {"ref": "[32] J. Neville and D. Jensen. Leveraging relational autocorrelation with latent group models. In Proceedings of the 4th International Workshop on Multi-relational Mining, MRDM \u201905, pages 49\u201355, New York, NY, USA, 2005. ACM. [33] J. Neville and D. Jensen. A bias/variance decomposition for models using collective inference. Machine Learning, 73(1):87\u2013106, 2008. [34] M. E. Newman. Modularity and community structure in networks. Proceedings of the National Academy of Sciences, 103(23):8577\u20138582, 2006."}, {"ref": "[35] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1988."}, {"ref": "[36] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems 24, pages 693\u2013701. 2011."}, {"ref": "[37] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classi\ufb01cation in network data. AI magazine, 29(3):93, 2008."}, {"ref": "[38] D. A. Spielman and S.-H. Teng. Nearly-linear time algorithms for graph partitioning, graph sparsi\ufb01cation, and solving linear systems. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 81\u201390. ACM, 2004."}, {"ref": "[39] L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201909, pages 817\u2013826, New York, NY, USA, 2009. ACM."}, {"ref": "[40] L. Tang and H. Liu. Scalable learning of collective behavior based on sparse social dimensions. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 1107\u20131116. ACM, 2009."}, {"ref": "[41] L. Tang and H. Liu. Leveraging social media networks for classi\ufb01cation. Data Mining and Knowledge Discovery, 23(3):447\u2013478, 2011."}, {"ref": "[42] S. Vishwanathan, N. N. Schraudolph, R. Kondor, and K. M. Borgwardt. Graph kernels. The Journal of Machine Learning Research, 99:1201\u20131242, 2010."}, {"ref": "[43] X. Wang and G. Sukthankar. Multi-label relational neighbor classi\ufb01cation using social context features. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 464\u2013472. ACM, 2013."}, {"ref": "[44] W. Zachary. An information \ufb02ow modelfor con\ufb02ict and \ufb01ssion in small groups1. Journal of anthropological research, 33(4):452\u2013473, 1977."}]}, {"author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "title": "EMBEDDING ENTITIES AND RELATIONS FOR LEARNING AND INFERENCE IN KNOWLEDGE BASES", "journal": "arXiv preprint", "year": 2015, "DOI": "DOI", "month": 8, "citations(google scholar)": 403, "abstract": "We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a uni\ufb01ed learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as BornInCitypa,bq^ CityInCountrypb,cq \u00f9\u00f1 Nationalitypa,cq. We \ufb01nd that embeddings learned from the bilinear objective are particularly good at capturing relational semantics, and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-ofthe-art con\ufb01dence-based rule mining approach in mining Horn rules that involve compositional reasoning.", "keywords": ["keywords"], "reference_count": 35, "ccfClass": "", "important": true, "references": [{"ref": "[1]Auer, S\u00a8oren, Bizer, Christian, Kobilarov, Georgi, Lehmann, Jens, Cyganiak, Richard, and Ives,Zachary. Dbpedia: A nucleus for a web of open data. In The semantic web, pp. 722\u2013735.Springer, 2007."}, {"ref": "[2]Bordes, Antoine, Weston, Jason, Collobert, Ronan, and Bengio, Yoshua. Learning structured embeddings of knowledge bases. In AAAI, 2011."}, {"ref": "[3]Bordes, Antoine, Glorot, Xavier, Weston, Jason, and Bengio, Yoshua. A semantic matching energy function for learning with multi-relational data. Machine Learning, pp. 1\u201327, 2013a."}, {"ref": "[4]Bordes, Antoine, Usunier, Nicolas, Garcia-Duran, Alberto,Weston, Jason, and Yakhnenko, Oksana.Translating embeddings for modeling multi-relational data. In NIPS, 2013b."}, {"ref": "[5]Bowman, Samuel R. Can recursive neural tensor networks learn logical reasoning? In ICLR, 2014."}, {"ref": "[6]Chang, Kai-Wei, Yih, Wen-tau, Yang, Bishan, and Meek, Chris. Typed tensor decomposition of knowledge bases for relation extraction. In EMNLP, 2014."}, {"ref": "[7]Deng, Li, Hinton, G., and Kingsbury, B. New types of deep neural network learning for speech recognition and related applications: An overview. In in ICASSP, 2013."}, {"ref": "[8]Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121\u20132159, 2011."}, {"ref": "[9]Gal\u00b4arraga, Luis Antonio, Teflioudi, Christina, Hose, Katja, and Suchanek, Fabian. Amie: association rule mining under incomplete evidence in ontological knowledge bases. In WWW, 2013."}, {"ref": "[10]Gao, Jianfeng, Pantel, Patrick, Gamon, Michael, He, Xiaodong, Deng, Li, and Shen, Yelong. Modeling interestingness with deep neural networks. In EMNLP, 2014."}, {"ref": "[11]Garc\u00b4\u0131a-Dur\u00b4an, Alberto, Bordes, Antoine, and Usunier, Nicolas. Effective blending of two and threeway interactions for modeling multi-relational data. In Machine Learning and Knowledge Discovery in Databases, pp. 434\u2013449. Springer, 2014."}, {"ref": "[12]Getoor, Lise and Taskar, Ben (eds.). Introduction to Statistical Relational Learning. The MIT Press,2007."}, {"ref": "[13]Grefenstette, Edward. Towards a formal distributional semantics: Simulating logical calculi with tensors. In *SEM, 2013."}, {"ref": "[14]Hinton, Geoff, Deng, L., Yu, D., Dahl, G., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V.,Nguyen, P., Sainath, T., and Kingsbury, B. Deep neural networks for acoustic modeling in speech recognition. IEEE Sig. Proc. Mag., 29:82\u201397, 2012."}, {"ref": "[15]Huang, Po-Sen, He, Xiaodong, Gao, Jianfeng, Deng, Li, Acero, Alex, and Heck, Larry. Learning deep structured semantic models for Web search using clickthrough data. In CIKM, 2013."}, {"ref": "[16]Hutchinson, B, Deng, L., and Yu, D. Tensor deep stacking networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1944\u20131957, 2013."}, {"ref": "[17]Jenatton, Rodolphe, Le Roux, Nicolas, Bordes, Antoine, and Obozinski, Guillaume. A latent factor model for highly multi-relational data. In NIPS, 2012."}, {"ref": "[18]Kemp, Charles, Tenenbaum, Joshua B, Griffiths, Thomas L, Yamada, Takeshi, and Ueda, Naonori.Learning systems of concepts with an infinite relational model. In AAAI, volume 3, pp. 5, 2006."}, {"ref": "[19]Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed representations of words and phrases and their compositionality. In NIPS, pp. 3111\u20133119, 2013."}, {"ref": "[20]Nickel, Maximilian, Tresp, Volker, and Kriegel, Hans-Peter. A three-way model for collective learning on multi-relational data. In ICML, pp. 809\u2013816, 2011."}, {"ref": "[21]Nickel, Maximilian, Tresp, Volker, and Kriegel, Hans-Peter. Factorizing YAGO: scalable machine learning for linked data. In WWW, pp. 271\u2013280, 2012."}, {"ref": "[22]Paccanaro, Alberto and Hinton, Geoffrey E. Learning distributed representations of concepts using linear relational embedding. IEEE Transactions on Knowledge and Data Engineering, 13(2):232\u2013244, 2001."}, {"ref": "[23]Richardson, Matthew and Domingos, Pedro. Markov logic networks. Machine learning, 62(1-2):107\u2013136, 2006."}, {"ref": "[24]Rockt\u00a8aschel, Tim, Bo\u02c7snjak, Matko, Singh, Sameer, and Riedel, Sebastian. Low-dimensional embeddings of logic. In ACL Workshop on Semantic Parsing, 2014."}, {"ref": "[25]Schoenmackers, Stefan, Etzioni, Oren, Weld, Daniel S, and Davis, Jesse. Learning first-order horn clauses from web text. In EMNLP, 2010."}, {"ref": "[26]Shen, Yelong, He, Xiaodong, Gao, Jianfeng, Deng, Li, and Mesnil, Gregoire. A latent semantic model with convolutional-pooling structure for information retrieval. In CIKM, 2014a."}, {"ref": "[27]Shen, Yelong, He, Xiaodong, Gao, Jianfeng, Deng, Li, and Mesnil, Gr\u00b4egoire. Learning semantic representations using convolutional neural networks for Web search. In WWW, pp. 373\u2013374,2014b."}, {"ref": "[28]Singh, Ajit P and Gordon, Geoffrey J. Relational learning via collective matrix factorization. In KDD, pp. 650\u2013658. ACM, 2008."}, {"ref": "[29]Socher, Richard, Huval, Brody, Manning, Christopher D., and Ng, Andrew Y. Semantic compositionality through recursive matrix-vector spaces. In EMNLP-CoNLL, 2012."}, {"ref": "[30]Socher, Richard, Chen, Danqi, Manning, Christopher D., and Ng, Andrew Y. Reasoning with neural tensor networks for knowledge base completion. In NIPS, 2013."}, {"ref": "[31]Suchanek, Fabian M, Kasneci, Gjergji, andWeikum, Gerhard. Yago: a core of semantic knowledge.In WWW, 2007."}, {"ref": "[32]Sutskever, Ilya, Tenenbaum, Joshua B, and Salakhutdinov, Ruslan. Modelling relational data using Bayesian clustered tensor factorization. In NIPS, pp. 1821\u20131828, 2009."}, {"ref": "[33]Vinyals, O., Jia, Y., Deng, L., and Darrell, T. Learning with recursive perceptual representations. In NIPS, 2012."}, {"ref": "[34]Yih, Wen-tau, He, Xiaodong, and Meek, Christopher. Semantic parsing for single-relation question answering. In ACL, 2014."}, {"ref": "[35]Yu, D., Deng, L., and Seide, F. The deep tensor neural network with applications to large vocabulary speech recognition. IEEE Trans. Audio, Speech and Language Proc., 21(2):388 \u2013396, 2013."}]}, {"author": ["Zhaocheng Zhu", "Shizhen Xu", "Meng Qu", "Jian Tang"], "title": "GraphVite:A High-Performance CPU-GPU Hybrid System for Node Embedding", "journal": "International World Wide Web Conference Committee", "year": 2019, "DOI": "10.1145/3308558.3313508", "month": 3, "citations(google scholar)": 0, "abstract": "Learning continuous representations of nodes is attracting growing interest in both academia and industry recently, due to their simplicity and effectiveness in a variety of applications. Most of existing node embedding algorithms and systems are capable of processing networks with hundreds of thousands or a few millions of nodes. However, how to scale them to networks that have tens of millions or even hundreds of millions of nodes remains a challenging problem. In this paper, we propose GraphVite, a high-performance CPU-GPU hybrid system for training node embeddings, by co-optimizing the algorithm and the system. On the CPU end, augmented edge samples are parallelly generated by random walks in an online fashion on the network, and serve as the training data. On the GPU end, a novel parallel negative sampling is proposed to leverage multiple GPUs to train node embeddings simultaneously, without much data transfer and synchronization. Moreover, an efficient collaboration strategy is proposed to further reduce the synchronization cost between CPUs and GPUs. Experiments on multiple real-world networks show that GraphVite is super efficient. It takes only about one minute for a network with 1 million nodes and 5 million edges on a single machine with 4 GPUs, and takes around 20 hours for a network with 66 million nodes and 1.8 billion edges. Compared to the current fastest system, GraphVite is about 50 times faster without any sacrifice on performance.", "keywords": ["Unsupervised node embedding", "parallel processing", "scalability", "graphics processing unit"], "reference_count": 40, "ccfClass": "A", "important": true, "references": [{"ref": "[1] Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.2016. Tensorflow: a system for large-scale machine learning.. In OSDI, Vol. 16.265\u2013283."}, {"ref": "[2] Natural Language Processing Lab at Tsinghua University. 2017. OpenNE: An open source toolkit for Network Embedding. https://github.com/thunlp/OpenNE."}, {"ref": "[3] Prasad G Bhavana and Vineet C Nair. 2019. BMF: Block matrix approach to factorization of large scale data. arXiv preprint arXiv:1901.00444 (2019)."}, {"ref": "[4] Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM,891\u2013900."}, {"ref": "[5] Dan C Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and J\u00fcrgen Schmidhuber. 2011. Flexible, high performance convolutional neural networks for image classification. In IJCAI Proceedings-International Joint Conference on Artificial Intelligence, Vol. 22. Barcelona, Spain, 1237."}, {"ref": "[6] Rainer Gemulla, Erik Nijkamp, Peter J Haas, and Yannis Sismanis. 2011. Large-scale matrix factorization with distributed stochastic gradient descent. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 69\u201377."}, {"ref": "[7] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George EDahl. 2017. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212 (2017)."}, {"ref": "[8] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 855\u2013864."}, {"ref": "[9] Saurabh Gupta and Vineet Khare. 2017. BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs. In Proceedings of the Machine Learning on HPC Environments. ACM, 6."}, {"ref": "[10] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Representation learning on graphs: Methods and applications. arXiv preprint arXiv:1709.05584 (2017)."}, {"ref": "[11] Shihao Ji, Nadathur Satish, Sheng Li, and Pradeep Dubey. 2016. Parallelizing word2vec in multi-core and many-core architectures. arXiv preprint arXiv:1611.06172 (2016)."}, {"ref": "[12] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016)."}, {"ref": "[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems. 1097\u20131105."}, {"ref": "[14] David Liben-Nowell and Jon Kleinberg. 2007. The link-prediction problem for social networks. Journal of the American society for information science and technology 58, 7 (2007), 1019\u20131031."}, {"ref": "[15] Long-Ji Lin. 1993. Reinforcement learning for robots using neural networks. Technical Report. Carnegie-Mellon Univ Pittsburgh PA School of Computer Science."}, {"ref": "[16] Robert Meusel, Sebastiano Vigna, Oliver Lehmberg, and Christian Bizer. 2015. The graph structure in the web: Analyzed on different aggregation levels. The Journal of Web Science 1, 1 (2015), 33\u201347."}, {"ref": "[17] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaev, Ganesh Venkatesh, et al. 2017. Mixed precision training. arXiv preprint arXiv:1710.03740 (2017)."}, {"ref": "[18] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems. 3111\u20133119."}, {"ref": "[19] Alan Mislove, Massimiliano Marcon, Krishna P Gummadi, Peter Druschel, and Bobby Bhattacharjee. 2007. Measurement and analysis of online social networks. In Proceedings of the 7th ACM SIGCOMM conference on Internet measurement. ACM, 29\u201342."}, {"ref": "[20] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 (2013)."}, {"ref": "[21] David Newman, Padhraic Smyth, Max Welling, and Arthur U Asuncion. 2008. Distributed inference for latent dirichlet allocation. In Advances in neural information processing systems. 1081\u20131088."}, {"ref": "[22] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.2017. Automatic differentiation in pytorch. (2017)."}, {"ref": "[23] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 701\u2013710."}, {"ref": "[24] Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. 2018. Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 459\u2013467."}, {"ref": "[25] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in neural information processing systems. 693\u2013701."}, {"ref": "[26] Radim Rehurek and Petr Sojka. 2010. Software framework for topic modelling with large corpora. In In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. Citeseer."}, {"ref": "[27] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29, 3 (2008), 93\u201393."}, {"ref": "[28] Trevor M Simonton and Gita Alaghband. 2017. Efficient and accurate Word2Vec implementations in GPU and shared-memory multicore architectures. In High Performance Extreme Computing Conference (HPEC), 2017 IEEE. IEEE, 1\u20137."}, {"ref": "[29] Stergios Stergiou, Zygimantas Straznickas, Rolina Wu, and Kostas Tsioutsiouliklis. 2017. Distributed Negative Sampling for Word Embeddings.. In AAAI. 2569\u20132575."}, {"ref": "[30] Damian Szklarczyk, John H Morris, Helen Cook, Michael Kuhn, Stefan Wyder, Milan Simonovic, Alberto Santos, Nadezhda T Doncheva, Alexander Roth, Peer Bork, et al. 2016. The STRING database in 2017: quality-controlled protein\u2013 protein association networks, made broadly accessible. Nucleic acids research (2016), gkw937."}, {"ref": "[31] Jian Tang, Jingzhou Liu, Ming Zhang, and Qiaozhu Mei. 2016. Visualizing large-scale and high-dimensional data. In Proceedings of the 25th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 287\u2013297."}, {"ref": "[32] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 1067\u20131077."}, {"ref": "[33] Anton Tsitsulin, Davide Mottin, Panagiotis Karras, and Emmanuel M\u00fcller. 2018. VERSE: Versatile Graph Embeddings from Similarity Measures. In Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 539\u2013548."}, {"ref": "[34] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903 1, 2 (2017)."}, {"ref": "[35] Petar Veli\u010dkovi\u0107, William Fedus, William L Hamilton, Pietro Li\u00f2, Yoshua Bengio, and R Devon Hjelm. 2018. Deep graph infomax. arXiv preprint arXiv:1809.10341 (2018)."}, {"ref": "[36] Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep network embedding. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 1225\u20131234."}, {"ref": "[37] Jaewon Yang and Jure Leskovec. 2015. Defining and evaluating network communities based on ground-truth. Knowledge and Information Systems 42, 1 (2015), 181\u2013213."}, {"ref": "[38] Hyokun Yun, Hsiang-Fu Yu, Cho-Jui Hsieh, SVN Vishwanathan, and Inderjit Dhillon. 2014. NOMAD: Non-locking, stOchastic Multi-machine algorithm for Asynchronous and Decentralized matrix completion. Proceedings of the VLDB Endowment 7, 11 (2014), 975\u2013986."}, {"ref": "[39] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. 2016. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160 (2016)."}, {"ref": "[40] Yong Zhuang, Wei-Sheng Chin, Yu-Chin Juan, and Chih-Jen Lin. 2013. A fast parallel SGD for matrix factorization in shared memory systems. In Proceedings of the 7th ACM conference on Recommender systems. ACM, 249\u2013256."}]}, {"author": ["Jian Tang", "Jingzhou Liu", "Ming Zhang", "Qiaozhu Mei"], "title": "Visualizing Large-scale and High-dimensional Data", "journal": "Proceedings of the 25th international conference on world wide web. International World Wide Web Conferences Steering Committee", "year": 2016, "DOI": "10.1145/2872427.2883041", "month": 4, "citations(google scholar)": 152, "abstract": "We study the problem of visualizing large-scale and highdimensional data in a low-dimensional (typically 2D or 3D) space. Much success has been reported recently by techniques that \ufb01rst compute a similarity structure of the data points and then project them into a low-dimensional space with the structure preserved. These two steps su\ufb00er from considerable computational costs, preventing the state-ofthe-art methods such as the t-SNE from scaling to largescale and high-dimensional data (e.g., millions of data points and hundreds of dimensions). We propose the LargeVis, a technique that \ufb01rst constructs an accurately approximated K-nearest neighbor graph from the data and then layouts the graph in the low-dimensional space. Comparing to tSNE, LargeVis signi\ufb01cantly reduces the computational cost of the graph construction step and employs a principled probabilistic model for the visualization step, the objective of which can be e\ufb00ectively optimized through asynchronous stochastic gradient descent with a linear time complexity. The whole procedure thus easily scales to millions of highdimensional data points. Experimental results on real-world data sets demonstrate that the LargeVis outperforms the state-of-the-art methods in both e\ufb03ciency and e\ufb00ectiveness. The hyper-parameters of LargeVis are also much more stable over di\ufb00erent data sets.", "keywords": ["Visualization", "big data", "high-dimensional data"], "reference_count": 28, "ccfClass": "A", "important": true, "references": [{"ref": "[1] M. Bastian, S. Heymann, M. Jacomy, et al. Gephi: an open source software for exploring and manipulating networks. ICWSM, 8:361{362, 2009."}, {"ref": "[2] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585{591, 2001."}, {"ref": "[3] J. L. Bentley. Multidimensional binary search trees used for associative searching. Communications of the ACM, 18(9):509{517, 1975."}, {"ref": "[4] A. Beygelzimer, S. Kakade, and J. Langford. Cover trees for nearest neighbor. In Proceedings of the 23rd international conference on Machine learning, pages 97{104. ACM, 2006."}, {"ref": "[5] S. K. Card, J. D. Mackinlay, and B. Shneiderman. Readings in information visualization: using vision to think. Morgan Kaufmann, 1999."}, {"ref": "[6] M. S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 380{388. ACM, 2002."}, {"ref": "[7] S. Dasgupta and Y. Freund. Random projection trees and low dimensional manifolds. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 537{546. ACM, 2008."}, {"ref": "[8] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pages 253{262. ACM, 2004."}, {"ref": "[9] W. Dong, C. Moses, and K. Li. Efficient k-nearest neighbor graph construction for generic similarity measures. In Proceedings of the 20th international conference on World wide web, pages 577{586. ACM, 2011."}, {"ref": "[10] J. H. Friedman, J. L. Bentley, and R. A. Finkel. An algorithm for Fnding best matches in logarithmic expected time. ACM Transactions on Mathematical Software (TOMS), 3(3):209{226, 1977."}, {"ref": "[11] T. M. Fruchterman and E. M. Reingold. Graph drawing by force-directed placement. Softw., Pract. Exper., 21(11):1129{1164, 1991."}, {"ref": "[12] A. Gionis, P. Indyk, R. Motwani, et al. Similarity search in high dimensions via hashing. In VLDB, volume 99, pages 518{529, 1999."}, {"ref": "[13] G. E. Hinton and S. T. Roweis. Stochastic neighbor embedding. In Advances in neural information processing systems, pages 833{840, 2002."}, {"ref": "[14] M. Jacomy, S. Heymann, T. Venturini, and M. Bastian. Forceatlas2, a continuous graph layout algorithm for handy network visualization. Medialab center of research, 560, 2011."}, {"ref": "[15] I. Jollie. Principal component analysis. Wiley Online Library, 2002."}, {"ref": "[16] D. Keim et al. Information visualization and visual data mining. Visualization and Computer Graphics, IEEE Transactions on, 8(1):1{8, 2002."}, {"ref": "[17] S. Martin, W. M. Brown, R. Klavans, and K. W. Boyack. Openord: an open-source toolbox for large graph layout. In IS&T/SPIE Electronic Imaging, pages 786806{786806. International Society for Optics and Photonics, 2011."}, {"ref": "[18] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111{3119, 2013."}, {"ref": "[19] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 693{701, 2011."}, {"ref": "[20] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323{2326, 2000."}, {"ref": "[21] C. Silpa-Anan and R. Hartley. Optimised kd-trees for fast image descriptor matching. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1{8. IEEE, 2008."}, {"ref": "[22] R. Tamassia. Handbook of graph drawing and visualization. CRC press, 2013."}, {"ref": "[23] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067{1077. International World Wide Web Conferences Steering Committee, 2015."}, {"ref": "[24] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319{2323, 2000."}, {"ref": "[25] W. S. Torgerson. Multidimensional scaling: I. theory and method. Psychometrika, 17(4):401{419, 1952."}, {"ref": "[26] L. Van Der Maaten. Accelerating t-sne using tree-based algorithms. The Journal of Machine Learning Research, 15(1):3221{3245, 2014."}, {"ref": "[27] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(2579-2605):85, 2008."}, {"ref": "[28] P. N. Yianilos. Data structures and algorithms for nearest neighbor search in general metric spaces. In SODA, volume 93, pages 311{321, 1993."}]}, {"author": ["Jian Tang", "Meng Qu2", "Mingzhe Wang", "Ming Zhang", "Jun Yan", "Qiaozhu Mei"], "title": "LINE: Large-scale Information Network Embedding", "journal": "Proceedings of the 24th international conference on world wide web. International World Wide Web Conferences Steering Committee", "year": 2015, "DOI": "10.1145/2736277.2741093", "month": 3, "citations(google scholar)": 1524, "abstract": "This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classi\ufb01cation, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the\u201cLINE,\u201dwhich is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the e\ufb00ectiveness and the e\ufb03ciency of the inference. Empirical experiments prove the e\ufb00ectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very e\ufb03cient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online.", "keywords": ["information network embedding; scalability; feature learning; dimension reduction"], "reference_count": 23, "ccfClass": "A", "important": true, "references": [{"ref": "[1] A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. In Proceedings of the 22nd international conference on World Wide Web, pages 37{48. International World Wide Web Conferences Steering Committee, 2013."}, {"ref": "[2] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585{591, 2001."}, {"ref": "[3] S. Bhagat, G. Cormode, and S. Muthukrishnan. Node classi cation in social networks. In Social Network Data Analytics, pages 115{148. Springer, 2011."}, {"ref": "[4] T. F. Cox and M. A. Cox. Multidimensional scaling. CRC Press, 2000."}, {"ref": "[5] J. R. Firth. A synopsis of linguistic theory, 1930{1955. In J. R. Firth (Ed.), Studies in linguistic analysis, pages 1{32."}, {"ref": "[6] M. S. Granovetter. The strength of weak ties. American journal of sociology, pages 1360{1380, 1973."}, {"ref": "[7] Q. Le and T. Mikolov. Distributed representations of sentences and documents. In Proceedings of The 31st International Conference on Machine Learning, pages 1188{1196, 2014."}, {"ref": "[8] O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In Advances in Neural Information Processing Systems, pages 2177{2185, 2014."}, {"ref": "[9] A. Q. Li, A. Ahmed, S. Ravi, and A. J. Smola. Reducing the sampling complexity of topic models. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 891{900. ACM, 2014."}, {"ref": "[10] D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019{1031, 2007."}, {"ref": "[11] C. D. Manning, P. Raghavan, and H. Sch\u007f utze. Introduction to information retrieval, volume 1. Cambridge university press Cambridge, 2008."}, {"ref": "[12] T. Mikolov, K. Chen, G. Corrado, and J. Dean. ESOcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013."}, {"ref": "[13] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111{3119, 2013."}, {"ref": "[14] S. A. Myers, A. Sharma, P. Gupta, and J. Lin. Information network or social network?: the structure of the twitter follow graph. In Proceedings of the companion publication of the 23rd international conference on World wide web companion, pages 493{498. International World Wide Web Conferences Steering Committee, 2014."}, {"ref": "[15] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking: Bringing order to the web. 1999."}, {"ref": "[16] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701{710. ACM, 2014."}, {"ref": "[17] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 693{701, 2011."}, {"ref": "[18] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323{2326, 2000."}, {"ref": "[19] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su. Arnetminer: extraction and mining of academic social networks. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 990{998. ACM, 2008."}, {"ref": "[20] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319{2323, 2000."}, {"ref": "[21] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(2579-2605):85, 2008."}, {"ref": "[22] S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin. Graph embedding and extensions: a general framework for dimensionality reduction. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(1):40{51, 2007."}, {"ref": "[23] X. Yu, X. Ren, Y. Sun, Q. Gu, B. Sturt, U. Khandelwal, B. Norick, and J. Han. Personalized entity recommendation: A heterogeneous information network approach. In Proceedings of the 7th ACM international conference on Web search and data mining, pages 283{292. ACM, 2014."}]}, {"author": ["Aditya Grover", "Jure Leskovec"], "title": "node2vec: Scalable Feature Learning for Networks", "journal": "Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining", "year": 2016, "DOI": "10.1145/2939672.2939754", "month": 8, "citations(google scholar)": 2000, "abstract": "Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader \ufb01eld of representation learning has led to signi\ufb01cant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec,an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec,we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We de\ufb01ne a \ufb02exible notion of a node\u2019s network neighborhood and design a biased randomwalk procedure, which ef\ufb01ciently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods,and we argue that the added \ufb02exibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the ef\ufb01cacy of node2vec over existing state-ofthe-art techniques on multi-label classi\ufb01cation and link prediction in several real-world networks from diverse domains. Taken together,our work represents a new way for ef\ufb01ciently learning stateof-the-art task-independent representations in complex networks.", "keywords": ["Information networks", "Feature learning", "Node embeddings", "Graph representations"], "reference_count": 38, "ccfClass": "A", "important": true, "references": [{"ref": "[1] L. A. Adamic and E. Adar. Friends and neighbors on the web.Social networks, 25(3):211\u2013230, 2003."}, {"ref": "[2] L. Backstrom and J. Leskovec. Supervised random walks: predicting and recommending links in social networks. In WSDM, 2011."}, {"ref": "[3] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. InNIPS, 2001."}, {"ref": "[4] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives.IEEE TPAMI, 35(8):1798\u20131828, 2013."}, {"ref": "[5] B.-J. Breitkreutz, C. Stark, T. Reguly, L. Boucher, A. Breitkreutz,M. Livstone, R. Oughtred, D. H. Lackner, J. B\u00e4hler, V. Wood, et al. The BioGRID interaction database. Nucleic acids research, 36:D637\u2013D640, 2008."}, {"ref": "[6] S. Cao, W. Lu,and Q. Xu. GraRep: Learning Graph Representations with global structural information. InCIKM, 2015."}, {"ref": "[7] S. Fortunato. Community detection in graphs.Physics Reports, 486(3-5):75 \u2013 174, 2010."}, {"ref": "[8] B. Gallagher and T. Eliassi-Rad. Leveraging label-independent features for classification in sparsely labeled networks: An empirical study. InLecture Notes in Computer Science: Advances in Social Network Mining and Analysis. Springer, 2009."}, {"ref": "[9] Z. S. Harris. Word.Distributional Structure, 10(23):146\u2013162, 1954. [10] K. Henderson, B. Gallagher, T. Eliassi-Rad, H. Tong, S. Basu, L. Akoglu, D. Koutra, C. Faloutsos, and L. Li. RolX: structural role extraction & mining in large graphs. In KDD, 2012."}, {"ref": "[11] K. Henderson, B. Gallagher, L. Li, L. Akoglu, T. Eliassi-Rad, H. Tong, and C. Faloutsos. It\u2019s who you know: graph mining using recursive structural features. InKDD, 2011."}, {"ref": "[12] P. D. Hoff, A. E. Raftery, and M. S. Handcock. Latent space approaches to social network analysis.J. of the American Statistical Association, 2002."}, {"ref": "[13] D. E. Knuth.The Stanford GraphBase: a platform for combinatorial computing, volume 37. Addison-Wesley Reading, 1993."}, {"ref": "[14] J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data, June 2014."}, {"ref": "[15] K. Li, J. Gao, S. Guo, N. Du, X. Li, and A. Zhang.LRBM: A restricted boltzmann machine based approach for representation learning on linked data. InICDM, 2014."}, {"ref": "[16] X. Li, N. Du, H. Li, K. Li, J. Gao, and A. Zhang. A deep learning approach to link prediction in dynamic networks. In ICDM, 2014."}, {"ref": "[17] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. InICLR, 2016."}, {"ref": "[18] D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks.J. of the American society for information science and technology, 58(7):1019\u20131031, 2007."}, {"ref": "[19] A. Liberzon, A. Subramanian, R. Pinchback, H. Thorvaldsd\u00f3ttir, P. Tamayo, and J. P. Mesirov. Molecular signatures database (MSigDB) 3.0.Bioinformatics, 27(12):1739\u20131740, 2011."}, {"ref": "[20] M. Mahoney. Large text compression benchmark. www.mattmahoney.net/dc/textdata, 2011."}, {"ref": "[21] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. InICLR, 2013."}, {"ref": "[22] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. InNIPS, 2013."}, {"ref": "[23] J. Pennington, R. Socher, and C. D. Manning. GloVe: Global vectors for word representation. InEMNLP, 2014."}, {"ref": "[24] B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online learning of social representations. InKDD, 2014."}, {"ref": "[25] P. Radivojac, W. T. Clark, T. R. Oron, A. M. Schnoes, T. Wittkop, A. Sokolov, K. Graim, C. Funk, Verspoor, et al. A large-scale evaluation of computational protein function prediction.Nature methods, 10(3):221\u2013227, 2013."}, {"ref": "[26] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. InNIPS, 2011."}, {"ref": "[27] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding.Science, 290(5500):2323\u20132326, 2000."}, {"ref": "[28] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: Large-scale Information Network Embedding. In WWW, 2015."}, {"ref": "[29] L. Tang and H. Liu. Leveraging social media networks for classification.Data Mining and Knowledge Discovery, 23(3):447\u2013478, 2011."}, {"ref": "[30] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319\u20132323, 2000."}, {"ref": "[31] F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu. Learning deep representations for graph clustering. InAAAI, 2014."}, {"ref": "[32] K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. Feature-rich part-of-speech tagging with a cyclic dependency network. InNAACL, 2003."}, {"ref": "[33] G. Tsoumakas and I. Katakis. Multi-label classification: An overview.Dept. of Informatics, Aristotle University of Thessaloniki, Greece, 2006."}, {"ref": "[34] A. Vazquez, A. Flammini, A. Maritan, and A. Vespignani. Global protein function prediction from protein-protein interaction networks.Nature biotechnology, 21(6):697\u2013700, 2003."}, {"ref": "[35] S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin. Graph embedding and extensions: a general framework for dimensionality reduction.IEEE TPAMI, 29(1):40\u201351, 2007."}, {"ref": "[36] J. Yang and J. Leskovec. Overlapping communities explain core-periphery organization of networks.Proceedings of the IEEE, 102(12):1892\u20131902, 2014."}, {"ref": "[37] S.-H. Yang, B. Long, A. Smola, N. Sadagopan, Z. Zheng, and H. Zha. Like like alike: joint friendship and interest propagation in social networks. InWWW, 2011."}, {"ref": "[38] R. Zafarani and H. Liu. Social computing data repository at ASU, 2009. [39] S. Zhai and Z. Zhang. Dropout training of matrix factorization and autoencoder for link predictionin sparse graphs. InSDM, 2015."}]}, {"author": ["Zhiqing Sun", "Zhi-Hong Deng", "Jian-Yun Nie", "Jian Tang"], "title": "ROTATE: KNOWLEDGE GRAPH EMBEDDING BY RELATIONAL ROTATION IN COMPLEX SPACE", "journal": "ICLR 2019", "year": 2019, "DOI": "0", "month": 2, "citations(google scholar)": 20, "abstract": "We study the problem of learning representations of entities and relations in knowledgegraphsforpredictingmissinglinks. Thesuccessofsuchataskheavily relies on the ability of modeling and inferring the patterns of (or between) the relations. Inthispaper,wepresentanewapproachforknowledgegraphembedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry,inversion,andcomposition. Speci\ufb01cally,theRotatE modelde\ufb01neseachrelationasarotationfromthesourceentitytothetargetentity inthecomplexvectorspace. Inaddition,weproposeanovelself-adversarialnegativesamplingtechniqueforef\ufb01cientlyandeffectivelytrainingtheRotatEmodel. Experimentalresultsonmultiplebenchmarkknowledgegraphsshowthattheproposed RotatE model is not only scalable, but also able to infer and model various relation patterns and signi\ufb01cantly outperform existing state-of-the-art models for link prediction.", "keywords": ["keywords"], "reference_count": 3, "ccfClass": "", "important": true, "references": [{"ref": "[1]Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pp. 1247\u20131250. AcM, 2008."}, {"ref": "[2]Antoine Bordes, Jason Weston, Ronan Collobert, Yoshua Bengio, et al. Learning structured embeddings of knowledge bases. In AAAI, volume 6, pp. 6, 2011."}, {"ref": "[3]Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In Advances in neural information processing systems, pp. 2787\u20132795, 2013."}, {"ref": "[4]Guillaume Bouchard, Sameer Singh, and Theo Trouillon. On approximate reasoning capabilities of low-rank vector spaces. AAAI Spring Syposium on Knowledge Representation and Reasoning (KRR): Integrating Symbolic and Neural Approaches, 2015."}, {"ref": "[5]Liwei Cai andWilliam YangWang. Kbgan: Adversarial learning for knowledge graph embeddings. arXiv preprint arXiv:1711.04071, 2017. Rajarshi Das, Arvind Neelakantan, David Belanger, and Andrew McCallum. Chains of reasoning over entities, relations, and text using recurrent neural networks. arXiv preprint arXiv:1607.01426, 2016."}, {"ref": "[6]Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. arXiv preprint arXiv:1707.01476, 2017."}, {"ref": "[7]Takuma Ebisu and Ryutaro Ichise. Toruse: Knowledge graph embedding on a lie group. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, pp. 1819\u20131826. AAAI Press, 2018."}, {"ref": "[8]Kelvin Guu, John Miller, and Percy Liang. Traversing knowledge graphs in vector space. arXiv preprint arXiv:1506.01094, 2015."}, {"ref": "[9]Yanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He, Zhanyi Liu, Hua Wu, and Jun Zhao. An endto-end model for question answering over knowledge base with cross-attention combining global knowledge. In Proceedings of the 55th Annual Meeting of the Association for Computational"}, {"ref": "[10]Linguistics (Volume 1: Long Papers), volume 1, pp. 221\u2013231, 2017."}, {"ref": "[11]Katsuhiko Hayashi and Masashi Shimbo. On the equivalence of holographic and complex embeddings for link prediction. arXiv preprint arXiv:1702.05563, 2017."}, {"ref": "[12]Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao. Learning to represent knowledge graphs with gaussian embedding. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pp. 623\u2013632. ACM, 2015."}, {"ref": "[13]Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst. Knowledge base completion: Baselines strikeback. arXiv preprint arXiv:1705.10744, 2017."}, {"ref": "[14]Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014."}, {"ref": "[15]Ni Lao, Tom Mitchell, and William W Cohen. Random walk inference and learning in a large scale knowledge base. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 529\u2013539. Association for Computational Linguistics, 2011."}, {"ref": "[16]Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Siwei Rao, and Song Liu. Modeling relation paths for representation learning of knowledge bases. arXiv preprint arXiv:1506.00379, 2015a."}, {"ref": "[17]Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation embeddings for knowledge graph completion. In AAAI, volume 15, pp. 2181\u20132187, 2015b."}, {"ref": "[18]Farzaneh Mahdisoltani, Joanna Biega, and Fabian M Suchanek. Yago3: A knowledge base from multilingual wikipedias. In CIDR, 2013."}, {"ref": "[19]Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111\u20133119, 2013."}, {"ref": "[20]George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39\u201341, 1995."}, {"ref": "[21]Arvind Neelakantan, Benjamin Roth, and Andrew McCallum. Compositional vector space models for knowledge base completion. arXiv preprint arXiv:1504.06662, 2015."}, {"ref": "[22]Dai Quoc Nguyen, Tu Dinh Nguyen, Dat Quoc Nguyen, and Dinh Phung. A novel embedding model for knowledge base completion based on convolutional neural network. arXiv preprint arXiv:1712.02121, 2017."}, {"ref": "[23]Dat Quoc Nguyen, Kairit Sirts, Lizhen Qu, and Mark Johnson. Stranse: a novel embedding model of entities and relationships in knowledge bases. arXiv preprint arXiv:1606.08140, 2016."}, {"ref": "[24]Maximilian Nickel, Lorenzo Rosasco, Tomaso A Poggio, et al. Holographic embeddings of knowledge graphs. In AAAI, volume 2, pp. 3\u20132, 2016."}, {"ref": "[25]Tim Rockt\u00a8aschel and Sebastian Riedel. End-to-end differentiable proving. In Advances in Neural Information Processing Systems, pp. 3788\u20133800, 2017."}, {"ref": "[26]Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. Yago: a core of semantic knowledge. In Proceedings of the 16th international conference on World Wide Web, pp. 697\u2013706. ACM, 2007."}, {"ref": "[27]Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, pp. 57\u201366, 2015."}, {"ref": "[28]Th\u00b4eo Trouillon, Johannes Welbl, Sebastian Riedel, \u00b4 Eric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In International Conference on Machine Learning, pp. 2071\u20132080, 2016."}, {"ref": "[29]Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by translating on hyperplanes. In AAAI, volume 14, pp. 1112\u20131119, 2014."}, {"ref": "[30]Chenyan Xiong, Russell Power, and Jamie Callan. Explicit semantic ranking for academic search via knowledge graph embedding. In Proceedings of the 26th international conference on world wide web, pp. 1271\u20131279. InternationalWorldWideWeb Conferences Steering Committee, 2017."}, {"ref": "[31]Bishan Yang and Tom Mitchell. Leveraging knowledge bases in lstms for improving machine reading. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 1436\u20131446, 2017."}, {"ref": "[32]Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. arXiv preprint arXiv:1412.6575, 2014."}, {"ref": "[33]Fan Yang, Zhilin Yang, andWilliamWCohen. Differentiable learning of logical rules for knowledge base completion. CoRR, abs/1702.08367, 2017."}, {"ref": "[34]Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, andWei-Ying Ma. Collaborative knowledge base embedding for recommender systems. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 353\u2013362. ACM, 2016."}]}, {"author": ["Seyed Mehran Kazemi", "David Poole"], "title": "Simple embedding for link prediction in knowledge graphs", "journal": "NIPS 2018", "year": 2018, "DOI": "0", "month": 10, "citations(google scholar)": 26, "abstract": "Knowledge graphs contain knowledge about the world and provide a structured representation of this knowledge. Current knowledge graphs contain only a small subset of what is true in the world. Link prediction approaches aim at predicting newlinksforaknowledgegraphgiventheexistinglinksamongtheentities. Tensor factorization approaches have proved promising for such link prediction problems. Proposedin1927,CanonicalPolyadic(CP)decompositionisamongthe\ufb01rsttensor factorization approaches. CP generally performs poorly for link prediction as it learns two independent embedding vectors for each entity, whereas they are really tied. We present a simple enhancement of CP (which we call SimplE) to allow the two embeddings of each entity to be learned dependently. The complexity of SimplE grows linearly with the size of embeddings. The embeddings learned through SimplE are interpretable, and certain types of background knowledge can be incorporated into these embeddings through weight tying. We prove SimplE is fully expressive and derive a bound on the size of its embeddings for full expressivity. We show empirically that, despite its simplicity, SimplE outperforms several state-of-the-art tensor factorization techniques. SimplE\u2019s code is available on GitHub at https://github.com/Mehran-k/SimplE.", "keywords": ["keywords"], "reference_count": 47, "ccfClass": "A", "important": true, "references": [{"ref": "[1] Mart\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg SCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016."}, {"ref": "[2] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In ACM SIGMOD, pages 1247\u20131250. AcM, 2008."}, {"ref": "[3] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Irreflexive and hierarchical relations as translations. arXiv preprint arXiv:1304.7158, 2013."}, {"ref": "[4] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In NIPS, pages 2787\u20132795, 2013."}, {"ref": "[5] George Cybenko. Approximations by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2:183\u2013192, 1989."}, {"ref": "[6] Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy, Alex Smola, and Andrew McCallum. Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning. NIPS Workshop on AKBC, 2017."}, {"ref": "[7] Luc De Raedt, Kristian Kersting, Sriraam Natarajan, and David Poole. Statistical relational artificial intelligence: Logic, probability, and computation. Synthesis Lectures on Artificial Intelligence and Machine Learning, 10(2):1\u2013189, 2016."}, {"ref": "[8] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. In AAAI, 2018."}, {"ref": "[9] Boyang Ding, Quan Wang, Bin Wang, and Li Guo. Improving knowledge graph embedding using simple constraints. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 2018."}, {"ref": "[10] Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, andWei Zhang. Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In ACM SIGKDD, pages 601\u2013610. ACM, 2014."}, {"ref": "[11] Jun Feng, Minlie Huang, Mingdong Wang, Mantong Zhou, Yu Hao, and Xiaoyan Zhu. Knowledge graph embedding by flexible translation. In KR, pages 557\u2013560, 2016."}, {"ref": "[12] Lise Getoor and Ben Taskar. Introduction to statistical relational learning. MIT press, 2007."}, {"ref": "[13] Shu Guo, Quan Wang, Lihong Wang, Bin Wang, and Li Guo. Jointly embedding knowledge graphs and logical rules. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 192\u2013202, 2016."}, {"ref": "[14] Katsuhiko Hayashi and Masashi Shimbo. On the equivalence of holographic and complex embeddings for link prediction. arXiv preprint arXiv:1702.05563, 2017."}, {"ref": "[15] Frank L Hitchcock. The expression of a tensor or a polyadic as a sum of products. Studies in Applied Mathematics, 6(1-4):164\u2013189, 1927."}, {"ref": "[16] Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):251\u2013257, 1991."}, {"ref": "[17] Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. Knowledge graph embedding via dynamic mapping matrix. In ACL (1), pages 687\u2013696, 2015."}, {"ref": "[18] Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst. Knowledge base completion: Baselines strike back. arXiv preprint arXiv:1705.10744, 2017."}, {"ref": "[19] Seyed Mehran Kazemi and David Poole. Relnn: A deep neural model for relational learning. In AAAI, 2018."}, {"ref": "[20] Ni Lao and William W Cohen. Relational retrieval using a combination of path-constrained random walks. Machine learning, 81(1):53\u201367, 2010."}, {"ref": "[21] Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Siwei Rao, and Song Liu. Modeling relation paths for representation learning of knowledge bases. EMNLP, 2015."}, {"ref": "[22] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation embeddings for knowledge graph completion. In AAAI, pages 2181\u20132187, 2015."}, {"ref": "[23] Hanxiao Liu, Yuexin Wu, and Yiming Yang. Analogical inference for multi-relational embeddings. AAAI, 2018."}, {"ref": "[24] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39\u201341, 1995."}, {"ref": "[25] Pasquale Minervini, Luca Costabello, Emir Mu\u00f1oz, V\u00edt Nov\u00e1\u02c7cek, and Pierre-Yves Vandenbussche. Regularizing knowledge graph embeddings via equivalence and inversion axioms. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 668\u2013683. Springer, 2017."}, {"ref": "[26] Dat Quoc Nguyen, Kairit Sirts, Lizhen Qu, and Mark Johnson. Stranse: a novel embedding model of entities and relationships in knowledge bases. In NAACL-HLT, 2016."}, {"ref": "[27] Dat Quoc Nguyen. An overview of embedding models of entities and relationships for knowledge base completion. arXiv preprint arXiv:1703.08098, 2017."}, {"ref": "[28] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data. In ICML, volume 11, pages 809\u2013816, 2011."}, {"ref": "[29] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. Factorizing yago: scalable machine learning for linked data. In World Wide Web, pages 271\u2013280. ACM, 2012."}, {"ref": "[30] Maximilian Nickel, Xueyan Jiang, and Volker Tresp. Reducing the rank in relational factorization models by including observable patterns. In NIPS, pages 1179\u20131187, 2014."}, {"ref": "[31] Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11\u201333, 2016."}, {"ref": "[32] Maximilian Nickel, Lorenzo Rosasco, Tomaso A Poggio, et al. Holographic embeddings of knowledge graphs. In AAAI, pages 1955\u20131961, 2016."}, {"ref": "[33] Tim Rockt\u00e4schel and Sebastian Riedel. End-to-end differentiable proving. In NIPS, pages 3791\u20133803, 2017."}, {"ref": "[34] Tim Rockt\u00e4schel, Matko Bo\u0161njak, Sameer Singh, and Sebastian Riedel. Low-dimensional embeddings of logic. In Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 45\u201349, 2014."}, {"ref": "[35] Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Tim Lillicrap. A simple neural network module for relational reasoning. In NIPS, 2017."}, {"ref": "[36] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593\u2013607. Springer, 2018."}, {"ref": "[37] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning with neural tensor networks for knowledge base completion. In NIPS, 2013."}, {"ref": "[38] Th\u00e9o Trouillon and Maximilian Nickel. Complex and holographic embeddings of knowledge graphs: a comparison. arXiv preprint arXiv:1707.01475, 2017."}, {"ref": "[39] Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In ICML, pages 2071\u20132080, 2016."}, {"ref": "[40] Th\u00e9o Trouillon, Christopher R Dance, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, and Guillaume Bouchard. Knowledge graph completion via complex tensor factorization. arXiv preprint arXiv:1702.06879, 2017."}, {"ref": "[41] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by translating on hyperplanes. In AAAI, pages 1112\u20131119, 2014."}, {"ref": "[42] Quan Wang, Bin Wang, Li Guo, et al. Knowledge base completion using embeddings and rules. In International Joint Conference on Artificial Intelligence, pages 1859\u20131866, 2015."}, {"ref": "[43] Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering, 29(12):2724\u20132743, 2017."}, {"ref": "[44] Yanjie Wang, Rainer Gemulla, and Hui Li. On multi-relational link prediction with bilinear models. AAAI, 2018."}, {"ref": "[45] Zhuoyu Wei, Jun Zhao, Kang Liu, Zhenyu Qi, Zhengya Sun, and Guanhua Tian. Large-scale knowledge base completion: Inferring via grounding network sampling over selected instances. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 1331\u20131340. ACM, 2015."}, {"ref": "[46] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. ICLR, 2015."}, {"ref": "[47] Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, and Tat-Seng Chua. Visual translation embedding network for visual relation detection. In CVPR, volume 1, page 5, 2017."}]}, {"author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Dur\u00b4an", "Jason Weston", "Oksana Yakhnenko"], "title": "Translating Embeddings for Modeling Multi-relational Data", "journal": "NIPS 2013", "year": 2013, "DOI": "0", "month": 12, "citations(google scholar)": 1478, "abstract": "We consider the problem of embedding entities and relationships of multirelational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.", "keywords": ["keywords"], "reference_count": 17, "ccfClass": "A", "important": true, "references": [{"ref": "[1] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, 2008."}, {"ref": "[2] A. Bordes, X. Glorot, J. Weston, and Y. Bengio. A semantic matching energy function for learning with multi-relational data. Machine Learning, 2013."}, {"ref": "[3] A. Bordes, J. Weston, R. Collobert, and Y. Bengio. Learning structured embeddings of knowledge bases. In Proceedings of the 25th Annual Conference on Artificial Intelligence (AAAI),2011."}, {"ref": "[4] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.In Proceedings of the International Conference on Artificial Intelligence and Statistics(AISTATS), 2010."}, {"ref": "[5] R. A. Harshman and M. E. Lundy. Parafac: parallel factor analysis. Computational Statistics & Data Analysis, 18(1):39\u201372, Aug. 1994."}, {"ref": "[6] R. Jenatton, N. Le Roux, A. Bordes, G. Obozinski, et al. A latent factor model for highly multi-relational data. In Advances in Neural Information Processing Systems (NIPS 25), 2012."}, {"ref": "[7] C. Kemp, J. B. Tenenbaum, T. L. Griffiths, T. Yamada, and N. Ueda. Learning systems of concepts with an infinite relational model. In Proceedings of the 21st Annual Conference on Artificial Intelligence (AAAI), 2006."}, {"ref": "[8] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems (NIPS 26), 2013."}, {"ref": "[9] G. Miller. WordNet: a Lexical Database for English. Communications of the ACM, 38(11):39\u201341, 1995."}, {"ref": "[10] K. Miller, T. Griffiths, and M. Jordan. Nonparametric latent feature models for link prediction.In Advances in Neural Information Processing Systems (NIPS 22), 2009."}, {"ref": "[11] M. Nickel, V. Tresp, and H.-P. Kriegel. A three-way model for collective learning on multirelational data. In Proceedings of the 28th International Conference on Machine Learning(ICML), 2011."}, {"ref": "[12] M. Nickel, V. Tresp, and H.-P. Kriegel. Factorizing YAGO: scalable machine learning for linked data. In Proceedings of the 21st international conference on World Wide Web (WWW),2012."}, {"ref": "[13] A. P. Singh and G. J. Gordon. Relational learning via collective matrix factorization. In Proceedings of the 14th ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD), 2008."}, {"ref": "[14] R. Socher, D. Chen, C. D. Manning, and A. Y. Ng. Learning new facts from knowledge bases with neural tensor networks and semantic word vectors. In Advances in Neural Information Processing Systems (NIPS 26), 2013."}, {"ref": "[15] I. Sutskever, R. Salakhutdinov, and J. Tenenbaum. Modelling relational data using bayesian clustered tensor factorization. In Advances in Neural Information Processing Systems (NIPS 22), 2009."}, {"ref": "[16] J. Weston, A. Bordes, O. Yakhnenko, and N. Usunier. Connecting language and knowledge bases with embedding models for relation extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2013."}, {"ref": "[17] J. Zhu. Max-margin nonparametric latent feature models for link prediction. In Proceedings of the 29th International Conference on Machine Learning (ICML), 2012."}]}]